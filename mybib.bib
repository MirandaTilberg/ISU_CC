@article{ANNasModelsofNeuralInfoProcessing,
	title = {Editorial: {Artificial} {Neural} {Networks} as {Models} of {Neural} {Information} {Processing}},
	volume = {11},
	issn = {1662-5188},
	shorttitle = {Editorial},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2017.00114/full},
	doi = {10.3389/fncom.2017.00114},
	abstract = {Editorial: Artificial Neural Networks as Models of Neural Information Processing},
	language = {English},
	urldate = {2019-01-16},
	journal = {Frontiers in Computational Neuroscience},
	author = {Gerven, Marcel van and Bohte, Sander},
	year = {2017},
	keywords = {artificial intelligence, computational neuroscience, neural networks, rate coding, spiking neural networks},
	file = {Full Text PDF:C\:\\Users\\Owner\\Zotero\\storage\\HCYDPRQS\\van Gerven and Bohte - 2017 - Editorial Artificial Neural Networks as Models of.pdf:application/pdf}
}

@book{machineVision,
   title =     {Machine vision},
   author =    {Ramesh Jain, Rangachar Kasturi, Brian G. Schunck},
   publisher = {McGraw-Hill Science/Engineering/Math},
   isbn =      {0070320187,9780070320185,0071134077,9780071134071,0072384867,9780072384864},
   year =      {1995},
   series =    {},
   edition =   {1},
   volume =    {},
   url =       {http://gen.lib.rus.ec/book/index.php?md5=96A703D27596EB72EE3A6521525EF9CC}
}
@article{ballardGeneralizingHoughTransform1981,
	title = {Generalizing the {Hough} transform to detect arbitrary shapes},
	volume = {13},
	issn = {00313203},
	url = {http://linkinghub.elsevier.com/retrieve/pii/0031320381900091},
	doi = {10.1016/0031-3203(81)90009-1},
	language = {en},
	number = {2},
	urldate = {2019-02-07},
	journal = {Pattern Recognition},
	author = {Ballard, D.H.},
	month = jan,
	year = {1981},
	note = {05541},
	pages = {111--122},
	file = {Ballard - 1981 - Generalizing the Hough transform to detect arbitra.pdf:/home/srvander/Zotero/storage/DSYITWUH/Ballard - 1981 - Generalizing the Hough transform to detect arbitra.pdf:application/pdf}
}

@incollection{yosinskiHowTransferableAre2014,
	title = {How transferable are features in deep neural networks?},
	url = {http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf},
	urldate = {2019-02-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	note = {02191},
	pages = {3320--3328},
	file = {NIPS Snapshot:/home/srvander/Zotero/storage/TUSQPF67/5347-how-transferable-are-features-in-deep-neural-networks.html:text/html;Yosinski et al_2014_How transferable are features in deep neural networks.pdf:/home/srvander/Zotero/storage/I53LNAP9/Yosinski et al_2014_How transferable are features in deep neural networks.pdf:application/pdf}
}

@article{grossVariabilitySignificanceClass2013,
  title = {The Variability and Significance of Class Characteristics in Footwear Impressions},
  volume = {63},
  number = {3},
  journal = {Journal of Forensic Identification},
  year = {2013},
  pages = {332},
  author = {Gross, Susan and Jeppesen, Dane and Neumann, Cedric},
  file = {/home/srvander/Zotero/storage/EC9IV5R5/Gross et al_2013_The variability and significance of class characteristics in footwear.pdf}
}

@article{hancockInterpretationShoeprintComparison2012,
  title = {The Interpretation of Shoeprint Comparison Class Correspondences},
  volume = {52},
  number = {4},
  journal = {Science and Justice},
  year = {2012},
  pages = {243--248},
  author = {Hancock, Sheida and Morgan-Smith, Rian and Buckleton, John},
  file = {/home/srvander/Zotero/storage/6ULEJZV5/Hancock et al_2012_The interpretation of shoeprint comparison class correspondences.pdf}
}

@incollection{krizhevskyImageNetClassificationDeep2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	urldate = {2019-03-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	note = {36300},
	pages = {1097--1105},
	file = {Krizhevsky et al_2012_ImageNet Classification with Deep Convolutional Neural Networks.pdf:/home/srvander/Zotero/storage/ZKY3H5RS/Krizhevsky et al_2012_ImageNet Classification with Deep Convolutional Neural Networks.pdf:application/pdf;NIPS Snapshot:/home/srvander/Zotero/storage/IRMJ83P8/4824-imagenet-classification-with-deep-convolutional-neural-networks.html:text/html}
}

@inproceedings{pavlouAutomaticExtractionClassification2006,
  title = {Automatic Extraction and Classification of Footwear Patterns},
  booktitle = {International {{Conference}} on {{Intelligent Data Engineering}} and {{Automated Learning}}},
  publisher = {{Springer}},
  year = {2006},
  pages = {721--728},
  author = {Pavlou, Maria and Allinson, Nigel M},
  file = {/home/srvander/Zotero/storage/EBZL8VGE/Pavlou_Allinson_2006_Automatic extraction and classification of footwear patterns.pdf}
}


@article{vgg16,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {https://arxiv.org/abs/1409.1556},
	language = {en},
	urldate = {2018-10-16},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = sep,
	year = {2014},
	file = {Simonyan_Zisserman_2014_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:/home/srvander/Zotero/storage/ULPNXPMS/Simonyan_Zisserman_2014_Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:application/pdf;Snapshot:/home/srvander/Zotero/storage/XG4LXKA2/1409.html:text/html}
}


@article{heDeepResidualLearning2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2019-03-04},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	file = {He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:/home/srvander/Zotero/storage/KQ4QB69E/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}

@book{goldsteinSensationPerception2016,
	title = {Sensation and perception},
	publisher = {Cengage Learning},
	author = {Goldstein, E Bruce and Brockmole, James},
	year = {2016},
	note = {03289},
	file = {Goldstein_Brockmole_2016_Sensation and perception.pdf:/home/srvander/Zotero/storage/TFYLC7HC/Goldstein_Brockmole_2016_Sensation and perception.pdf:application/pdf}
}
@inproceedings{gatysImageStyleTransfer2016,
	address = {Las Vegas, NV, USA},
	title = {Image {Style} {Transfer} {Using} {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780634/},
	doi = {10.1109/CVPR.2016.265},
	abstract = {Rendering the semantic content of an image in different styles is a difﬁcult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic information and, thus, allow to separate image content from style. Here we use image representations derived from Convolutional Neural Networks optimised for object recognition, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can separate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an arbitrary photograph with the appearance of numerous wellknown artworks. Our results provide new insights into the deep image representations learned by Convolutional Neural Networks and demonstrate their potential for high level image synthesis and manipulation.},
	language = {en},
	urldate = {2019-02-14},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	month = jun,
	year = {2016},
	note = {00757},
	pages = {2414--2423},
	file = {Gatys et al. - 2016 - Image Style Transfer Using Convolutional Neural Ne.pdf:/home/srvander/Zotero/storage/ITLGHVQ4/Gatys et al. - 2016 - Image Style Transfer Using Convolutional Neural Ne.pdf:application/pdf}
}
@inproceedings{szegedyGoingDeeperConvolutions2015,
	address = {Boston, MA, USA},
	title = {Going deeper with convolutions},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7298594/},
	doi = {10.1109/CVPR.2015.7298594},
	abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classiﬁcation and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classiﬁcation and detection.},
	language = {en},
	urldate = {2019-03-04},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = jun,
	year = {2015},
	pages = {1--9}
}

@article{ILSVRC15,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}

@article{guRecentAdvancesConvolutional2018,
	title = {Recent advances in convolutional neural networks},
	volume = {77},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320317304120},
	doi = {10.1016/j.patcog.2017.10.013},
	abstract = {In the last few years, deep learning has led to very good performance on a variety of problems, such as visual recognition, speech recognition and natural language processing. Among different types of deep neural networks, convolutional neural networks have been most extensively studied. Leveraging on the rapid growth in the amount of the annotated data and the great improvements in the strengths of graphics processor units, the research on convolutional neural networks has been emerged swiftly and achieved state-of-the-art results on various tasks. In this paper, we provide a broad survey of the recent advances in convolutional neural networks. We detailize the improvements of CNN on different aspects, including layer design, activation function, loss function, regularization, optimization and fast computation. Besides, we also introduce various applications of convolutional neural networks in computer vision, speech and natural language processing.},
	language = {en},
	urldate = {2019-02-18},
	journal = {Pattern Recognition},
	author = {Gu, Jiuxiang and Wang, Zhenhua and Kuen, Jason and Ma, Lianyang and Shahroudy, Amir and Shuai, Bing and Liu, Ting and Wang, Xingxing and Wang, Gang and Cai, Jianfei and Chen, Tsuhan},
	month = may,
	year = {2018},
	pages = {354--377}
}

@book{mallot2000computational,
	title = {Computational {Vision}: {Information} {Processing} in {Perception} and {Visual} {Behaviour}},
	isbn = {978-0-262-13381-4},
	url = {https://books.google.com/books?id=LrGfKrmQbQoC},
	publisher = {Bradford book},
	author = {Mallot, H.A. and Allen, J.S. and Sejnowski, T.J.},
	year = {2000}
}


@article{geirhosComparingDeepNeural2017,
	title = {Comparing deep neural networks against humans: object recognition when the signal gets weaker},
	shorttitle = {Comparing deep neural networks against humans},
	url = {http://arxiv.org/abs/1706.06969},
	urldate = {2019-02-18},
	journal = {arXiv:1706.06969 [cs, q-bio, stat]},
	author = {Geirhos, Robert and Janssen, David H. J. and Schütt, Heiko H. and Rauber, Jonas and Bethge, Matthias and Wichmann, Felix A.},
	month = jun,
	year = {2017}
}

@article{LabelMe,
 author = {Russell, Bryan C. and Torralba, Antonio and Murphy, Kevin P. and Freeman, William T.},
 title = {LabelMe: A Database and Web-Based Tool for Image Annotation},
 journal = {Int. J. Comput. Vision},
 issue_date = {May       2008},
 volume = {77},
 number = {1-3},
 month = may,
 year = {2008},
 issn = {0920-5691},
 pages = {157--173},
 numpages = {17},
 url = {http://dx.doi.org/10.1007/s11263-007-0090-8},
 doi = {10.1007/s11263-007-0090-8},
 acmid = {1345999},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {Annotation tool, Database, Object detection, Object recognition},
}


@techreport{papert_summer_1966,
  type = {{{MIT AI Memos}}},
  title = {The {{Summer Vision Project}}},
  abstract = {The summer vision project is an attempt to use our summer workers effectively in the construction of a significant part of a visual system. The particular task was chosen partly because it can be segmented into sub-problems which allow individuals to work independently and yet participate in the construction of a system complex enough to be real landmark in the development of "pattern recognition". The basic structure is fixed for the first phase of work extending to some point in July. Everyone is invited to contribute to the discussion of the second phase. Sussman is coordinator of "Vision Project" meetings and should be consulted by anyone who wishes to participate. The primary goal of the project is to construct a system of programs which will divide a vidisector picture into regions such as likely objects, likely background areas and chaos. We shall call this part of its operation FIGURE-GROUND analysis. It will be impossible to do this without considerable analysis of shape and surface properties, so FIGURE-GROUND analysis is really inseparable in practice from the second goal which is REGION DESCRIPTION. The final goal is OBJECT IDENTIFICATION which will actually name objects by matching them with a vocabulary of known objects.},
  number = {100},
  author = {Papert, Seymour},
  year = {1966}
}



@article{russakovsky_imagenet_2015,
  title = {{{ImageNet Large Scale Visual Recognition Challenge}}},
  volume = {115},
  issn = {1573-1405},
  doi = {10.1007/s11263-015-0816-y},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
  language = {en},
  number = {3},
  journal = {International Journal of Computer Vision},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and {Fei-Fei}, Li},
  month = dec,
  year = {2015},
  keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
  pages = {211-252},
  file = {C:\\Users\\Owner\\Zotero\\storage\\V5JNXBBQ\\Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf}
}



@book{bodziak_footwear_2000,
	address = {Boca Raton, Florida},
	title = {Footwear {Impression} {Evidence}: {Detection}, {Recovery}, and {Examination}},
	isbn = {0-8493-1045-8},
	publisher = {CRC Press},
	author = {Bodziak, William J.},
	year = {2000},
	note = {00319},
	keywords = {iai\_recommended\_course\_of\_study\_2006}
}
