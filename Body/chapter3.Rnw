% Chapter 3: Results

\Sexpr{knitr::set_parent('../thesis.Rnw')}
\graphicspath{{Images/chapter3/}}

<<ch3-setup, fig.keep='all', cache=FALSE, echo=FALSE, eval=TRUE, include=FALSE>>=
options(replace.assign=TRUE,width=70,scipen=3)
require(knitr)

wd <- getwd()
# Set paths for everything for compiling subdocument
if(!"Body" %in% list.files()){
  opts_chunk$set(fig.path = 'figure/chapter3/fig-', cache.path = 'cache/',
                 fig.align ='center', fig.width = 5, fig.height = 5,
                 fig.show = 'hold', par = TRUE, cache = TRUE,
                 concordance = TRUE, autodep = TRUE, root.dir = "../",
                 message = F, warning = F, error = F)
  datadir <- "../data/chapter3/"
  imgdir <- "../figure/chapter3/"
  codedir <- "../code/"
} else {
  opts_chunk$set(fig.path = 'figure/chapter3/fig-', cache.path = 'cache/',
                 fig.align = 'center', fig.width = 5, fig.height = 5,
                 fig.show = 'hold', par = TRUE, cache = TRUE,
                 concordance = TRUE, autodep = TRUE,
                 message = F, warning = F, error = F)
  datadir <- "data/chapter3/"
  imgdir <- "figure/chapter3/"
  codedir <- "code/"
}
@


\chapter{RESULTS}

\section{Model Evaluation}\label{ch3:model-eval}

\subsection{Model Training}

\begin{itemize}
\item Model training image (Why over-fitting is dangerous and how we've addressed it with number of epochs. Image should show we stop before it occurs)
\item Discuss how accuracy and loss change by epoch

\end{itemize}

- Goal: process that helps model predict new data given repeated exposure to training data. 

-Examples of prediction

-Ways to measure accuracy (TPR, FPR, ROC/AUC) \svp{This is in the introduction, you then use those measures and interpret them} \mt{I think that's what I meant}
-Interesting case studies

\subsection{Model Accuracy}\label{ch3:model-accuracy}

<<eval = F, echo = F>>=
source(file.path(codedir, "keras_test.R"))

get_confusion_matrix(predictions = preds, classes = classes, 
                     test_labels = test_labs) %>%
  set_names(str_to_title(classes)) %>%
  ggcorrplot(., hc.order = F, outline.col = "white", lab = T) +
  scale_fill_gradient("Classification\nRate", low = "white", 
                      high = "cornflowerblue", limits = c(0, 1)) +
  scale_x_discrete("Image Label") + scale_y_discrete("Prediction") +
  theme(axis.title.x = element_text(size = 14), 
        axis.title.y = element_text(size = 14, angle = 90, vjust = 1)) +
  ggtitle("CoNNOR Multi-Class Confusion Matrix: Test Set Performance") + 
  theme_bw() + 
  theme(panel.grid.major = element_line(color = "grey50"),
        panel.grid.minor = element_line(color = "grey60")) +
  theme(plot.margin = grid::unit(c(0,0,0,0), "mm"), 
        plot.background = element_rect(fill = "transparent", color = NA),
        plot.subtitle = element_blank(), plot.caption = element_blank(), 
        panel.spacing = unit(c(0, 0, 0, 0), "mm"))


@

\mt{Let's get rid of this :) :) :)}
\subsection{Model Consistency}\label{ch3:model-consistency}
\svp{Look at how model predictions for the same feature of different color options for a shoe change. Should be a fun case study - does CoNNOR actually have the robustness we claim it should?}

\subsection{Heatmaps - Model Diagnostics} \svp{Add the fun stuff in here!}
