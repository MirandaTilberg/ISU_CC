% Chapter 3: Results

\Sexpr{knitr::set_parent('../thesis.Rnw')}
\graphicspath{{Images/chapter3/}}

<<ch3-setup, fig.keep='all', cache=FALSE, echo=FALSE, eval=TRUE, include=FALSE>>=
options(replace.assign = TRUE, width = 70, scipen = 3)
require(knitr)

wd <- getwd()
# Set paths for everything for compiling subdocument
if (!"Body" %in% list.files()) {
  opts_chunk$set(fig.path = 'figure/chapter3/fig-', cache.path = 'cache/',
                 fig.align = 'center', fig.width = 5, fig.height = 5,
                 fig.show = 'hold', par = TRUE, cache = TRUE,
                 concordance = TRUE, autodep = TRUE, root.dir = "../",
                 message = F, warning = F, error = F)
  datadir <- "../data/chapter3/"
  imgdir <- "../figure/chapter3/"
  codedir <- "../code/"
  modeldir <- "../model/"
} else {
  opts_chunk$set(fig.path = 'figure/chapter3/fig-', cache.path = 'cache/',
                 fig.align = 'center', fig.width = 5, fig.height = 5,
                 fig.show = 'hold', par = TRUE, cache = TRUE,
                 concordance = TRUE, autodep = TRUE,
                 message = F, warning = F, error = F)
  datadir <- "data/chapter3/"
  imgdir <- "figure/chapter3/"
  codedir <- "code/"
  modeldir <- "model/"
}
@


\chapter{RESULTS}

\section{Model Evaluation}\label{ch3:model-eval}

- Goal: process that helps model predict new data given repeated exposure to training data.

-Examples of prediction


\subsection{Model Training}
<<training-accuracy, fig.width = 7.5, fig.height = 5, fig.cap = "Training and validation accuracy and loss for each epoch of the fitting process. Training and validation accuracy reach 89.5\\% around epoch 9. After that point, validation loss remains the same and training loss decreases slightly, while validation accuracy increases more slowly than training accuracy.", fig.scap = "Training and validation accuracy and loss during each epoch.">>=
data.frame(history$metrics) %>%
  mutate(epoch = 1:n()) %>%
  gather(key = "measure", value = "value", -epoch) %>%
  mutate(Type = ifelse(str_detect(measure, "val"), "Validation", "Training"),
         measure = ifelse(str_detect(measure, "acc"), "Accuracy", "Loss")) %>%
  # bind_rows(tibble(epoch = NA, value =  .6, measure = "Accuracy", Type = "Validation")) %>%
  # bind_rows(tibble(epoch = NA, value =  .33, measure = "Loss", Type = "Validation")) %>%
  ggplot(aes(x = epoch, y = value, color = Type)) +
  geom_point() +
  geom_smooth(se = F) +
  facet_grid(measure~., scales = "free_y", switch = "both") +
  theme_bw() +
  scale_y_continuous("") +
  scale_x_continuous("Epoch") +
  ggtitle("CoNNOR Training Performance") + mytheme +
  theme(axis.title.y = element_blank(), legend.position = c(1, .5), legend.justification = c(1.03, -0.05), legend.background = element_rect(fill = "white"))
@

\svp{\autoref{fig:training-accuracy} shows the training and validation accuracy and loss at each epoch of the fitting process. Overfitting, or fitting a model which performs too well on the training data relative to the validation data, is seen when the validation loss starts to increase after reaching a global minimum. This point has not yet occurred, indicating that we have halted the model optimization process at an appropriate epoch.}

The model-training process occurs in a pre-set number of steps, called epochs, in which the model learns the training data via back-propogation and then is tested on the validation data. The model's accuracy is quantified using a loss function. Accuracy of classification of training data will increase with each epoch; however, it is not true that improved training accuracy results in improved classification of new (i.e., test) data. The model's performance on the validation ddata is evaluated for each epoch, but this validation data is never included in the model's learning, so performance on validation classification is the best indicator of how a model will perform on novel data cases.

If the number of epochs is too high, the model will be over-fit to the training data. Thus, it will learn that, for example, a circle can only occur in the ways that circles exist in the training data, so any images of circles in the test data that differ from those in the training data will not be recognized. Thus, the model-training process should be monitored and adjusted by comparing the training accuracy and loss to the validation accuracy and loss for each epoch. Over-fitting occurs when the training performance improves but the validation performance is not improving. As can be seen in (the training image below), our model is trained for 15(?) epochs, and there is not a large discrepancy in the training and validation performance in the later epochs.

<<>>=
#TRAINING IMAGE
@

\subsection{Model Accuracy}

\begin{itemize}
\item Example of model prediction
\item TPR, FPR, and EER
\item Confusion matrix (at EER)
\item ROC/AUC
\end{itemize}
<<roc-code>>=
library(pROC)
pred_df <- as_tibble(preds) %>% gather(key = feature, value = value)
test_labs_df <- as_tibble(test_labs) %>% gather(key = feature, value = value)
whole_model_roc <- roc(test_labs_df$value, pred_df$value)

whole_model_roc_df <- tibble(tpr = whole_model_roc$sensitivities,
                             fpr = 1 - whole_model_roc$specificities,
                             thresholds = whole_model_roc$thresholds,
                             auc = whole_model_roc$auc[1]) %>%
  nest(tpr, fpr, thresholds, .key =  "roc_plot") %>%
  mutate(eer = purrr::map(roc_plot, eer))
ggplot() +
  geom_line(aes(x = fpr, y = tpr), data = unnest(whole_model_roc_df, roc_plot), size = 1.25) +
  geom_label(aes(x = 1, y = .07, label = sprintf("AUC: %0.2f", auc)), hjust = 1, vjust = -0.2, data = whole_model_roc_df) +
  geom_point(aes(x = fpr, y = tpr, color = "Equal Error Rate"), data = unnest(whole_model_roc_df, eer), size = 2) +
  scale_color_manual("", values = "black") +
  scale_x_continuous("False Positive Rate", breaks = c(0, .25, .5, .75, 1), labels = c("0.0", "", "0.5", "", "1.0")) +
  scale_y_continuous("True Positive Rate", breaks = c(0, .25, .5, .75, 1), labels = c("0.0", "", "0.5", "", "1.0")) +
  ggtitle("CoNNOR Test Set Performance (All Classes)") +
  coord_fixed() + mytheme +
  theme(legend.position = c(1, 0), legend.justification = c(1.01, -0.01), legend.title = element_blank(), legend.background = element_rect(fill = "white"))

@

<<auc-code>>=
aucs <- plot_onehot_roc(preds, test_labs, str_to_title(classes))
thresholds <- purrr::map_dbl(aucs$data$eer, ~.$thresholds)
aucs$data$thresholds <- thresholds

ggplot() +
  geom_line(aes(x = fpr, y = tpr), data = unnest(aucs$data, roc_plot), size = 1.25) +
  geom_label(aes(x = 1, y = 0, label = sprintf("AUC: %0.2f\nEER: %0.2f", auc, thresholds)), hjust = 1, vjust = -0.02, data = aucs$data) +
  geom_point(aes(x = fpr, y = tpr, color = "Equal Error\nRate (EER)"), data = unnest(aucs$data, eer), size = 2.5) +
  scale_color_manual("", values = "black") +
  facet_wrap(~class) +
  scale_x_continuous("False Positive Rate", breaks = c(0, .25, .5, .75, 1), labels = c("0.0", "", "0.5", "", "1.0")) +
  scale_y_continuous("True Positive Rate", breaks = c(0, .25, .5, .75, 1), labels = c("0.0", "", "0.5", "", "1.0")) +
  ggtitle("CoNNOR Test Set Performance") +
  facet_wrap(~class, nrow = 2) +
  coord_fixed() +
  theme(legend.position = c(1, 0), legend.justification = c(1, 0))
@


\subsection{Model Accuracy}\label{ch3:model-accuracy}

-Ways to measure accuracy (TPR, FPR, ROC/AUC) \svp{This is in the introduction, you then use those measures and interpret them} \mt{I think that's what I meant}
-Interesting case studies

\mt{Let's get rid of this :) :) :)}\svp{no, we're just moving the bullet point down from the previous section}

\subsection{Model Consistency}\label{ch3:model-consistency}
\svp{Look at how model predictions for the same feature of different color options for a shoe change. Should be a fun case study - does CoNNOR actually have the robustness we claim it should?}

\mt{Want to include interesting prediction cases, like the circle of triangles or whether the model sees a circle in text, or something else that may come up in prediction of test data. I'm guessing those are best saved for heatmaps, since heatmaps let us see why the model gives the prediction it does.}

<<eval = F, echo = F>>=

#This works on Miranda's computer, but isn't a long-term solution

source(file.path(codedir, "keras_test.R"))

get_confusion_matrix(predictions = preds, classes = classes,
                     test_labels = test_labs) %>%
  set_names(str_to_title(classes)) %>%
  ggcorrplot(., hc.order = F, outline.col = "white", lab = T) +
  scale_fill_gradient("Classification\nRate", low = "white",
                      high = "cornflowerblue", limits = c(0, 1)) +
  scale_x_discrete("Image Label") + scale_y_discrete("Prediction") +
  theme(axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14, angle = 90, vjust = 1)) +
  ggtitle("CoNNOR Multi-Class Confusion Matrix: Test Set Performance") +
  theme_bw() +
  theme(panel.grid.major = element_line(color = "grey50"),
        panel.grid.minor = element_line(color = "grey60")) +
  theme(plot.margin = grid::unit(c(0,0,0,0), "mm"),
        plot.background = element_rect(fill = "transparent", color = NA),
        plot.subtitle = element_blank(), plot.caption = element_blank(),
        panel.spacing = unit(c(0, 0, 0, 0), "mm"))
@

\subsection{Heatmaps - Model Diagnostics} \svp{Add the fun stuff in here!}
