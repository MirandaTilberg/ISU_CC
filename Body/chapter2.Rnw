% Chapter 2: Data and Methods

\Sexpr{knitr::set_parent('../thesis.Rnw')}
\graphicspath{{Images/chapter2/}}

<<ch2-setup, fig.keep='all', cache=FALSE, echo=FALSE, eval=TRUE, include=FALSE>>=
options(replace.assign = TRUE, width = 70, scipen = 3)
require(knitr)

wd <- getwd()
# Set paths for everything for compiling subdocument
if (!"Body" %in% list.files()) {
  opts_chunk$set(fig.path = 'figure/chapter2/fig-', cache.path = 'cache/',
                 fig.align = 'center', fig.width = 5, fig.height = 5,
                 fig.show = 'hold', par = TRUE, cache = TRUE,
                 concordance = TRUE, autodep = TRUE, root.dir = "../",
                 message = F, warning = F, error = F)
  datadir <- "../data/chapter2/"
  imgdir <- "../figure/chapter2/"
  codedir <- "../code/"
  modeldir <- "../model/"
} else {
  opts_chunk$set(fig.path = 'figure/chapter2/fig-', cache.path = 'cache/',
                 fig.align = 'center', fig.width = 5, fig.height = 5,
                 fig.show = 'hold', par = TRUE, cache = TRUE,
                 concordance = TRUE, autodep = TRUE, root.dir = "../",
                 message = F, warning = F, error = F)
  datadir <- "data/chapter2/"
  imgdir <- "figure/chapter2/"
  codedir <- "code/"
  modeldir <- "model/"
}
@

<<model-setup, cache = F, echo = F, eval = T, include = F>>=
library(magrittr)
library(tidyverse)
library(ggplot2)
library(keras)
mytheme <- theme_bw() #+
  # theme(panel.grid.major = element_line(color = "grey50"),
  #       panel.grid.minor = element_line(color = "grey60"))

theme_set(mytheme)
knitr::opts_chunk$set(echo = FALSE, message = F, warning = F, cache = T, dpi = 300, dev = 'png')

source(file.path(codedir, "Generate_Model_Images.R"))
@

<<labeled-data-setup, include = F>>=
model_path <- file.path(modeldir, "TrainedModels")

newest_model <- get_newest(dir = model_path, pattern = "weights.h5")
newest_data_file <- file.path(modeldir, "RProcessedImages",
                              newest_model$process_dir, "cropped_photos.Rdata")

load(newest_data_file)
source(file.path(codedir, "count_images.R"))
@

<<load-keras-model, include = F, eval = T, echo = F>>=
model_dir <- newest_model$path
load(list.files(model_dir, pattern = "-history.Rdata", full.names = T)[1])
load(file.path(get_newest()$path, get_newest(pattern = "\\d.Rdata")$base_file))
@


\chapter{DATA AND METHODS}\label{ch2}

\section{Data}

\subsection{Geometric Class Characteristics}

Class characteristics, as defined in Section~\ref{sec:class-chars-desc}, are characteristics which can be used to exclude shoes from a match at a crime scene, but cannot be used for individualized matching because they are shared by many shoes. A sufficiently well-defined set of features can separate shoes into make and model categories \citep{grossVariabilitySignificanceClass2013}. \citet{grossVariabilitySignificanceClass2013} define geometric features such as circle/oval, crepe, herringbone, hexagon, parallel lines, logo/lettering/numbering, perimeter lugs, star, and other. Working from these categories, a set of categories were assembled that is more suited to recognition by convolutional neural networks, as some of the definitions used in \citet{grossVariabilitySignificanceClass2013} require spatial context which is not preserved during labeling (for example, lugs are required to be on the perimeter of the shoe). \autoref{tab:class-char-examples} shows three examples of each class.

\begin{description}
\item [Bowtie] Bowtie shapes are roughly quadrilateral, with two opposite concave faces. The remaining two faces can be convex or straight, and the concave faces may have straight portions, so long as there is a concave region. Using this definition, shapes such as butterflies are included as bowties.
\item [Chevron] Chevron shapes include repeating parallel lines as well as individual ``v" shapes. They may be angular but can also be curved.
\item [Circle] Circles include ellipses and ovals; they must be round.
\item [Line] Lines are repeated and parallel; a more general definition of a line would be difficult to differentiate from many other patterns. Lines can be mildly curved.
\item [Polygon] Polygons are defined in this standard to have more than 4 sides. They include pentagons, hexagons, and octagons.
\item [Quadrilateral] Quadrilaterals (quads) have four sides. They may have rounded or square corners.
\item [Star] Stars are any shape with alternating concave and convex regions, or lines which emanate from a central point. ``X" and ``+" shapes are also classified as stars.
\item [Text] Text is any shape which would be identified as text by a reasonable human. In most cases, the text on the outsole images used is made up of Latin alphabet characters; the model will likely not recognize text in other scripts (but could be trained if non-Latin text images could be obtained).
\item [Triangle] Triangles are any three-sided figure. Like quadrilaterals, they can have rounded corners. In some cases, it is difficult to distinguish between a trapezoidal shape and a triangle when rounded corners are involved.
\item [Other] Other features which were marked include logos, various textures (including crepe, stippling, etc.), and smooth regions with no discernible features. These regions are grouped and provide additional information---that none of the previous nine categories are present.
\end{description}

\begin{table}
\centering

\setlength\tabcolsep{1mm}
\begin{tabular}{rccl}
     Bowtie & \raisebox{-.5\height}{\includegraphics[width=.3\textwidth]{class_examples/bowtie_examples.png}} &
     \raisebox{-.5\height}{\includegraphics[width=.3\textwidth]{class_examples/chevron_examples.png}} & Chevron \vspace{1mm}\\
     Circle & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{class_examples/circle_examples.png}} &
     \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{class_examples/line_examples.png}} & Line  \vspace{1mm}\\
     Polygon & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{class_examples/polygon_examples.png}} &
     \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{class_examples/quad_examples.png}} & Quad  \vspace{1mm}\\
     Star & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{class_examples/star_examples.png}} &
     \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{class_examples/text_examples.png}} & Text  \vspace{1mm}\\
     Triangle & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{class_examples/triangle_examples.png}} &
      \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{class_examples/other_examples.png}} & Other \\
\end{tabular}
\caption[Geometric elements used to classify tread patterns.]{A set of geometric elements used to classify tread patterns. Categories modified from \cite{grossVariabilitySignificanceClass2013}.}\label{tab:class-char-examples}
\end{table}


Defining categories this way does not remove all ambiguities. The best example lies in considering text. The letter ``v" can easily be considered a chevron, and the letter ``o" is clearly a circle. However, text is also an important category to encompass the variety of ways text appears on footwear outsoles, and it is not necessarily helpful (or possible) to try to categorize every shape in text into another category. Many of the ambiguities that arise can be solved by applying multiple labels to an image, but some shapes also do not fit into any categories. Applying comprehensive and consistent labels to difficult or ambiguous shapes is the most difficult part of this process. \autoref{fig:classification-is-hard} shows one shoe that has a tread pattern which does not easily fit into the predefined categories. Generally, the majority of this shoe is labeled ``other".

\begin{figure}
\centering
\includegraphics[width=.6\textwidth]{class_examples/el-naturalista.jpg}
\caption[Classification outliers.]{A shoe which has elements which are not easily classified; the lines in the design do not meet the specified definition of lines (which must be roughly parallel). The rectangle in the center of the shoe can be correctly classified.}\label{fig:classification-is-hard}
\end{figure}
\FloatBarrier


\subsection{Data Collection}

Thousands of outsole images were scraped online shoe retail sites. These images were then uploaded for use in a tool called LabelMe \citep{LabelMe}, a labeling/annotating interface which allows users to easily select and label regions of an image. To date, \Sexpr{unique(ann_df$base_image) %>% length()} shoes have been labeled, yielding \Sexpr{nrow(dfunion)} multi-label images.

After annotation using the LabelMe software package, images are processed by an R script that identifies the minimum bounding rectangle of the region, crops the image to that region, and scales the cropped area to a 256 x 256 pixel image suitable for analysis by the convolutional neural network. During this process, aspect ratio is not preserved, though efforts are made to label regions which are relatively square to minimize the effect of this distortion.


\subsection{Data Characteristics}

<<class-characteristic-barchart, fig.width = 6, fig.height = 4, dpi = 300, fig.cap = "Distribution of classes in all labeled images. Quadrilaterals, lines, circles, text, and chevrons are relatively common; stars, polygons, and bowties are relatively rare.", fig.scap = "Distribution of classes in all labeled images.">>=
annotated_imgs %>%
  mutate(label_type = str_replace(label_type, "multi", "multiple")) %>%
  mutate(labels = str_to_title(labels)) %>%
  mutate(labels = factor(labels, levels = c("Quad", "Line", "Text", "Circle",
                                            "Chevron", "Triangle", "Polygon",
                                            "Star", "Bowtie", "Other"))) %>%
  ggplot() +
  geom_bar(aes(x = labels, fill = label_type), color = "black") +
  scale_fill_manual("Labels", values = c("single" = "#6ba2b9",
                                         "multiple" = "#2e5597")) +
  coord_flip() +
  ylab("# Labeled Images") +
  xlab("")  +
  ggtitle("Current Class Distribution (All Labeled Images)") +
  mytheme +
  theme(legend.position = c(1, 1),
        legend.justification = c(1.03, 1.03),
        legend.background = element_rect(fill = "white"))
@

\autoref{fig:class-characteristic-barchart} shows the relative frequency of each class in the labeled data. Quadrilaterals are the most frequent shape, followed by line and text; the least frequent shapes are bowtie and star. The large class imbalance must be accounted for in both model training and evaluation. Since backpropogation considers each training image equally when updating model weights, training with unbalanced data incentivizes the cost function to prioritize accuracy of the most common classes at the expense of the rarer classes. To mitigate this effect, the weight of each class was set during training with respect to its frequency in the data to ensure that training optimized performance across all classes evenly. Relevant model diagnostics (e.g., ROC curves) were also adjusted to account for class imbalance, most notably by breaking out measures for each class rather than relying on summary metrics that could obfuscate large errors in predicting small classes.


\subsection{Augmentation}

Labeled images are scarce relative to the amount of data necessary to train a neural network. One solution to this scarcity is to artificially enlarge the data set using a process called image augmentation \citep{krizhevskyImageNetClassificationDeep2012}. Augmentation is the transformation of original input data using image operations such as cropping, zoom, skew, rotation, and color balance modification in order to distort or alter the image while maintaining the essential features corresponding to the label. This process reduces the potential for overfitting the model to the specific set of image data used during the training process, and also increases the amount of data available for training. During the model fitting process, images are augmented once using a subset of the augmentation operations discussed above; examples of pre- and post-augmentation images are shown in \autoref{fig:augmented}.

\begin{figure}
\centering
\includegraphics[width=.1\textwidth]{augmentation/bowtie(R)-1-ugg-sienna-enamel-blue_product_8726365_color_120557.jpg}\hfill
\includegraphics[width=.1\textwidth]{augmentation/aug_bowtie(R)-1-ugg-sienna-enamel-blue_product_8726365_color_120557_0_8344.jpg}\hfill\hfill\hfill
\includegraphics[width=.1\textwidth]{augmentation/quad-2-nike-kids-downshifter-7-infant-toddler-gunsmoke-sunset-pulse-atmosphere-grey_product_8800875_color_720973.jpg}\hfill
\includegraphics[width=.1\textwidth]{augmentation/aug_quad-2-nike-kids-downshifter-7-infant-toddler-gunsmoke-sunset-pulse-atmosphere-grey_product_8800875_color_720973_0_7533.jpg}\hfill\hfill\hfill
\includegraphics[width=.1\textwidth]{augmentation/quad-2-puma-suede-classic-x-mac-three-port-royale-port-royale_product_9123838_color_251426.jpg}\hfill
\includegraphics[width=.1\textwidth]{augmentation/aug_quad-2-puma-suede-classic-x-mac-three-port-royale-port-royale_product_9123838_color_251426_0_5689.jpg}\hfill\hfill\hfill
\includegraphics[width=.1\textwidth]{augmentation/text-1-bernie-mev-kids-catwalk-little-kid-big-kid-multi-camo_product_9084647_color_87089.jpg}\hfill
\includegraphics[width=.1\textwidth]{augmentation/aug_text-1-bernie-mev-kids-catwalk-little-kid-big-kid-multi-camo_product_9084647_color_87089_0_1891.jpg}
\caption[Original and augmented images.]{Four sets of original (left) and augmented (right) labeled images.}\label{fig:augmented}
\end{figure}


\section{VGG16}

\subsection{Architecture}

Developed by Oxford's Visual Graphics Group, VGG16 is a CNN with 16 ``functional" (i.e., convolutional and densely connected) layers and 5 ``structural" max pooling layers, as shown in \autoref{fig:vgg16-structure}. In contrast to other popular networks, like ResNet, VGG has a relatively simple structure that provides easier training and interpretability with very little sacrificed accuracy (the structure is shown in \autoref{fig:vgg16-structure}). The simplicity of this structure provides the ability to peer into the inner workings of the network for diagnostic purposes, providing a distinct advantage over more complicated network structures with slightly higher accuracy ratings.

VGG16 and a similar network, VGG19, won the 2014 ILSVR challenge; they have since been surpassed in performance by networks with more complicated structures, such as GoogLeNet/Inception \citep{szegedyGoingDeeperConvolutions2015} and ResNet \citep{heDeepResidualLearning2015}. VGG-style networks are still commonly used as building blocks for other types of convolutional networks; their streamlined structure allows for easy extension to other tasks, such as texture detection and style transfer \citep{gatysImageStyleTransfer2016}.

\begin{figure}
\centering
\includegraphics[width=.95\textwidth]{vgg16-layer-size}
\caption[VGG16 model structure.]{The VGG16 model structure, for input images of 256 x 256 pixels. There are 5 convolutional blocks in the VGG16 model base. Each block is followed by a max pooling layer, which reduces the size of the input to the next block by a factor of 4. The output of the final pooled block is then connected to output classes using several fully connected layers.} \label{fig:vgg16-structure}
\end{figure}


\subsection{Convolutional Filters}\label{subsec:vgg16-filters}

Much like the specialized feature detector cells contained within the human visual system for detecting specific angles, colors, and shapes, convolutional filters are designed to detect a wide array of features. The early convolutional layers of VGG16 contain 64 filters that primarily detect colors and edge patterns, as shown by the activation maps in \autoref{fig:b1} and \autoref{fig:b2}. Later convolutional layers detect increasingly complex features; \autoref{fig:b3} shows a selection of activation maps from convolutional layers in block 3 of the model, and \autoref{fig:b4} shows a selection from block 4 of the model. Note that some of the filters appear to detect more than one ``feature"; some filters are multi-purpose. The final convolutional layers of VGG16, in contrast, contain 512 filters that represent much more complex features, like animal fur patterns or distinct bird heads, which can be seen in \autoref{fig:b5}.

In order to visualize the layers in a convolutional neural network, backpropagation can be used to generate an image which maximally activates a particular filter. This process generates so-called ``activation maps", which provide a visual reference for the features identified by a particular layer. The activation maps shown in this section are generated using the \texttt{KerasVis} R package, which makes functions from the \texttt{keras-vis} python library \citep{raghakotkerasvis} available in R.

\FloatBarrier

<<b1, fig.width = 7, fig.height = 2.3, fig.cap = "A selection of filters from block 1 of VGG16. Block 1 contains two convolutional layers, each with 64 filters.", fig.scap = "A selection of filters from block 1 of VGG16.">>=
imgs <- list.files(here::here("Images", "chapter2", "vgg16layers", "b1"), full.names = T)
pngs <- lapply(imgs, function(x) png::readPNG(x)[46:1596, 32:1582, ])
grobs <- lapply(pngs, grid::rasterGrob)
gridExtra::grid.arrange(grobs = grobs, nrow = 2)
@

<<b2, fig.width = 7, fig.height = 2.3, fig.cap = "A selection of filters from block 2 of VGG16. Block 2 contains two convolutional layers, each with 128 filters.", fig.scap = "A selection of filters from block 2 of VGG16.">>=
imgs <- list.files(here::here("Images", "chapter2", "vgg16layers", "b2"), full.names = T)
pngs <- lapply(imgs, function(x) png::readPNG(x)[46:1596, 32:1582, ])
grobs <- lapply(pngs, grid::rasterGrob)
gridExtra::grid.arrange(grobs = grobs, nrow = 2)
@

<<b3, fig.width = 7, fig.height = 3.5, fig.cap = "A selection of filters from block 3 of VGG16. Block 3 contains three convolutional layers, each with 256 filters.", fig.scap = "A selection of filters from block 3 of VGG16.">>=
imgs <- list.files(here::here("Images", "chapter2", "vgg16layers", "b3"), full.names = T)
pngs <- lapply(imgs, function(x) png::readPNG(x)[46:1596, 32:1582, ])
grobs <- lapply(pngs, grid::rasterGrob)
gridExtra::grid.arrange(grobs = grobs, nrow = 3)
@

<<b4, fig.width = 7, fig.height = 3.5, fig.cap = "A selection of filters from block 4 of VGG16. Block 4 contains three convolutional layers, each with 512 filters.", fig.scap = "A selection of filters from block 4 of VGG16.">>=
imgs <- list.files(here::here("Images", "chapter2", "vgg16layers", "b4"), full.names = T)
pngs <- lapply(imgs, function(x) png::readPNG(x)[46:1596, 32:1582, ])
grobs <- lapply(pngs, grid::rasterGrob)
gridExtra::grid.arrange(grobs = grobs, nrow = 3)
@

<<b5, fig.width = 7, fig.height = 3.5, fig.cap = "A selection of filters from block 5 of VGG16. Block 5 contains three convolutional layers, each with 512 filters.", fig.scap = "A selection of filters from block 5 of VGG16.">>=
imgs <- list.files(here::here("Images", "chapter2", "vgg16layers", "b5"), full.names = T)
pngs <- lapply(imgs, function(x) png::readPNG(x)[46:1596, 32:1582, ])
grobs <- lapply(pngs, grid::rasterGrob)
gridExtra::grid.arrange(grobs = grobs, nrow = 3)
@

\FloatBarrier


\subsection{Model Training}

\paragraph{Computation}

Model training was conducted using the \texttt{keras} package in R \citep{keras-package}, which provides an interface to the Python keras API. The keras API uses a TensorFlow \citep{tensorflow2015-whitepaper} back end. The model was trained using CPUs; the amount of memory required in order to train the model using the GPU was prohibitive.

Initial training utilized the output from the VGG16 convolutional base as input to a new model consisting of a densely connected layer with 256 nodes and 50\% dropout, followed by an activation layer with a sigmoid activation function (see \autoref{fig:activation-functions}) and 9 output classes corresponding to the 9 geometric features that have been identified. Once the separate model head was fit, the convolutional base of VGG16 and the newly trained model head were combined into a single, unified model object for prediction.

Code is provided in \autoref{app:code}. Using a 48-thread CPU with 128 GB of RAM, the time required to process the LabelMe annotations, generate 256 $\times$ 256 pixel labeled images, augment these images, and train the model is just under 3 hours; the model fitting process itself takes less than one hour.

%CoNNOR was trained using a 20dB gnu powered by a 4000 kJ hamster wheel and graduate student tears
% I'm seriously considering leaving that sentence in, but I think as a faculty member/actual adult I'm probably not supposed to condone seeing if your committee actually read the whole thing...


\paragraph{Model Training Parameters}

The \Sexpr{nrow(dfunion)} images were split such that 60\% were used for training. Since the categories do not exist in equal proportion in the labeled data, the training data were weighted by proportion during the training process to prevent the loss function from being overwhelmed by more frequent categories. Of the remaining 40\% of data, half were used for validation, to monitor the training process, and the remaining data were for testing the performance of the fitted model. Code to calculate the class weights is provided as part of \autoref{app:code}, but a static example of the weights and image counts is shown in \autoref{tab:model-weight}.

<<model-weight-calc, results = 'asis', eval = F>>=
load(file.path(model_dir, paste0(newest_model$start_date, "_",
                                 newest_model$prefix, "fullimage.rdata")))
bind_cols(class = classes,
          count = colSums(train$labels),
          class_proportion = class_proportions) %>%
  bind_rows(bind_cols(class = "total",
                      count = sum(train$labels),
                      class_proportion = sum(class_proportions))) %>%
  xtable::xtable(caption = c("Class counts and proportions. Note that multiple-label images are counted separately for each label. Proportions are passed to keras as class weights to ensure that lower-probability classes are learned with equal attention to the available data.", "Class counts and proportions."),
                 digits = c(0, 0, 0, 4)) %>%
  xtable::print.xtable(include.rownames = F, hline.after = c(-1, 0, 9, 10))
@

\begin{table}[ht]
\centering
\begin{tabular}{lrr}
  \hline
Class & Count & Class Proportion \\
  \hline
bowtie & 1102 & 0.2422 \\
  chevron & 4372 & 0.0610 \\
  circle & 4810 & 0.0555 \\
  line & 6490 & 0.0411 \\
  polygon & 1364 & 0.1956 \\
  quad & 8518 & 0.0313 \\
  star & 1294 & 0.2062 \\
  text & 6144 & 0.0434 \\
  triangle & 2160 & 0.1235 \\
   \hline
Total & 36254 & 1.0000 \\
   \hline
\end{tabular}
\caption[Class counts and proportions.]{Class counts and proportions. Note that multiple-label images are counted separately for each label. Proportions are passed to keras as class weights to ensure that lower-probability classes are learned with equal attention to the available data.}\label{tab:model-weight}
\end{table}

During the model training process, there are a set number of ``epochs", which consist of a forward propagation step and a backpropagation step, for the entire set of training data. At the end of the forward propagation step, predicted probabilities are computed and the loss function is applied to the predictions; the average loss over all training images is then the cost function that is used in the backward propagation portion of the epoch. Finally, the validation data is used to assess the model's performance at the end of the backward propagation step; the validation data is used to assess the model, but is not used in any updating of weights. The separate validation set is used to assess the model's performance and identify any tendency toward overfitting. Chapter~\ref{ch:results} discusses the fitting process for this specific model and contains an evaluation of the model's performance.
