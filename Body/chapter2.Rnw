% Chapter 2: Data and Methods

\Sexpr{knitr::set_parent('../thesis.Rnw')}
\graphicspath{{Images/chapter2/}{../Images/chapter2/}}

<<ch2-setup, fig.keep='all', cache=FALSE, echo=FALSE, eval=TRUE, include=FALSE>>=
options(replace.assign = TRUE, width = 70, scipen = 3)
require(knitr)

wd <- getwd()
# Set paths for everything for compiling subdocument
if (!"Body" %in% list.files()) {
  opts_chunk$set(fig.path = 'figure/chapter2/fig-', cache.path = 'cache/',
                 fig.align = 'center', fig.width = 5, fig.height = 5,
                 fig.show = 'hold', par = TRUE, cache = TRUE,
                 concordance = TRUE, autodep = TRUE, root.dir = "../",
                 message = F, warning = F, error = F)
  datadir <- "../data/chapter2/"
  imgdir <- "../figure/chapter2/"
  codedir <- "../code/"
  modeldir <- "../model/"
} else {
  opts_chunk$set(fig.path = 'figure/chapter2/fig-', cache.path = 'cache/',
                 fig.align = 'center', fig.width = 5, fig.height = 5,
                 fig.show = 'hold', par = TRUE, cache = TRUE,
                 concordance = TRUE, autodep = TRUE, root.dir = "../",
                 message = F, warning = F, error = F)
  datadir <- "data/chapter2/"
  imgdir <- "figure/chapter2/"
  codedir <- "code/"
  modeldir <- "model/"
}
@

<<model-setup, cache = F, echo = F, eval = T, include = F>>=
library(magrittr)
library(tidyverse)
library(ggplot2)
library(keras)
library(furrr)
plan(multicore)
mytheme <- theme_bw() #+
  # theme(panel.grid.major = element_line(color = "grey50"),
  #       panel.grid.minor = element_line(color = "grey60"))

theme_set(mytheme)
knitr::opts_chunk$set(echo = FALSE, message = F, warning = F, cache = T, dpi = 300, dev = 'png')

source(file.path(codedir, "Generate_Model_Images.R"))
@

<<labeled-data-setup, include = F>>=
model_path <- file.path(modeldir, "TrainedModels")

newest_model <- get_newest(dir = model_path, pattern = "weights.h5")
newest_data_file <- file.path(modeldir, "RProcessedImages",
                              newest_model$process_dir, "cropped_photos.Rdata")

load(newest_data_file)

label_fixes <- c("triangles" = "triangle",
                 "trianglee" = "triangle",
                 "circlemtraignle" = "circle_triangle",
                 "circle triangle" = "circle_triangle",
                 "circleline" = "circle_line",
                 "circle text" = "circle_text",
                 "circle_elongated" = "circle",
                 "cricle|cirle" = "circle",
                 "chrevron" = "chevron",
                 "lie" = "line",
                 "texxt" = "text",
                 "star quad" = "star_quad",
                 "exc_idd" = "exclude",
                 "quad?" = "quad",
                 "qaud|qud" = "quad",
                 "stars|start" = "star",
                 "exlude" = "exclude",
                 "rounded" = "",
                 "smooth_texture" = "other",
                 "octagon" = "polygon",
                 "crepe" = "other",
                 "hex" = "polygon",
                 "smooth|hatching" = "other"
                 )

annotated_imgs <- select(dfunion, image, name) %>%
  mutate(base_image = basename(image)) %>%
  mutate(num_labels = str_count(name, "_") + 1,
         annot_num = 1:n()) %>%
  mutate(labels = str_remove_all(name, "\\(.{1,2}\\)") %>%
           str_replace_all(label_fixes) %>%
           str_split("_")) %>%
  unnest(labels) %>%
  mutate(labels = str_replace_all(labels, label_fixes)) %>%
  mutate(label_type = ifelse(num_labels == 1, "single", "multi")) %>%
  filter(labels != "") %>%
  filter(labels != "exclude") %>%
  filter(labels != "ribbon" & labels != "logo") %>%
  filter(labels %in% c(default_classes, "other"))

fix_annotation <- function(annot) {
  useful_vars <- c("name", "deleted", "date", "id", "username")
  new_vars <- set_names(useful_vars, c("name", "deleted", "date", "objID", "username"))

  get_vars <- new_vars[new_vars %in% names(annot)]

  if (!is.null(annot)) select(annot, !!get_vars) %>% as_tibble() else tibble()
}

# authors <- df %>%
#   filter(!is.null(fullannot)) %>%
#   mutate(base_image = basename(image)) %>%
#   select(id, base_image, image, fullannot) %>%
#   filter(!is.null(date)) %>%
#   mutate(miniannot = map(fullannot, fix_annotation)) %>%
#   select(-fullannot) %>%
#   unnest(miniannot)
@

<<img-setup, include = F>>=
new_img_dir <- dirname(newest_data_file)

ann_df <- select(annotated_imgs, base_image, name, num_labels, annot_num, labels, label_type) %>%
  group_by(base_image, name) %>%
  arrange(annot_num) %>%
  mutate(ln = 1:n())

img_df <- tibble(img = list.files(file.path(new_img_dir, c("test", "train", "validation")), full.names = T)) %>%
  mutate(type = str_extract(img, "(test|train|validation)"),
         image = basename(img),
         aug = grepl("^aug", image),
         name = str_extract(image, "^([a-z\\(\\)RE]*?_?){1,}-\\d{1,}-"),
         ln = str_extract(name, "-\\d{1,}-$") %>% gsub(pattern = "-", replacement = "") %>% as.numeric(),
         name = gsub(pattern = "-\\d{1,}-$", "", name),
         name = gsub(pattern = "aug_", "", name),
         name = gsub(pattern = "logo|ribbon|logo\\(R\\)|ribbon\\(R\\)", replacement = "other", name),
         base_image = gsub(image, pattern = "^([a-z\\(\\)RE]*?_?){1,}-\\d{1,}-", replacement = "") %>%
           gsub(., pattern = "_\\d_\\d{1,}.jpg", replacement = ".jpg")) %>%
  select(type, base_image, name, ln, aug, img_path = image) %>%
  mutate(num_labels = str_count(name, "_") + 1,
         annot_num = 1:n()) %>%
  mutate(labels = str_remove_all(name, "\\(.{1,2}\\)") %>%
           str_replace_all(label_fixes) %>%
           str_split("_")) %>%
  unnest(labels) %>%
  mutate(labels = str_replace_all(labels, label_fixes)) %>%
  mutate(label_type = ifelse(num_labels == 1, "single", "multi")) %>%
  filter(labels != "" & labels != "hatching" & labels != "exclude" & !is.na(labels))
@

<<load-keras-model, include = F>>=
model_dir <- newest_model$path
load(list.files(model_dir, "-history.Rdata", full.names = T)[1])
load(file.path(get_newest()$path, get_newest(pattern = "\\d.Rdata")$base_file))
@
\chapter{DATA AND METHODS}

\section{Data}

\subsection{Geometric Class Characteristics}

Class characteristics, as defined in \autoref{sec:class-chars-desc}, are characteristics which can be used to exclude shoes from a match at a crime scene, but cannot be  used for individualized matching because they are shared by many shoes. A sufficiently well-defined set of \svp{features} can separate shoes into make and model categories \citep{grossVariabilitySignificanceClass2013}. \citet{grossVariabilitySignificanceClass2013} define geometric features such as circle/oval, crepe, herringbone, hexagon, parallel lines, logo/lettering/numbering, perimeter lugs, star, and other. Working from these categories, we assembled a set of categories which were more suited to recognition by convolutional neural networks, as some of the definitions used in \citet{grossVariabilitySignificanceClass2013} require spatial context which is not preserved during \svp{labeling} (for example, lugs are required to be on the perimeter of the shoe). \autoref{tab:class-char-examples} shows three examples of each class.

\begin{description}
\item [Bowtie] Bowtie shapes are roughly quadrilateral, with two opposite concave faces. The remaining two faces can be convex or straight, and the concave faces may have straight portions, so long as there is a concave region. Using this definition, shapes such as butterflies are included as bowties.
\item [Chevron] Chevron shapes include repeating parallel lines as well as individual ``v" shapes. They may be angular but can also be curved.
\item [Circle] Circles include ellipses and ovals; they must be round.
\item [Line] Lines are repeated and parallel; a more general definition of a line would be difficult to differentiate from many other patterns. Lines can be mildly curved.
\item [Polygon] Polygons are defined in this standard to have more than 4 sides. They include pentagons, hexagons, and octagons.
\item [Quadrilateral] Quadrilaterals (quads) have four sides. They may have rounded or square corners.
\item [Star] Stars are any shape with alternating concave and convex regions, or lines which emanate from a central point. ``X" and ``+" shapes are also classified as stars.
\item [Text] Text is any shape which would be identified as text by a reasonable human. In most cases, the text on our images is made up of latin alphabet charcters; the model will likely not recognize text in other scripts at this point (but could be trained with text in other scripts if such training images could be obtained). Text frequently includes component shapes such as circles; where these shapes are clearly defined, they are labeled as well, as the model does not have the ability at this time to impose the necessary context on images to differentiate an ``o" from a circle.
\item [Triangle] Triangles are any three-sided figure. Like quadrilaterals, they can have rounded corners. In some cases, it is difficult to distinguish between a trapezoidal shape and a triangle when rounded corners are involved.
\item [Other] Other features which were marked include logos, various textures (including crepe, stippling, etc.), and smooth regions with no discernible features. These regions are grouped and provide additional information - that none of the previous nine categories are present.
\end{description}


Defining categories this way does not remove all ambiguities. The best example lies in considering text. The letter "v" can easily be considered a chevron, and the letter "o" is clearly a circle. However, text is also an important category to encompass the variety of ways text appears on footwear outsoles, and it is not necessarily helpful (or possible) to try to categorize every shape in text into another category. Many of the ambiguities that arise can be solved by applying multiple labels to an image, but some shapes also do not fit into any categories. Applying comprehensive and consistent labels to difficult or ambiguous shapes is the most difficult part of this process.

\begin{table}
\centering

\setlength\tabcolsep{1mm}
\begin{tabular}{rccl}
     Bowtie & \raisebox{-.5\height}{\includegraphics[width=.3\textwidth]{class_examples/bowtie_examples.png}} &
     \raisebox{-.5\height}{\includegraphics[width=.3\textwidth]{class_examples/chevron_examples.png}} & Chevron \vspace{1mm}\\
     Circle & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{class_examples/circle_examples.png}} &
     \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{class_examples/line_examples.png}} & Line  \vspace{1mm}\\
     Polygon & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{class_examples/polygon_examples.png}} &
     \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{class_examples/quad_examples.png}} & Quad  \vspace{1mm}\\
     Star & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{class_examples/star_examples.png}} &
     \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{class_examples/text_examples.png}} & Text  \vspace{1mm}\\
     Triangle & \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{class_examples/triangle_examples.png}} &
      \raisebox{-.5\height}{\includegraphics[width=0.3\textwidth]{class_examples/other_examples.png}} & Other \\
\end{tabular}
\caption[Geometric elements]{Geometric Elements. Categories modified from \cite{grossVariabilitySignificanceClass2013}.}\label{tab:class-char-examples}
\end{table}

\subsection{Data Collection}

Thousands of outsole images were web-scraped from Zappos.com, a large online shoe retailer. These images were then uploaded for use in a tool called LabelMe \mt{(cite)}, a labeling/annotating interface which allows users to easily select and label regions of an image. To date, about [2,200] shoes have been labeled, yielding about [24,000] multi-label images.

\svp{After annotation using the LabelMe software package, images are processed by an R script, which identifies the minimum bounding rectangle of the region, crops the image to that region, and scales the cropped area to a 256 x 256 pixel image suitable for analysis by the convolutional neural network. During this process, aspect ratio is not preserved, though efforts are made to label images which are relatively square to minimize the effect of this distortion.}
%\mt{Once images are labeled in LabelMe, they are processed in R by a script that employs the imager and spatial packages to chop the labeled image into a 256x256 square pixel region. This script employs a number of strategies to deal with abnormally-shaped labels. For example, while there have been efforts to apply labels to the raw images in relatively square regions, this is not always possible. Regions are thus appropriately rotated and scaled down, but aspect ratio is not necessarily preserved.} % Slight rephrasing for a bit tidier flow of thought.


\subsection{Data Characteristics}

<<class-characteristic-barchart, fig.width = 6, fig.height = 4, dpi = 300, fig.cap = "Distribution of classes in all labeled images. Quadrilaterals, lines, circles, text, and chevrons are relatively common; stars, polygons, and bowties are relatively rare.", fig.scap = "Distribution of classes in all labeled images">>=
annotated_imgs %>%
  mutate(label_type = str_replace(label_type, "multi", "multiple")) %>%
  mutate(labels = str_to_title(labels)) %>%
  mutate(labels = factor(labels, levels = c("Quad", "Line", "Text", "Circle", "Chevron", "Triangle", "Polygon", "Star", "Bowtie", "Other"))) %>%
  ggplot() +
  geom_bar(aes(x = labels, fill = label_type), color = "black") +
  scale_fill_manual("Labels", values = c("single" = "#6ba2b9", "multiple" = "#2e5597")) +
  coord_flip() +
  ylab("# Labeled Images") +
  xlab("")  +
  ggtitle("Current Class Distribution (All Labeled Images)") +
  mytheme +
  theme(legend.position = c(1, 1), legend.justification = c(1.03, 1.03),
        legend.background = element_rect(fill = "white"))
@

\svp{Describe \autoref{fig:class-characteristic-barchart} in words. Talk about the fact that the class imbalance means we will have to weight the classes for the model, and that this also affects the utility of accuracy measures. }

% \svp{One thing I need to examine is what happens if we use histogram equalization to modify the images before augmentation. I did a bit of that for presentation images and it changed the output probabilities a lot... which suggests it may help with images that don't have even color balance.}

\subsection{Augmentation}
Labeled images are scarce relative to the amount of data necessary to train a neural network. One solution to this scarcity is to artificially enlarge the data set using a process called image augmentation\citep{krizhevskyImageNetClassificationDeep2012}. Augmentation is the transformation of original input data using image operations such as cropping, zoom, skew, rotation, and color balance modification in order to distort or alter the image while maintaining the essential features corresponding to the label. This process reduces the potential for overfitting the model to the specific set of image data used during the training process, and also increases the amount of data available for training. During the model fitting process, images are augmented once using a subset of the augmentation operations discussed above; examples of pre- and post- augmentation images are shown in \autoref{fig:augmented}.

\begin{figure}
\centering
\includegraphics[width=.1\textwidth]{augmentation/bowtie(R)-1-ugg-sienna-enamel-blue_product_8726365_color_120557.jpg}\hfill
\includegraphics[width=.1\textwidth]{augmentation/aug_bowtie(R)-1-ugg-sienna-enamel-blue_product_8726365_color_120557_0_8344.jpg}\hfill\hfill\hfill
\includegraphics[width=.1\textwidth]{augmentation/quad-2-nike-kids-downshifter-7-infant-toddler-gunsmoke-sunset-pulse-atmosphere-grey_product_8800875_color_720973.jpg}\hfill
\includegraphics[width=.1\textwidth]{augmentation/aug_quad-2-nike-kids-downshifter-7-infant-toddler-gunsmoke-sunset-pulse-atmosphere-grey_product_8800875_color_720973_0_7533.jpg}\hfill\hfill\hfill
\includegraphics[width=.1\textwidth]{augmentation/quad-2-puma-suede-classic-x-mac-three-port-royale-port-royale_product_9123838_color_251426.jpg}\hfill
\includegraphics[width=.1\textwidth]{augmentation/aug_quad-2-puma-suede-classic-x-mac-three-port-royale-port-royale_product_9123838_color_251426_0_5689.jpg}\hfill\hfill\hfill
\includegraphics[width=.1\textwidth]{augmentation/text-1-bernie-mev-kids-catwalk-little-kid-big-kid-multi-camo_product_9084647_color_87089.jpg}\hfill
\includegraphics[width=.1\textwidth]{augmentation/aug_text-1-bernie-mev-kids-catwalk-little-kid-big-kid-multi-camo_product_9084647_color_87089_0_1891.jpg}
\caption[Original and augmented images]{Four sets of original (left) and augmented (right) labeled images.}\label{fig:augmented}
\end{figure}

% \mt{Augmentation} \svp{operations include} \mt{crop, zoom, skew, rotate, and color. Images are augmented once. Helps increase amount of data and improves model generalizability.}
%
% Image augmentation is a common technique to both increase the amount of data available for training a model and improve the generalizability of model predictions. Augmenting an image refers to applying a number of visual transformations, including (but not limited to) cropping, zooming, skewing or shearing), rotating, and adjusting color channels.

% \mt{Add image examples here when Bigfoot isn't busy}

\section{VGG16}

\subsection{Architecture}
Developed by Oxford's Visual Graphics Group, VGG16 is a CNN with 16 "functional" (i.e., convolutional and densely connected) layers and 5 "structural" max-pooling layers. In contrast to other popular networks, like ResNet, VGG has a relatively simple structure that provides easier training and interpretability with very little sacrificed accuracy.

\includegraphics[width = \linewidth]{vgg16-shoe-nolabel.png}

The early convolutional layers of VGG16 contain 64 filters that primarily detect colors and edge patterns. Later convolutional layers of VGG16, in contrast, contain 512 filters that represent much more complex features, like animal fur patterns or distinct bird heads. The images shown in this section are generated using the \texttt{KerasVis} R package, which makes functions from the \texttt{keras-vis} python library\citep{raghakotkerasvis} available in R.

\mt{Really want to include bird head images here. When will the KerasVis stuff be ready in R? :)}

\mt{XXX VGG16 follows groups of 2 or 3 convolutional layers with a max-pooling layer, which summarizes the information in the feature maps and scales information down by a factor of 4.}

<<kerasvis, echo = F, eval = F>>=

### KerasVis DOESN'T WORK ON MIRANDA'S COMPUTER


library(KerasVis)
KerasVis:::import_all_dependendices()
source(file.path(codedir, "Generate_Model_Images.R"))
model_path <- "/models/shoe_nn/TrainedModels/"
newest_model <- get_newest(dir = model_path, pattern = "weights.h5")
model_wts_file <- file.path(newest_model$path, newest_model$base_file)
loaded_model <- set_weights(model_wts_file)
layer_names <- list("block1_conv1")
visualize_activation(loaded_model,
                 selected_filters = list(1L:63L),
                 layer_names = layer_names, save_folder = file.path("Images", "chapter2", "vgg16layers"))
@


\subsection{Model Training}

\paragraph{Computation}

Model training was conducted using the \texttt{keras} package in R\citep{keras-package}, which provides an interface to the Python keras API. \svp{The keras API uses a tensorflow \citep{tensorflow2015-whitepaper} backend. The model was trained using CPUs; the amount of memory required in order to train the model using the GPU was prohibitive.

Initial training utilized the output from the VGG16 convolutional base as input to a new model consisting of a densely connected layer with 50\% dropout, followed by an activation layer with a sigmoid activation function (see \autoref{fig:activation-functions}) and 9 output classes corresponding to the 9 geometric features we have identified. Once the separate model head was fit, we utilized keras' facilities for loading model weights to combine the convolutional base of VGG16 with the model head into a single, unified model object.

Code is provided in \autoref{app:Code}.  Using a 48-thread CPU with 128 GB of RAM, the time required to process the LabelMe annotations, generate 256 $\times$ 256 pixel labeled images, augment these images, and train the model is just under 3 hours; the model fitting process itself takes less than one hour.
}
%Our model was trained using a 20dB GPU powered by a 4000 kJ hamster wheel and graduate student tears (JK, Susan help?.) % I'm seriously considering leaving that sentence in, but I think as a faculty member/actual adult I'm probably not supposed to condone seeing if your committee actually read the whole thing...


\paragraph{Model Training Parameters}

The \Sexpr{nrow(dfunion)} images were split such that 60\% were used for training. Since the categories do not exist in equal proportion in the labeled data, the training data were weighted by proportion during the training process to prevent the loss function from being overwhelmed by more frequent categories. Of the remaining 40\% of data, 20\% were used for validation, to monitor the training process, and the remaining data were for testing \svp{the performance of the fitted model.} \svp{Code to calculate the class weights is provided as part of \autoref{app:model-head-training}, but a static example of the weights and image counts is shown in \autoref{tab:model-weight}.}

<<model-weight-calc, results = 'asis', eval = F>>=
load(file.path(model_dir, paste0(newest_model$start_date, "_", newest_model$prefix, "fullimage.rdata")))
bind_cols(class = classes, count = colSums(train$labels), class_proportion = class_proportions) %>%
  bind_rows(bind_cols(class = "total", count = sum(train$labels), class_proportion = sum(class_proportions))) %>%
  xtable::xtable(caption = c("Class counts and proportions. Note that multiple-label images are counted separately for each label. Proportions are passed to keras as class weights to ensure that lower-probability classes are learned with equal attention to the available data.", "Class counts and proportions.")) %>%
  xtable::print.xtable(include.rownames = F, hline.after = c(0, 9, 10))
@
\begin{table}[ht]
\centering
\begin{tabular}{lrr}
 class & count & class\_proportion \\
  \hline
bowtie & 1102.00 & 0.24 \\
  chevron & 4372.00 & 0.06 \\
  circle & 4810.00 & 0.06 \\
  line & 6490.00 & 0.04 \\
  polygon & 1364.00 & 0.20 \\
  quad & 8518.00 & 0.03 \\
  star & 1294.00 & 0.21 \\
  text & 6144.00 & 0.04 \\
  triangle & 2160.00 & 0.12 \\
   \hline
total & 36254.00 & 1.00 \\
   \hline
\end{tabular}
\caption[Class counts and proportions.]{Class counts and proportions. Note that multiple-label images are counted separately for each label. Proportions are passed to keras as class weights to ensure that lower-probability classes are learned with equal attention to the available data.}\label{tab:model-weight}
\end{table}

