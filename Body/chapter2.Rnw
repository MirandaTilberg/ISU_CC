% Chapter 2 of the Thesis Template File
%   which includes bibliographic references.

\Sexpr{knitr::set_parent('../thesis.Rnw')}
\graphicspath{{Images/chapter2/}{../Images/chapter2/}}

<<ch2-setup, fig.keep='all', cache=FALSE, echo=FALSE, eval=TRUE, include=FALSE>>=
options(replace.assign=TRUE,width=70,scipen=3)
require(knitr)

wd <- getwd()
# Set paths for everything for compiling subdocument
if(!"Body" %in% list.files()){
  opts_chunk$set(fig.path='figure/chapter2/fig-', cache.path='cache/', fig.align='center', fig.width=5, fig.height=5, fig.show='hold', par=TRUE, cache=TRUE, concordance=TRUE, autodep=TRUE, root.dir="../", message=F, warning=F, error=F)
  datadir <- "../data/chapter2/"
  imgdir <- "../figure/chapter2/"
} else {
  opts_chunk$set(fig.path='figure/chapter2/fig-', cache.path='cache/', fig.align='center', fig.width=5, fig.height=5, fig.show='hold', par=TRUE, cache=TRUE, concordance=TRUE, autodep=TRUE, message=F, warning=F, error=F)
  datadir <- "data/chapter2/"
  imgdir <- "figure/chapter2/"
}
@
\chapter{DATA AND METHODS}

\section{Data}

\subsection{Geometric Class Characteristics}

Class characteristics, as defined in \autoref{sec:class-chars-desc}, are characteristics which can be used to exclude shoes from a match at a crime scene, but cannot be  used for individualized matching because they are shared by many shoes. A sufficiently well-defined set of characteristics can separate shoes into make and model categories \citep{grossVariabilitySignificanceClass2013}. \citet{grossVariabilitySignificanceClass2013} define geometric features such as circle/oval, crepe, herringbone, hexagon, parallel lines, logo/lettering/numbering, perimeter lugs, star, and other. Working from these categories, we assembled a set of categories which were more suited to recognition by convolutional neural networks, as some of the definitions used in Gross require spatial context which is not preserved during the labeling process (for example, lugs are required to be on the perimeter of the shoe). \autoref{tab:class-char-examples} shows three examples of each class.

\begin{description}
\item [Bowtie] Bowtie shapes are roughly quadrilateral, with two opposite concave faces. The remaining two faces can be convex or straight, and the concave faces may have straight portions, so long as there is a concave region. Using this definition, shapes such as butterflies are included as bowties.
\item [Chevron] Chevron shapes include repeating parallel lines as well as individual ``v" shapes. They may be angular but can also be curved.
\item [Circle] Circles include ellipses and ovals; they must be round.
\item [Line] Lines are repeated and parallel; a more general definition of a line would be difficult to differentiate from many other patterns. Lines can be mildly curved.
\item [Polygon] Polygons are defined in this standard to have more than 4 sides. They include pentagons, hexagons, and octagons.
\item [Quadrilateral] Quadrilaterals (quads) have four sides. They may have rounded or square corners.
\item [Star] Stars are any shape with alternating concave and convex regions, or lines which emanate from a central point. ``X" and ``+" shapes are also classified as stars.
\item [Text] Text is any shape which would be identified as text by a reasonable human. In most cases, the text on our images is made up of latin alphabet charcters; the model will likely not recognize text in other scripts at this point (but could be trained with text in other scripts if such training images could be obtained). Text frequently includes component shapes such as circles; where these shapes are clearly defined, they are labeled as well, as the model does not have the ability at this time to impose the necessary context on images to differentiate an ``o" from a circle.
\item [Triangle] Triangles are any three-sided figure. Like quadrilaterals, they can have rounded corners. In some cases, it is difficult to distinguish between a trapezoidal shape and a triangle when rounded corners are involved.
\item [Other] Other features which were marked include logos, various textures (including crepe, stippling, etc.), and smooth regions with no discernible features. These regions are grouped and provide additional information - that none of the previous nine categories are present.
\end{description}


Defining categories this way does not remove all ambiguities. The best example lies in considering text. The letter "v" can easily be considered a chevron, and the letter "o" is clearly a circle. However, text is also an important category to encompass the variety of ways text appears on footwear outsoles, and it is not necessarily helpful (or possible) to try to categorize every shape in text into another category. Many of the ambiguities that arise can be solved by applying multiple labels to an image, but some shapes also do not fit into any categories. Applying comprehensive and consistent labels to difficult or ambiguous shapes is the most difficult part of this process.

\begin{table}
\centering

\setlength\tabcolsep{1mm}
\begin{tabular}{rccl}
     Bowtie & \raisebox{-.5\height}{\includegraphics[width=.3\linewidth]{class_examples/bowtie_examples.png}} &
     \raisebox{-.5\height}{\includegraphics[width=.3\linewidth]{class_examples/chevron_examples.png}} & Chevron \vspace{1mm}\\
     Circle & \raisebox{-.5\height}{\includegraphics[width=0.3\linewidth]{class_examples/circle_examples.png}} &
     \raisebox{-.5\height}{\includegraphics[width=0.3\linewidth]{class_examples/line_examples.png}} & Line  \vspace{1mm}\\
     Polygon & \raisebox{-.5\height}{\includegraphics[width=0.3\linewidth]{class_examples/polygon_examples.png}} &
     \raisebox{-.5\height}{\includegraphics[width=0.3\linewidth]{class_examples/quad_examples.png}} & Quad  \vspace{1mm}\\
     Star & \raisebox{-.5\height}{\includegraphics[width=0.3\linewidth]{class_examples/star_examples.png}} &
     \raisebox{-.5\height}{\includegraphics[width=0.3\linewidth]{class_examples/text_examples.png}} & Text  \vspace{1mm}\\
     Triangle & \raisebox{-.5\height}{\includegraphics[width=0.3\linewidth]{class_examples/triangle_examples.png}} &
      \raisebox{-.5\height}{} & \\
\end{tabular}
\caption[Geometric elements]{Geometric Elements. Categories modified from \cite{grossVariabilitySignificanceClass2013}.}\label{tab:class-char-examples}
\end{table}

\subsection{Data Collection}

Thousands of outsole images were web-scraped from Zappos.com, a large online shoe retailer. These images were then uploaded for use in a tool called LabelMe \mt{(cite)}, a labeling/annotating interface which allows users to easily select and label regions of an image. To date, about [2,200] shoes have been labeled, yielding about [24,000] multi-label images.

\svp{After annotation using the LabelMe software package, images are processed by an R script, which identifies the minimum bounding rectangle of the region, crops the image to that region, and scales the cropped area to a 256 x 256 pixel image suitable for analysis by the convolutional neural network. During this process, aspect ratio is not preserved, though efforts are made to label images which are relatively square to minimize the effect of this distortion.}
%\mt{Once images are labeled in LabelMe, they are processed in R by a script that employs the imager and spatial packages to chop the labeled image into a 256x256 square pixel region. This script employs a number of strategies to deal with abnormally-shaped labels. For example, while there have been efforts to apply labels to the raw images in relatively square regions, this is not always possible. Regions are thus appropriately rotated and scaled down, but aspect ratio is not necessarily preserved.} % Slight rephrasing for a bit tidier flow of thought.


\subsection{Data Characteristics}

-Quantities, examples, etc
\svp{Show histogram of class distribution}
\svp{Talk about ``other" - it's necessary to train the model on null data as well} \mt{It's already in the description above. Does it need more discussion?}


\mt{Ideally, I'll assemble three "other" examples to add to the class examples above. I'll add it to my to-do list :)}


% \svp{One thing I need to examine is what happens if we use histogram equalization to modify the images before augmentation. I did a bit of that for presentation images and it changed the output probabilities a lot... which suggests it may help with images that don't have even color balance.}

\subsection{Augmentation}
\mt{Augmentation includes crop, zoom, skew, rotate, and color. Images are augmented once. Helps increase amount of data and improves model generalizability.}
\svp{Labeled images are scarce relative to the amount of data necessary to train a neural network. One solution to this scarcity is to artificially enlarge the data set using a process called image augmentation\citep{krizhevskyImageNetClassificationDeep2012}. Augmentation is the transformation of original input data using image operations such as cropping, zoom, skew, rotation, and color balance modification in order to distort or alter the image while maintaining the essential features of the image corresponding to the label. This process reduces the potential for overfitting the model to the specific set of image data used during the training process, and also increases the amount of data available for training. During the model fitting process, images are augmented once using a subset of the augmentation operations discussed above.}

\section{VGG16}

\subsection{Architecture}
Developed by Oxford's Visual Graphics Group, VGG16 is a CNN with 16 "functional" (i.e., convolutional and densely connected) layers and 5 "structural" max-pooling layers. In contrast to other popular networks, like ResNet, VGG has a relatively simple structure that provides easier training and interpretability with very little sacrificed accuracy. \svp{VGG16 is also widely used in transfer learning due to its' structure and broad generalizability. XXX Citations}

% \svp{Contrast VGG16 structure with ResNet - \href{https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624}{one source}, explain VGG16 is simpler, so we can more easily explain it and produce diagnostic images.}

\includegraphics[width = \linewidth]{vgg16-shoe-nolabel.png}

The early convolutional layers of VGG16 contain 64 filters that primarily detect colors and edge patterns. Later convolutional layers of VGG16, in contrast, contain 512 filters that represent much more complex features, like animal fur patterns or distinct bird heads.

\mt{Really want to include bird head images here. When will the KerasVis stuff be ready in R? :)}

\mt{XXX VGG16 follows groups of 2 or 3 convolutional layers with a max-pooling layer, which summarizes the information in the feature maps and scales information down by a factor of 4.}

<<kerasvis, echo = F, eval = F>>=
library(KerasVis)
KerasVis:::import_all_dependendices()
source("/models/shoe_nn/Generate_Model_Images.R")
model_path <- "/models/shoe_nn/TrainedModels/"
newest_model <- get_newest(dir = model_path, pattern = "weights.h5")
model_wts_file <- file.path(newest_model$path, newest_model$base_file)
loaded_model <- set_weights(model_wts_file)
layer_names <- list("block1_conv1")
visualize_filter(loaded_model,
                 selected_filters = list(list(1L:8L)),
                 layer_names = layer_names)
@



\mt{

\subsection{Implementation (Moved here from Ch 3)}

\subsubsection{Computation}

\begin{itemize}
\item Keras in R: Model training was conducted using the *keras* package in R, which provides an interface to the neural network API of the same name which is written in Python, with a TensorFlow computational backend.
\item GPU, big computation power needed, amt of time required?
\end{itemize}

\subsubsection{Model Training Parameters}

\begin{itemize}
\item 60/20/20 data split
\item Training data weighted to equal
\item Drop-out rate
\item Number of epochs and why
\item Train only dense layers
\item Training image here or in results? (I.e. training and validation loss and accuracy plot)
\end{itemize}


}
