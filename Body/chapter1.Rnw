% Chapter 1: Motivation and Background

\Sexpr{knitr::set_parent('../thesis.Rnw')}
\graphicspath{{Images/chapter1/}{figure/chapter1/}}

<<ch1-setup, fig.keep='all', cache = F, echo = F, eval = T, include = F>>=
options(replace.assign = TRUE, width = 70, scipen = 3)
require(knitr)

wd <- getwd()
# Set paths for everything for compiling subdocument
if (!"Body" %in% list.files()) {
  opts_chunk$set(fig.path = 'figure/chapter1/fig-', cache.path = 'cache/',
                 fig.align = 'center', fig.width = 5, fig.height = 5,
                 fig.show = 'hold', par = TRUE, cache = TRUE,
                 concordance = TRUE, autodep = TRUE, root.dir = "../",
                 message = F, warning = F, error = F)
  datadir <- "../data/chapter1/"
  imgdir <- "../figure/chapter1/"
  codedir <- "../code/"
  modeldir <- "../model/"
} else {
  opts_chunk$set(fig.path = 'figure/chapter1/fig-', cache.path = 'cache/',
                 fig.align = 'center', fig.width = 5, fig.height = 5,
                 fig.show = 'hold', par = TRUE, cache = TRUE,
                 concordance = TRUE, autodep = TRUE,
                 message = F, warning = F, error = F)
  datadir <- "data/chapter1/"
  imgdir <- "figure/chapter1/"
  codedir <- "code/"
  modeldir <- "model/"
}
library(tidyverse)
@


\chapter{INTRODUCTION}

\section{Motivation}

In forensic science, shoeprints and outsole characteristics fall into the category of pattern evidence. When a shoeprint or impression is found at a crime scene, the investigator may ask a series of questions. Initially, it may be important to determine the make and model of the shoe, which may help detectives locate comparison shoes from suspects. Later in the investigation, the forensic examiner may consider individualizing characteristics found in the print; that is, small defects that make it possible to tie a specific shoe to the print left at the scene. In cases where such individualizing characteristics are not considered (estimated at 95\% of cases in the United States according to some experts\footnote{Leslie Hammer, presentation to CSAFE on March 5, 2018}), it is \svp{useful} to be able to assess the \svp{\emph{random match probability,} that is, the} probability that the specific model of shoe which made the print would be found \svp{in a random member of the population's possession}. This question is much more difficult than identifying the make and model of the shoe, because it requires that the forensic examiner have access to a database containing information about the frequency of shoes in the local population, where the local population itself may be difficult to define. Any tractable solution to the problem of assessing the random match probability of a shoeprint based only on class characteristics \citep{bodziak_footwear_2000} (make, model, and other characteristics determined during the manufacturing process) requires a way to assemble this database: an automated solution to efficiently classify many types of shoes within a common system. This project is designed to address the computational and statistical process of assembling features which can be used to assess similarity between two or more images, with the goal of producing software which can be integrated into an automatic \svp{system to collect and automatically identify features in images of shoe soles}.


\section{Outsole Class Characteristics}\label{sec:class-chars-desc}

According to \citet{grossVariabilitySignificanceClass2013}, the four generally accepted conclusions that can be made from a footwear examination are elimination, inconclusive, class association, and identification. Typically, the ultimate goal of an examination is identification: matching a shoeprint to an individual shoe, ideally owned by the suspect. This is difficult because identification to a specific individual's shoe requires the matching of randomly acquired characteristics, which occur due to wear and damage, and that information is frequently unavailable due to the quality of the print or impression recovered from the crime scene. Thus, examiners spend most of their time considering class associations to identify one or more shoe models and sizes which are consistent with the recovered print.

Class characteristics are defined as the set of features which allow an object to be placed into a group with other physically similar objects. In the context of footwear, the term refers to the design and physical dimension of the shoe, particularly with regard to the shoe's outsole. While class characteristics are not sufficient for identification, they in many cases enable the exclusion of footwear \citep{bodziak_footwear_2000}.

When categorizing a shoe, it is common to use features like brand, size, and general type (e.g., boot, tennis shoe, dress shoe). \svp{Unfortunately, these features prove quite difficult to use as characteristics for identification or exclusion.} Size, for example, is far from straightforward, as size standards vary significantly across different manufacturers and scales in different countries, and different shoe styles with same size inside may have different size outsoles, making direct measurement or estimation of foot size difficult \citep{bodziak_footwear_2000}. Identifying the model of shoe is also not \svp{trivial} because manufacturers are constantly developing new models, discontinuing existing models, or reviving discontinued models. \svp{There are also } look-alikes for many common models that are difficult to distinguish from the models they emulate. Thus, when defining features to describe any possible shoe outsole that may be found at a crime scene, it is important that any set of descriptors be general enough to describe a large variety of shoes and specific enough to differentiate between shoes that may have similar qualities. One such set of descriptors are geometric shapes, which have been found to be useful in differentiating between different shoes \citep{grossVariabilitySignificanceClass2013}. Matching tread patterns by comparing the spatial distribution of geometric shapes ``is of considerable evidential value" \citep{hancockInterpretationShoeprintComparison2012} for class characteristic comparisons.


\section{Computational Image Analysis and Convolutional Neural Networks}

Shoeprint evidence from crime scenes is most commonly collected in the form of a photograph, so any useful method to automatically identify outsole characteristics must take the form of an image analysis task. There are a number of methods that may be employed to identify shapes and features in an image. The Hough transform is a feature extraction technique carried out in parameter space that was classically used to identify straight lines but has been extended to identifying circles and ellipses, as well as other shapes \citep{ballardGeneralizingHoughTransform1981}. There are a number of \svp{other} low-level feature extraction methods aimed at detecting specific shapes, such as edges, corners, blobs, or ridges \citep[Ch 15]{machineVision}. While these methods are useful in identifying these specific features at a low level, they are very computationally intensive and only identify features on a very small scale; \svp{as a result, they cannot reliably} identify large geometric shapes like those that may be found in an outsole image. These \svp{classical} methods are \svp{also} extremely sensitive to lighting or color changes. \svp{As one or models would be required for each geometric shape, classical methods would require use of additional machine learning models such as random forests} to aggregate low-level features into \svp{functional geometries} found on outsoles.

Convolutional neural networks (CNNs) are \svp{widely recognized as superior for novel image classification}. CNNs are a form of artificial neural network which make use of the image convolution operator used by many low-level feature extraction methods\footnote{The Google Trends interest graph for CNNs and computer vision shows a massive increase in convolutional neural networks between mid-2014 and 2018 \url{https://trends.google.com/trends/explore?date=2010-01-01\%202019-02-18&q=convolutional\%20neural\%20network,computer\%20vision}}. As CNNs have evolved, their architecture has become more complex, but the fundamental reliance on the image convolution \svp{operator} sets CNNs apart from other artificial neural networks \citep{guRecentAdvancesConvolutional2018}.
CNNs have deep architectures that can be trained to identify complex patterns, but they are structurally similar to the human visual architecture and output binary or probabilistic predictions for given labels that are readily interpretable. As CNNs make use of labeled training data, the predictions generated are for features which are similar to those identified by humans, resulting in models with greater face validity. Once a CNN is trained, it is relatively fast and easy to apply the model to new images and obtain classifications.


\subsection{General Approach}\label{sec:general-approach}
Visual classification (i.e., assigning a label to an object based on visual input) is a complex task that humans do very well \citep{mallot2000computational}. Sight is our dominant sense and a significant part of our brain is dedicated to vision, which means that the structures used to impart meaning on a visual scene have been optimized through millions of years of evolution. As convolutional neural networks are organized to mimic the process of object recognition in the human visual cortex, it will be useful to briefly describe that process.

\paragraph{Human Vision}
The visual perception process begins with the transfer of information from the visual world to the brain via rods and cones in the retina. Chemical signals travel along the optic nerve from the retina into the brain, where the signals are processed by a series of biological modules which aggregate information across multiple cells and provide meaning and order on otherwise chaotic chemical and electrical signaling. As information is aggregated, spatial relationships between objects in the physical world are maintained in the brain. Specialized feature detector cells respond preferentially to specific stimuli (e.g. cells which respond to lines oriented horizontally, vertically, or at specific angles), and these feature detectors are aggregated to identify more complex stimuli \citep[Ch. 4]{goldsteinSensationPerception2016}. In addition to the successive compilation of increasingly complex features, there are also specific modules for particularly important tasks, such as facial recognition; information from these regions is also integrated into the overall hierarchy of recognized objects from the visual input.

The problem of general object recognition is quite difficult---a three-dimensional object has infinitely many projections into two-dimensional space, and in real scenes objects are often at least partially obscured. In addition, a two-dimensional image can map back to many different three-dimensional objects, because of the ambiguity introduced by the projection onto a flat surface. Several psychological theories exist as to how object recognition occurs within the brain (gestalt heuristics, recognition-by-components, and inferential contexts all have experimental support), but in general the process seems to require both spatial integration and learned associations \citep[Ch. 5]{goldsteinSensationPerception2016}.

Differentiating between two objects is quite easy when features are distinct; however, there are many cases when differentiating features are rather subtle. For example, as shown in \autoref{fig:caterpillar-carrots}, an orange caterpillar and a carrot may be of similar color, shape, and size, but one is more fuzzy than the other; remarkably, the distinction between the two categories is very strong even with all of the features that are shared. Thus, our brains have learned that when faced with a small, cylindrical orange object, texture is a critically important feature when assigning a label to that object (which keeps us from accidentally ingesting caterpillars).

\begin{figure}[htbp]\centering
\includegraphics[height=.25\linewidth]{caterpillar.png}
\hskip 1cm
\includegraphics[height=.25\linewidth]{carrots.png}
\caption[Feature Similarity.]{A fuzzy caterpillar and a bunch of carrots have many similar visual features, but our brains easily distinguish between them.}\label{fig:caterpillar-carrots}
\end{figure}

\paragraph{Computer Vision Using Neural Networks}
While our brains are adept at parsing images and classifying the objects within them, the task has proved much more difficult for computers, as evidenced by \autoref{fig:xkcd-img-recognition}. Human visual processing is so complex in part because of the successive aggregation of increasingly complex features; only within the last 10 years have we been able to adequately mimic this process with computer modeling.

\begin{figure}[htbp]\centering
\includegraphics[width = .3\linewidth]{xkcd.png}
\caption[Computer vision is a difficult problem.]{Computer vision was thought to be easy in 1966 when a researcher at MIT believed that teaching a computer to separate picture regions into objects and background regions could be completed as a summer project \citep{papert_summer_1966}. The task proved much more difficult than expected, and has only become tractable with convolutional neural network based approaches. Image source: \url{https://xkcd.com/1425/}. }\label{fig:xkcd-img-recognition}
\end{figure}

Convolutional neural networks are a widely implemented method for automated image recognition; their structure typically emulates the successive aggregation of low-level features into higher-level features that is seen in the human visual cortex. CNNs perform comparably to humans on certain image recognition tasks \citep{geirhosComparingDeepNeural2017}. The ImageNet Large Scale Visual Recognition Competition (ILSVRC) is a widely followed contest to produce the best algorithm for image classification; since 2014, it has been dominated by convolutional neural networks \citep{russakovsky_imagenet_2015}. Various models are tested on about 1.2 million images spanning 1000 categories, which are part of the ImageNet image dataset \citep{deng2009imagenet}. These categories range from natural and man-made objects (e.g., daisy, chainsaw) to living creatures (e.g., ring-tailed lemur, sea lion, and dingo). There are also many categories which require subtle distinctions, such as golden retrievers and labrador retrievers, as shown in \autoref{fig:retrievers}.


\begin{figure}[htbp]\centering
\includegraphics[height=.25\linewidth]{labrador-retriever.jpg}
\hskip 1cm
\includegraphics[height=.25\linewidth]{golden-retriever.jpg}
\caption[Feature Similarity.]{The features used to distinguish between two similar categories may be subtle, like the features that would differentiate a golden retriever from a labrador retriever \citep[Images from][]{deng2009imagenet}.}\label{fig:retrievers}
\end{figure}


\subsection{Building Blocks of a Convolutional Neural Network}
CNNs are made up of several distinct types of layers which transition from input image to output class probabilities; the remainder of this section discusses several important layer types which are used in most CNNs.

\paragraph{Image Convolution and Convolutional Layers}
Convolutional neural networks make use of convolutional layers, which use image convolution as a primary operation. Defined mathematically, image convolution is an function performed on an image $x$ using a smaller-dimension matrix $\beta$.

Let \(x\) be an image represented as a numerical matrix, indexed by \(i, j\), and \(\beta\) be a filter of dimension \((2a + 1) \times (2b + 1)\). The convolution of image \(x\) and filter \(\beta\) is \begin{equation}\label{eqn:convolution}(\beta \ast x)(i, j) = \sum_{s = -a}^a\sum_{t = -b}^b \beta(s, t) x(i-s, j-t)\end{equation}

Convolutional neural networks are named to highlight their use of image convolution operations to extract information from an image. As shown in \autoref{fig:image-convolution-setup}, a single convolutional filter is a small array of real valued weights that represents some feature (shown in green). When a filter is applied to a portion of the image (shown in blue), a single value is returned that is associated with the presence of the feature for a given subsection of the input image. When applied over an entire image, the resulting matrix of values maps the strength of the feature across the entire image, as shown in \autoref{fig:image-convolution-illustration}. 

\begin{figure}[bh]
\centering
\includegraphics[width=.5\textwidth]{filter.png}
\caption[Images and convolutional filters.]{An image (blue) and a convolutional filter (green). Image from \protect\citet{prabhuUnderstandingConvolutionalNeural2018}.}\label{fig:image-convolution-setup}
\end{figure}

Once the entire image has been convolved with the filter, the \svp{resulting} feature map is transformed using a nonlinear activation function like those found in \autoref{fig:activation-functions}. A convolutional layer of a CNN takes a large number of these filters and passes them over the image to return one \svp{activation layer} per filter.

\begin{figure}
\centering
\includegraphics[width=.45\textwidth]{filter1.png}\hfill\includegraphics[width=.45\textwidth]{filter2.png}
\caption[Image convolution, illustrated.]{The convolution operation consists of the smaller filter (green) applied to each region of the larger image (blue); each application results in a single value which is stored in the feature map. Image from \protect\citet{prabhuUnderstandingConvolutionalNeural2018}.}\label{fig:image-convolution-illustration}
\end{figure}

Convolutional layers typically do not decrease the dimensions of the matrix by a significant amount, as the filter $\beta$ is typically small: $3\times 3$ or $5\times 5$. Many models pad the input image in order to prevent any reduction in dimension. In these neural networks, dimension reduction is generally performed by pooling layers, which identify the strongest local features in each filter layer of the convolutional output.

\paragraph{Pooling Layers}
Pooling layers are added to reduce the size, and therefore computational load, of feature maps through structured down-sampling. Most commonly, pooling layers apply the maximum function (max pooling) over adjacent regions of a feature map (using a sliding window). This encodes the maximum strength of a feature in a region of an image, while reducing redundant or unnecessary information about less prominent activations.

In most cases, the stride, or offset between subsequent pieces, is the same as the window size (non-overlapping pooling); for this simplified case, the pooling layer values can be calculated using the following relationship, for pooling function $f$, window size $s$, layer $\ell-1$, and output layer $\ell$:
\begin{align}\label{eqn:element-pooling}
x^\ell_{ij} &= f\left(x^{\ell - 1}_{(i - 1)s + 1 \leq y \leq is, (j - 1)s + 1\leq z\leq js} \right)
\end{align}

As a general matrix notation for pooling is difficult to specify intuitively, we will define a pooling operator, $p$, which applies \autoref{eqn:element-pooling} element-wise.
\begin{align}
p\left(x, f, w, s\right) :=&\text{matrix-wise pooling on matrix } x \text{ with function } f, \\& \text{ with window } w, \text{ and stride } s\nonumber
\end{align}

For example, taking 2x2 pieces of a feature map and keeping only the largest of the four values reduces the size of the feature map by a factor of 4, as shown in \autoref{fig:max-pooling}. \citet{krizhevskyImageNetClassificationDeep2012} suggests pooling layers reduce overfitting on certain datasets and increase translation invariance. Using too large of a pooling window can be destructive and limits the total number of convolutional layers which can be combined in a single network \citep{CS231nConvolutionalNeural}. Pooling layers do disrupt the model optimization process; as an alternative, some fully convolutional networks eliminate pooling layers and use convolutional layers with increased stride and window size to reduce layer dimensions \citep{springenbergStrivingSimplicityAll2014}.


\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{maxpooling}
\caption[Max pooling layer.]{Max pooling layer. Image from \protect\citet{prabhuUnderstandingConvolutionalNeural2018}.}\label{fig:max-pooling}
\end{figure}

Once the dimension of the image has been reduced and features have been identified using a combination of convolutional and pooling layers, local features must be integrated into a more unified whole. This integration is performed using densely connected layers.

\paragraph{Activation Functions}
The convolution operation discussed above relies on nonlinear activation functions, which operate on each feature map value. Different activation functions are used to produce different network effects---in the convolutional layers, the desire is often to minimize the computational complexity, so very simple activation functions are used, such as the Rectified Linear Unit (ReLU), Exponential Linear Unit (ELU), or a differentiable analog, SoftPlus. The output layer must map features onto binary predictions or probabilities, so the sigmoid activation function is commonly used for this purpose. \autoref{fig:activation-functions} shows several common nonlinear activation functions.

<<activation-functions, echo = F, fig.scap = "Common nonlinear activation functions used in neural networks.", fig.cap = "Common nonlinear activation functions used in neural networks.", out.width = "\\textwidth", fig.width = 6, fig.height = 2>>=
sigmoid <- function(x) 1/(1 + exp(-x))
relu <- function(x) pmax(x, 0)
elu <- function(x, a) a*(exp(x) - 1) * (x < 0) + x * (x > 0)
softplus <- function(x) log(1 + exp(x))

x <- seq(-3, 3, .005)
tibble(x = x, Linear = x, Sigmoid = sigmoid(x),
       ReLU = relu(x), ELU = elu(x, 1), SoftPlus = softplus(x)) %>%
  gather(key = key, value = value, -x) %>%
  mutate(key = factor(key, levels = c("Linear", "ReLU", "ELU",
                                      "SoftPlus", "Sigmoid"))) %>%
  ggplot(aes(x = x, y = value)) + geom_line() + facet_grid(.~key) +
  ggtitle("Common Activation Functions") +
  theme(axis.title = element_blank()) +
  coord_fixed()
@

Activation functions are also used in densely connected layers, which are discussed in the next section. Frequently, the same activation function is used throughout a CNN, though it is (mathematically) possible to use a different activation function for each layer.

\paragraph{Densely Connected Layers}
Densely connected layers are typically the final layers in a CNN, making up what is known as the model head. These layers form the meaningful connection between the features of an image (detected by convolutional and max pooling layers) and the corresponding labels associated with the image. Just as humans learn which combinations of features should be associated with a given label, densely connected layers use real-valued weights to represent these associations. For example, an item which is orange, small, and fuzzy is commonly associated with the word ``caterpillar". As seen in \autoref{fig:caterpillar-carrot-nodes}, fuzzy is not a feature typically associated with the word carrot, so there is little connection between the feature ``fuzzy" and the label ``carrot".

<<caterpillar-carrot-nodes, fig.height = 3, fig.width = 3, echo = F, fig.cap = 'A representation of how different features are connected to labels for classification. Note that the feature ``fuzzy" is connected to the notion of caterpillar but not to carrot, and ``pointy" is only related to carrots.' , fig.scap = "A representation of the connections between features and classification labels.">>=
source(file.path(codedir, "Generate_Model_Images.R"))
col_vec <- rep("grey50", 32)
col_vec[seq(1, 29, by = 4)] <- rep(c("blue", "orange"), 4)

lty <- rep("solid", 32)
lty[c(21,25)] <- "dashed"

plot <- plot_deepviz2(c(4,2), edge_col = col_vec, line_type = lty)

df <- data.frame(x = plot$data$x,
                 y = plot$data$y,
                 labs = c("Orange", "Long", "Fuzzy", "Pointy",
                          "Caterpillar", "Carrot"))

plot + geom_label(aes(x = x, y = y, label = labs), data = df)
@

Similarly, in densely connected or fully connected layers, each final feature produced by the convolutional and pooling layers is connected to each possible label through weights (hence the name ``densely connected") learned during the training process. Weights are optimized via backpropagation (discussed in \autoref{sec:prop}) in order to minimize errors (measured by a cost function) and improve classification accuracy.

When training fully connected layers there is a danger that co-dependence will develop between nodes, which lessens the power of each individual node and usually leads to over-fitting. To prevent this, fully connected layers are often trained by utilizing a pruning mechanism known as drop-out, where at any given stage each node is kept in the model with probability $p$ or temporarily disabled with probability $1-p$ and training is performed on the reduced model. Densely connected layers with and without dropout nodes are shown in \autoref{fig:dense-layer}.

<<dense-layer, echo = F, fig.width = 5, fig.height = 3, out.width = ".45\\textwidth", fig.cap = "(Left) A densely-connected layer with 12 input nodes and 4 output nodes. (Right) A densely-connected layer with 12 input nodes, 4 output nodes, and a 50\\% dropout rate.", fig.scap = "Densely connected layers with and without dropout.", fig.show =  'hold'>>=
source(file.path(codedir, "Generate_Model_Images.R"))
plot_deepviz2(c(12, 4), r = .005)

plot_deepviz_sample(c(12, 4), r = .005, dropout_rate = 0.5) +
  theme(legend.position = c(1, 0), legend.justification = c(1, 0))
@

\paragraph{Output Classification Layer}
In order to transform the neural network node values into output class probabilities, the final layer of the model uses an activation function selected to conform to the problem specifications. For instance, if the goal is to perform binary classification, the sigmoid activation function shown in \autoref{fig:activation-functions} will map any real valued number to a value between 0 and 1 (that is, to a class probability). In a multinomial classification problem, an extension of the sigmoid activation function, the \emph{softmax} activation function, is used: For inputs $y_i$, $\displaystyle S(y_i) = \frac{e^{y_i}}{\sum_j e^{y_j}}.$ The softmax activation function produces class probabilities which sum to 1. In classification problems where there are $n$ classes, but each object can have between 0 and $n$ assigned labels (i.e., a multi-class, multi-label problem), the sigmoid activation function can be used for each class label separately, producing a $n$-dimensional vector of probabilities between 0 and 1.


\subsection{Forward and Backward Propagation}\label{sec:prop}
\paragraph{Forward Propagation}
In order to evaluate an input image, it is necessary to move from the image representation through each of the layers in the network, with a final result of a set of $n$ output class probabilities.

We first define some notation. Let $x^{(0)}$ be an input image, represented as a numerical matrix with two dimensions of length and height, and additionally a third dimension representing the color channels, if the input image is in color. Let $x^\ell$ be the layers in the network, that is, $x^1$ is the first (convolutional) layer, $x^2$ is the second, and so on. Convolutional layers are assembled from a set of filters, $\beta^\ell_k$, where there are a set of $p^\ell$ $m\times m$ filters convolved with $x^{\ell-1}$ to create layer $x^\ell$. Each convolutional layer also has a bias matrix $\gamma^\ell$ which is used in the calculation of all filters in the $\ell$-th layer. We additionally define $\sigma^\ell(\cdot)$ as the nonlinear activation function used in layer $\ell$ (some common nonlinear activation functions are shown in \autoref{fig:activation-functions}). Finally, we define $W^\ell$ to be a weight matrix used in fully connected layer $\ell$; $W$'s dimensions are chosen such that the output dimension is equal to the prespecified number of output classes.

During forward propagation, the calculation of the $\ell$th layer uses the $\ell-1$th layer in an iterative process:
\begin{align}\label{math:forwardprop}
x^{(\ell)}_k &= \sigma^\ell\left({\beta^\ell_k}\ast x^{(\ell-1)} + \gamma^\ell\right) \text{ for convolution}\\
x^{(\ell)}_k &= p\left(x^{(\ell-1)}, \max, s, s\right) \text{ for max pooling layers}\nonumber\\
x^{(\ell)}_k &= \sigma^\ell\left(Wx^{(\ell-1)}\right) \text{ for densely connected layers}\nonumber
\end{align}

\paragraph{Loss and Cost Functions}

In a multi-label classification task, the standard loss function used is binary cross-entropy loss. For a single input image $x^{(0)}$ with true labels $\mathbf{y} \in \left\{0,1\right\}^C$ and prediction $\mathbf{\hat{p}} \in [0,1]^C$, where $C$ is the number of output classes,

\begin{align}
L(\mathbf{y}, \mathbf{\hat{p}}) &= \sum_{i=1}^C -\left[y_i \log(p_i) + (1 - y_i)\log(1-p_i) \right]
\end{align}

The cost function is defined as the average of the loss function over all training images.

\paragraph{Backpropagation}
Backpropagation, short for ``the backward propagation of errors", is the name of the optimization algorithm that is used to train deep neural networks. In the process, the error of each output is calculated and then distributed backwards through the network's layers; weights are updated in order to reduce the errors for the next iteration of the algorithm. In essence, the goal of backpropagation is to use gradient descent to adjust the parameters of the network to achieve a local minimum of the cost function, where the gradient is computed through repeated application of the chain rule.

For convolutional layers, backpropagation works using the recurrence relationship in \autoref{eqn:backprop-conv}.

\begin{align}\label{eqn:backprop-conv}
\left(\frac{\partial L}{\partial \beta^\ell_k}\right) &= \underbrace{\frac{\partial L}{\partial \left(\beta^\ell_k \ast x^{\ell - 1}\right)}}_\text{gradient} x^{\ell-1}\\
\frac{\partial L}{\partial \left(\beta^\ell_k \ast x^{\ell - 1}\right)} &= \frac{\partial L}{\partial x^\ell} \left[\sigma'\left(\beta^\ell_k \ast x^{\ell - 1}\right)\right]\nonumber
\end{align}

There is no backpropagation through pooling layers because there are no weights to optimize, so these layers are just pass-through layers. Backpropagation through fully connected layers takes place similar to \autoref{eqn:backprop-conv}, shown in \autoref{eqn:backprop-fc}.
\begin{align}\label{eqn:backprop-fc}
\left(\frac{\partial L}{\partial W^\ell}\right) &= \underbrace{\frac{\partial L}{\partial \left(W^\ell x^{\ell - 1}\right)}}_\text{gradient} x^{\ell-1}\\
\frac{\partial L}{\partial \left(W^\ell x^{\ell - 1}\right)} &= \frac{\partial L}{\partial x^\ell} \left[\sigma'\left(W^\ell x^{\ell - 1}\right)\right]\nonumber
\end{align}


\subsection{Transfer Learning}
Convolutional layers and max pooling layers in a CNN are analogous to the human visual perception process, and densely connected layers behave like the human brain. In short, the approach to classifying an image is to detect the features in the image, like our eyes do, and then assign labels to combinations of those features, like our brains do. This analogy is also appropriate because it reflects the difficulty of the task: it takes many years and a significant amount of effort for humans to learn how to distinguish a large variety of features and also to connect those features to labels that are often complex, hierarchical, and subtle. Similarly, training a CNN is no small task. Even relatively simple CNNs can have many millions of trainable parameters in the convolutional and densely connected layers. Optimizing all of these weights requires an incredible amount of computational power. In addition, the features learned by a neural network trained on one dataset often generalize to different data sets: particularly in the initial layers, the feature maps of a trained neural network typically activate based on color, low-level textures, and other features which are broadly generalizable \citep{yosinskiHowTransferableAre2014}. Although later layers detect more complex and specific features, there are usually a larger number of these filters, so the \svp{huge number of} possible combinations of many specific filters creates a model base that is generalizable to new image classification tasks. Examples of the filters found at several different layers of a trained network are shown in \autoref{subsec:vgg16-filters}; it is clear that the layers shown in \autoref{fig:b5} are more complex than \autoref{fig:b1}, but that combinations of filters from \autoref{fig:b5} are able to detect a wide variety of features that the original model may not have been explicitly trained to recognize.

\emph{Transfer learning} is the process of using layers from a CNN (or other classification model) trained on a general image recognition task when fitting a model intended for a more specific purpose. The layers which are deemed to identify broadly generalizable patterns are used with their pre-trained weights; new layers are added to customize the model to the specific task at hand, and typically, only these new weights are updated when the model is fit. In many cases, the entire model base is used, and fitted with a new model head; the modularity of CNNs makes this process relatively simple, and allows researchers to leverage pre-trained networks when working with different sets of image data. Transfer learning allows CNNs to be applied to smaller datasets of several thousand images \svp{and also reduces the amount of computational time} required to fit the model.

\begin{figure}
\centering
\includegraphics[width=.95\textwidth]{vgg16-base-head}
\caption[Transfer learning using pre-trained neural networks.]{This diagram is for a pre-trained convolutional neural network. The model base consists of 5 convolutional blocks; the model head consists of several fully connected layers capped with an activation layer which transforms the aggregate visual input to output class probabilities. During transfer learning, the model base weights are fixed to the values derived from the initial input material used to train the original model; only the weights in the model head are retrained to accommodate the input training data.} \label{fig:transfer-learning-diagram}
\end{figure}

Transfer learning leverages the modularity of neural networks---the pretrained base of the model can be separated from the full model and a new model head can be trained to connect that base to the output classes, as shown in \autoref{fig:transfer-learning-diagram}.


\section{Machine Learning Model Evaluation}\label{sec:model-eval}

Classification tasks are considered single-class when there is one decision \svp{between two mutually exclusive classes}, and multi-class when there \svp{are more than two} classes that an object may belong to. While the true classification is a binary decision, it is common for models to predict these classifications using probability, thus reporting a value between 0 and 1 reflecting how certainly the item can be attributed to a given class. \svp{In multi-class problems, the true classification and output probabilities are represented as vectors, with length corresponding to the number of classes. When the classes are mutually exclusive, the output probabilities sum to 1.}

Multi-label classification is \svp{a} special case of multi-class \svp{classification problems} where categories are not mutually exclusive, that is, an item may fall into a combination of categories simultaneously. \svp{In multi-label classification problems, classes are not mutually exclusive, so the the model output is a vector of probabilities which are each between 0 and 1; output probabilities do not  sum to 1 in this case.}

Evaluating model accuracy in classification problems requires addressing both the labels that are assigned and the labels that are not. Ideally, an image is labeled perfectly, and a true positive occurs when the model assigns a label which matches that of the image. A false positive in this scenario occurs when the model assigns a label which does not match that of the image. A true negative occurs if the model does not assign a label which does not occur in the image, and a false negative occurs when the model does not assign a label which does occur in the image. \autoref{tab:error-binary-problem} illustrates these terms for a simple binary classification problem.

\begin{table}
\centering
\begin{tabular}{cccc}
 & & \multicolumn{2}{c}{Model---Assigned Label} \\\cline{3-4}
 & & A & Not A \\\hline
 \multirow{2}{*}{True Label} & A & True Positive & False Negative\\\cline{2-4}
 & Not A & False Positive & True Negative \\\hline
\end{tabular}
\caption[Model errors for a two-class binary decision problem.]{Model errors for a two-class binary decision problem. Correct decisions are shown along the diagonal; incorrect decisions are in the off-diagonal cells.}\label{tab:error-binary-problem}
\end{table}

\emph{Recall}, or \emph{sensitivity}, which is the true positive rate, is the number of all true positives divided by the number of positive cases in the data. \emph{Specificity}, which is the true negative rate, is the number of all true negatives divided by the number of negative cases in the data. \emph{Precision} is the sum of all true positives divided by the number of positive predictions made by the model.


<<example-roc, echo = F, fig.scap = "ROC curve plots, showing model performance.", fig.cap = "ROC curves, showing the performance of a random forest to predict iris species from sepal length from Fisher's Iris Data. Note that prediction is very accurate for species Setosa, but is not much better than random chance for species Versicolor.", fig.width = 9, fig.height = 3>>=

library(pROC)
mod <- randomForest::randomForest(y = iris$Species,
                                  x = iris[,"Sepal.Length", drop = F])
spec <- unique(iris$Species)

roc_list <- purrr::map(spec, ~roc(iris$Species == ., mod$votes[,.]))

roc_df <- purrr::map_df(roc_list, ~tibble(tpr = .$sensitivities,
                                          fpr = 1-.$specificities,
                                          thresholds = .$thresholds,
                                          auc = .$auc[1]),
                        .id = "species") %>%
  nest(tpr, fpr, thresholds, .key = "roc_plot") %>%
  mutate(eer = purrr::map(roc_plot, eer)) %>%
  mutate(species = as.character(spec)[as.numeric(species)] %>% str_to_title)

ggplot() +
  facet_grid(.~species) +
    geom_line(aes(x = fpr, y = tpr), data = unnest(roc_df, roc_plot) %>%
                arrange(fpr, tpr), size = 1.25) +
    geom_text(aes(x = 1, y = .15, label = sprintf("AUC: %0.2f", auc)),
               hjust = 1, vjust = -0.2, data = roc_df) +
    geom_point(aes(x = fpr, y = tpr, color = "Equal Error Rate"),
               data = unnest(roc_df, eer), size = 4) +
    scale_color_manual("", values = "red") +
    scale_x_continuous("False Positive Rate",
                       breaks = c(0, .25, .5, .75, 1),
                       labels = c("0.0", "", "0.5", "", "1.0")) +
    scale_y_continuous("True Positive Rate",
                       breaks = c(0, .25, .5, .75, 1),
                       labels = c("0.0", "", "0.5", "", "1.0")) +
    ggtitle("ROC Curves for Iris Species Prediction") +
    coord_fixed() +
    theme_bw() +
    theme(legend.position = c(1, 0),
          legend.justification = c(1.01, -0.02),
          legend.title = element_blank(),
          legend.background = element_rect(fill = "transparent"),
          axis.text.y = element_text(angle = 90, hjust = 0.5))
@

ROC curves plot the false positive rate (1 - specificity) against the true positive rate. Ideally, a model will have a high true positive rate and a low false positive rate, so a perfect ROC curve would hug the top-left corner of the graph; a straight line between corners would indicate that the prediction is no better than random chance. The Area Under the Curve (AUC) quantifies the shape of the ROC curve, with an AUC of 1 corresponding to perfect prediction and 0.5 indicating random chance. Examples of ROC curves are shown in \autoref{fig:example-roc} for a random forest model to predict species for Fisher's Iris Data from sepal length. Typically, ROC curves are used for single-label, or binary, classification; \autoref{fig:example-roc} follows this convention by showing a curve for each species being predicted. ROC curves have been extended to multi-class problems, but in most multi-class extension methods \citep{handSimpleGeneralisationArea2001}, it is not easy to see how the model performs for each class; this issue is exacerbated when classes are unbalanced.

<<example-conf-mat, echo = F, fig.height = 6, fig.width = 6, fig.scap = "Confusion matrix, showing model performance.", fig.cap = "Confusion matrix, showing the performance of a random forest to predict iris species from sepal length from Fisher's Iris Data. Again, it is clear that prediction is very accurate for species Setosa, but the model is not always able to distinguish between the species Versicolor and Virginica.", out.width = ".75\\textwidth">>=
mod$confusion[,1:3] %>%
  as.data.frame() %>%
  sweep(., MARGIN = 1, STATS = colSums(.), FUN = "/") %>%
  set_names(str_to_title(spec)) %>%
  set_rownames(str_to_title(spec)) %>%
  ggcorrplot(., hc.order = F, outline.col = "white", lab = T) +
  scale_fill_gradient("Classification\nRate", low = "white",
                      high = "cornflowerblue", limits = c(0, 1)) +
  scale_x_discrete("True Species") + scale_y_discrete("Prediction") +
  theme(axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14, angle = 90, vjust = 1)) +
  ggtitle("Prediction Accuracy of Species by Sepal Length") +
  theme_bw() +
  theme(axis.text.y = element_text(angle = 90, hjust = .5)) +
  # theme(panel.grid.major = element_line(color = "grey50"),
  #       panel.grid.minor = element_line(color = "grey60")) +
  theme(plot.margin = grid::unit(c(0,0,0,0), "mm"),
        plot.background = element_rect(fill = "transparent", color = NA),
        plot.subtitle = element_blank(), plot.caption = element_blank(),
        panel.spacing = unit(c(0, 0, 0, 0), "mm"))
@

A confusion matrix is a cross-tabulation between the observed and the predicted classes of the data. A confusion matrix can help identify which classes are being correctly predicted and which classes are commonly mixed-up with others. Confusion matrices are typically used with binary classification problems; an aggregate version of \autoref{tab:error-binary-problem} would be a confusion matrix, where totals for each decision would be shown in the matrix cells. Confusion matrices do not extend easily to multi-class problems because any single miscategorization produces both a false negative decision and a false positive decision; it is not clear which should be shown in each cell, as shown in \autoref{tab:error-3class-problem}.

\begin{table}
\centering
\begin{tabular}{ccccc}
 & & \multicolumn{3}{c}{Model-Assigned Label} \\\cline{3-5}
 & & A & B & C \\\hline
 \multirow{6}{*}{True Label} & \multirow{2}{*}{A} & \multirow{2}{*}{True Positive (A)} & False Positive (B) & False Positive (C) \\
 & & & False Negative (A) & False Negative (A)\\\cline{2-5}
 & \multirow{2}{*}{B} & False Positive (A) & \multirow{2}{*}{True Positive (B)} & False Positive (C) \\
 & & False Negative (B) & & False Negative (B) \\\cline{2-5}
 & \multirow{2}{*}{C} & False Positive (A) & False Positive (B) & \multirow{2}{*}{True Positive (C)} \\
 & & False Negative (C) & False Negative (C) & \\\hline
\end{tabular}
\caption[Model errors for a three-class binary decision problem.]{Model errors for a three-class binary decision problem. Correct decisions are shown along the diagonal; incorrect decisions are in the off-diagonal cells. Note that for each misclassification, two incorrect decisions are made---the omission of the correct label and the addition of an incorrect label.}\label{tab:error-3class-problem}
\end{table}

However, in some situations, it is possible to reduce the classification problem to a series of binary classifications; in these situations, which will be described in more detail in Chapter~\ref{ch3:model-accuracy}, a modification of the confusion matrix is possible to better visualize model performance.
