% Chapter 1 of the Thesis Template File

\Sexpr{knitr::set_parent('../thesis.Rnw')}
\graphicspath{{Images/chapter1/}{figure/chapter1/}}

<<ch1-setup, fig.keep='all', cache=FALSE, echo=FALSE, eval=TRUE, include=FALSE>>=
options(replace.assign=TRUE,width=70,scipen=3)
require(knitr)

wd <- getwd()
# Set paths for everything for compiling subdocument
if(!"Body" %in% list.files()){
  opts_chunk$set(fig.path='figure/chapter1/fig-', cache.path='cache/', fig.align='center', fig.width=5, fig.height=5, fig.show='hold', par=TRUE, cache=TRUE, concordance=TRUE, autodep=TRUE, root.dir="../", message=F, warning=F, error=F)
  datadir <- "../data/chapter1/"
  imgdir <- "../figure/chapter1/"
} else {
  opts_chunk$set(fig.path='figure/chapter1/fig-', cache.path='cache/', fig.align='center', fig.width=5, fig.height=5, fig.show='hold', par=TRUE, cache=TRUE, concordance=TRUE, autodep=TRUE, message=F, warning=F, error=F)
  datadir <- "data/chapter1/"
  imgdir <- "figure/chapter1/"
}
@

\chapter{INTRODUCTION}

\section{Motivation}

In forensic science, shoe prints and outsole characteristics fall into the category of pattern evidence. When a shoe print or impression is found at a crime scene, the investigator may ask a series of questions. Initially, it may be important to determine the make and model of the shoe, which may help detectives locate comparison shoes from suspects. Later in the investigation, the forensic examiner may consider individualizing characteristics found in the print; that is, small defects that make it possible to tie a specific shoe to the print left at the scene. In cases where such individualizing characteristics are not considered (estimated at 95\% of cases in the United States according to some experts\footnote{Leslie Hammer, presentation to CSAFE on March 5, 2018}), it is important to be able to assess the probability that the specific model of shoe which made the print would be found in the suspect's posessions. This question is much more difficult than identifying the make and model of the shoe, because it requires that the forensic examiner have access to a database containing information about the frequency of shoes in the local population, where the local population itself may be difficult to define. Any tractable solution to the problem of assessing the random match probability of a shoeprint based only on class characteristics \citep{bodziak_footwear_2000} (make, model, and other characteristics determined during the manufacturing process) requires a way to assemble this database: an automated solution to efficiently classify many types of shoes within a common system. This project is designed to address the computational and statistical process of assembling statistical features which can be used to assess similarity beteween two or more images.

% \section{Background}

\section{Outsole Class Characteristics}\label{sec:class-chars-desc}

According to \citet{grossVariabilitySignificanceClass2013}, the four generally accepted conclusions that can be made from a footwear examination are identification, elimination, class association, and inconclusive. Typically, the ultimate goal of an examination is identification, which is matching a shoe print to an individual shoe, perhaps owned by a suspect of an the investigation. This is difficult because identification to a specific individual's shoe requires the matching of randomly acquired characterisics, such as damage, and often times that information is not available from the print left at the scene due to quality of the print or the impression or image of the print. Most of the work being done with shoeprints involves class association \mt{cite Leslie Hammer?}, which means identifying a group of shoe types that could have made the print.

Class characteristics are defined as the set of features which allow an object to be placed into a group with other physically-similar objects. In the context of footwear, the term refers to the design and physical dimension of the shoe, particularly with regard to the shoe's outsole. While class characteristics are not sufficient for identification, they in many cases enable the exclusion of footwear \citep{bodziak_footwear_2000}.

When categorizing a shoe in daily life, it is common to use features like brand, size, and general type (e.g., boot, tennis shoe, dress shoe). When defining useful class characteristics for identification or exclusion, however, these features prove quite difficult to use. Size, for example, is far from straightforward. Size standards vary significantly across different manufacturers and scales in different countries, and different shoe styles with same size inside may be differently sized outside, making direct measurement or estimation of foot size difficult \citep{bodziak_footwear_2000}. Model of shoe is also not easily characterized due to the large number of look-alikes and the constantly changing market. Thus, when defining features to describe any possible shoe outsole that may be found at a crime scene, it is important to be both general enough to describe a large variety of shoes and specific enough to differentiate between shoes that may have similar qualities. This provides ample motivation for using geometric shapes to describe outsole designs, as geometric elements have been found to be able to differentiate between different shoes \citep{grossVariabilitySignificanceClass2013} and pattern matching "is of considerable evidential value" \citep{hancockInterpretationShoeprintComparison2012}.

\section{\svp{Computational} Image Analysis and Convolutional Neural Networks}

Shoeprint evidence from crime scenes is most commonly collected in the form of a photograph, so any useful method to automatically identify outsole characteristics must take the form of an image analysis task. There are a number of methods that may be employed to identify shapes and features in an image. The Hough transform is a feature extraction technique carried out in parameter space that was classically used to identify straight lines but has been extended to identifying circles and ellipses, as well as other shapes (cite Wikipedia, lol). In addition, there are a number of low-level feature extraction methods aimed at detecting specific shapes, such as edges, corners, blobs, or ridges. While these methods are useful in identifying these specific features at a low level, they are very computationally intensive and features they produce/identify exist on a very small scale, often only a few pixels wide, and they are thus not able to identify large geometric shapes like those that may be found in an outsole image. Furthermore, these methods are extremely sensitive to lighting or color changes, and thus require additional modeling (like random forests) to aggregate low-level features into the geometric shapes found on outsoles.

For novel image classification tasks, Convolutional Neural Networks (CNNs) have become \svp{standard}
\footnote{The Google Trends interest graph shows a massive increase in convolutional neural networks between mid-2014 and 2018 \url{https://trends.google.com/trends/explore?date=2010-01-01\%202019-02-18&q=convolutional\%20neural\%20network,computer\%20vision}}
\svp{\citep{guRecentAdvancesConvolutional2018}}. CNNs have deep architectures that can be trained to identify complex patterns, but they are structurally similar to the human visual architecture and output binary or probabilistic predictions for given labels that are readily interpretable by humans. As CNNs make use of labeled training data, the predictions generated are for features which are similar to those identified by humans, providing greater face validity to the model. Once a CNN is trained, it is relatively fast and easy to apply the model to new images and obtain classifications.

% \subsection{Convolutional Neural Networks}

\subsection{General Approach}
Visual classification (i.e., assigning a label to an object based on visual input) is a complex task that humans do very well \citep{mallot2000computational}. Sight is our dominant sense and a significant part of our brain is dedicated to vision, which means that the structures used to impart meaning on a visual scene have been optimized through millions of years of evolution. As Convolutional Neural Networks are organized to mimic the process of object recognition in the human visual cortex, it will be useful to briefly describe that process.


\mt{I will read Goldstein this week and fill in this paragraph.}
(Describe the human visual/classification process: feature detection, routing to the brain, and label application) \svp{Use Sensation \& Perception (Goldstein) heavily here. You might also use some of the computer vision books to compare/contrast.} \svp{Skip the retina, talk briefly about "feature detectors" e.g. Hubel and Wiesel (flag 1). Describe how those feature detectors are assembled into progressively higher-order features (flag 2), and then talk about special processing regions (flag 3). Describe why object recognition is hard(things look different from different angles, general class vs. specifics, etc.) and talk about recognition-by-components (flag 4). The assembly of these components into an intelligent framework is what makes human object recognition so much more advanced (flag 5), and only recently have we been able to mimic that level of organizational structure with computer modeling. You can integrate the caterpillar/carrot example here with feature integration and recognition-by-components (we're using a more general definition of components than the geom-based components in the text)}

Differentiating between two objects is quite easy when features are distinct; however, there are many cases when differentiating features are rather subtle. For example, \svp{as shown in \autoref{fig:caterpillar-carrots},} an orange caterpillar and a baby carrot may be of similar color, shape, and size, but one is distinctly more fuzzy than the other. Thus, our brains have learned that when faced with a small, cylindrical orange object, texture becomes an important feature when assigning a label to that object (which keeps us from accidentally ingesting caterpillars).

\vskip .2cm

\begin{figure}[htbp]\centering
\includegraphics[height=.25\linewidth]{caterpillar.png}
\hskip 1cm
\includegraphics[height=.25\linewidth]{carrots.png}
\caption{\svp{A fuzzy caterpillar and a bunch of carrots have many similar visual features, but our brains easily distinguish between them.}}\label{fig:caterpillar-carrots}
\end{figure}

While our brains are adept at parsing images and classifying the objects within them, the task has proved much more difficult for computers, as evidenced by \autoref{fig:xkcd-img-recognition}.

\begin{figure}[htbp]\centering
\includegraphics[width = .3\linewidth]{xkcd.png}
\caption{Computer vision was thought to be easy in 1966 when a researcher at MIT believed that teaching a computer to separate picture regions into objects and background regions could be completed as a summer project \citep{papert_summer_1966}. The task proved much more difficult than expected, and \svp{has only become tractable with convolutional neural network based approaches.}}\label{fig:xkcd-img-recognition}
\end{figure}

Now, CNNs are a widely implemented method for automated image recognition and perform comparably to humans on certain tasks \citep{geirhosComparingDeepNeural2017}. The ImageNet Large Scale Visual Recognition Competition (ILSVRC) is a widely followed contest to produce the best algorithm for image classification; since 2014, it has been dominated by convolutional neural networks \citep{russakovsky_imagenet_2015}.

Convolutional neural networks (CNNs) are a tool for supervised deep-learning that have become standard in recent years for automatic image classification. CNNs are a form of artificial neural network, which were inspired by biological processes in the brain \citep{ANNasModelsofNeuralInfoProcessing}. CNNs primarily use combinations of convolutional and pooling layers to filter raw information into features. These features are then fed into densely connected layers which are trained to associate given sets of features with their desired labels. This translation-invariant automated classification closely mimics the human eye-to-brain classification process.

\subsection{Building Blocks of a Convolutional Neural Network}
\svp{Discuss generally - convolutional neural networks are a specific type of artificial neural network which are used to work with image data. Images are represented as numerical matrices.... The use of convolutional layers is particularly useful for images and is what gives CNNs their power. Then transition into defining convolution and convolutional layers in the next (labeled paragraph)}

\svp{CNNs are made up of several distinct types of layers which transition from input image to output class probabilities; in the remainder of this section, we discuss several important layer types which are used in most CNNs.}

\paragraph{Image Convolution and Convolutional Layers}
\svp{Convolutional Neural Networks make use of convolutional layers, which use image convolution as a primary operation. Defined mathematically, image convolution is an function performed on an image $x$ using a smaller-dimension matrix $\beta$.}

Let \(x\) be an image represented as a numerical matrix, indexed by \(i, j\), and \(\beta\) be a filter of dimension \((2a + 1) \times (2b + 1)\). The convolution of image \(x\) and filter \(\beta\) is $$(\beta \ast x)(i, j) = \sum_{s = -a}^a\sum_{t = -b}^b \beta(s, t) x(i-s, j-t)$$

Convolutional Neural Networks are named to highlight their use of convolution to extract information from an image. A single convolutional filter is a small array (say 5x5x3) of real valued weights that represents some feature of the image. When a filter is applied to a portion of the image, a single value is returned that is associated with the presence of the feature for a given subsection of the input image. When applied over an entire image, the resulting matrix of values maps the strength of the feature across the entire image. A convolutional layer of a CNN takes a large number of these filters and passes them over the image to return one feature map per filter.

\svp{Add in pictures showing convolutional layer, activation map. Be sure to talk about the fact that there is a nonlinear activation function (ReLU) applied - this is what makes the convolution operation part of a neural network.}

\svp{Convolutional layers typically do not hugely decrease the dimensions of the matrix; dimension reduction is typically performed by pooling layers, which identify the strongest local features in each filter layer of the convolutional output.}

\paragraph{Max Pooling Layers}
\svp{You could talk more generally about pooling layers instead of max pooling specifically if you want, but you don't have to.}
Max-pooling is a technique to reduce the size, and therefore computational load, of feature maps through structured down-sampling. Max-pooling layers apply a maximum function over adjacent regions of a feature map (like using a sliding window) to encode the important information of how strongly a feature was activated in a given region of the image while simultaneously reducing redundant or unneccessary information about smaller activations. For example, taking 2x2 pieces of a feature map and keeping only the largest of the four values reduces the size of the feature map by a factor of 4! Max-pooling is also beneficial in that it allows CNN "vision" to be relatively translation invariant, because it emphasizes the relative position of a feature rather than its absolute position.

\svp{Once the dimension of the image has been reduced and features have been identified using a combination of convolutional and pooling layers, local features must be integrated into a more unified whole. This integration is performed using densely connected layers.}

\paragraph{Densely Connected Layers}
Densely connected layers are typically the final layers in a CNN. These layers form the meaningful connection between the features of an image (detected by convolutional and max-pooling layers) and the corresponding labels associated with the image. These layers act like the human brain: just as we learned which combinations of features should be associated with a given label, densely connected layers use real-valued weights to represent these associations. For example, if we see an item that is orange, small, and fuzzy, we are taught to call it "caterpillar". Fuzzy is not a feature we meaningfully associate with a baby carrot, so there is little connection between the feature "fuzzy" and the label "carrot". Similarly, in CNNs, each final feature is connected to each label through a weight (hence the name "densely connected"), and those weights are learned through the training process (using an algorithm called back-propogation) to minimize loss and thus improve classification accuracy. \svp{Go ahead and add the pictures in here - you have them, may as well use them...}

\paragraph{Output Classification Layer}
\mt{Include a description of } \svp{sigmoid} \mt{ and softmax and when each is used.}


\subsubsection{Transfer Learning}
\svp{You may also want to talk about "modularity" in that the Convolutional part is one module, the head is another, and they're separately trainable and useable}

\mt{This paragraph is another beast for another night. Goal: establish the need for transfer learning by explaining the problem of training from scratch and showing how much easier the task is without it}

As we have just seen, convolutional layers and max-pooling layers in a CNN are analogous to the human visual perception process, and densely connected layers behave like the human brain. In short, the approach to classifying an image is to detect the features in the image (like our eyes) and then assign labels to combinations of those features (brain). This analogy is also appropriate because it reflects the difficulty of the task: it takes many years and a significant amount of effort for humans to learn how to distinguish a large variety of features and also to connect those features to labels that are often complex, hierarchical, and subtle. Similarly, training a CNN is no small task. \svp{Even relatively simple convnets can have many millions of trainable parameters in the model base (the convolutional and pooling layers). Optimizing all of these weights requires an incredible amount of computational power. In addition, the features learned by a neural network trained on one dataset often generalize to different data sets: particularly in the initial layers, the feature maps of a trained neural network typically activate based on color, low-level textures, and other features which are broadly generalizable\citep{yosinskiHowTransferableAre2014}.} \svp{\emph{Transfer learning} is the process of using layers from a CNN (or other classification model) trained on a general image recognition task when fitting a model intended for a more specific purpose. The layers which are deemed to identify broadly generalizable patterns are used with their pre-trained weights; new layers are added to customize the model to the specific task at hand, and typically, only these new weights are updated when the model is fit. Transfer learning allows CNNs to be applied to smaller datasets of several thousand images, with additional gains in the amount of time required to fit the model.}

\svp{Introduce VGG16 here. Show the diagram. Indicate that we're using the 5 convolutional blocks of the model, and adding a (shorter) model head. Use the awkward paragraph below as a source for material, but you don't need to go into too much detail.}

% VGG16, in particular, has over 14.7 million trainable parameters in its "eyes" alone. Luckily, CNNs offer one benefit that humans do not: you can utilize the eyes and replace the brain for new tasks. In terms of CNNs, it is possible to build a CNN that uses the weights already trained on over 1.2 million images in the convolutional layers, and then only retrain a new classifier for any new classification task. This reduced task brings the number of required training images down from millions to only thousands. Furthermore, this kind of approach is quite reasonable when considering what the CNN was originally trained to classsify. Since the 1,000 categories from the ILSVRC span a huge variety of natural and unnatural objects, we can likely trust that the features detected by the pre-trained CNN to be diverse enough to be applied to a new task.

% An alternative approach to the above paragraph:
%The structure of convolutional neural networks can vary depending on the type of classification the model is designed for, as well as the desired speed and accuracy. While networks can be built and trained from scratch, to do so typically requires a large amount of computing power and millions of images for any practical classification task. Another common way to use CNNS for a novel task is to use a pre-trained network

*I don't love this paragraph. Not sure how much to include, awkward flow.*
Pre-trained CNNs are CNNs that have been trained on a standard data set. \svp{One such dataset is } ImageNet, a database containing over 14 million images in about 22,000 categories (called "synsets", short for "synonym sets"). The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) was established in 2010 as a contest for CNN accuracy on a specific subset of ImageNet. Various CNN structures are tested on about 1.2 million images spanning 1,00 categories. These categories range from natural and man-made objects (e.g., daisy, chainsaw) to living creatures (e.g., ring-tailed lemur, sea lion, and dingo). There are also many categories which require subtle distinctions, such as differentiating between a grass snake and a vine snake. \mt{Something about the best CNNs for this task are the ones that are famous.} Some of the most well-known pre-trained CNNs include AlexNet, GoogLeNet/Inception, VGG, and ResNet. \mt{modularity property: Can use just structure and train weights yourself, use fully trained model to reproduce ILSVRC results, or just use pre-trained weights for feature detection}



\section{Machine Learning Model Evaluation}

Classification tasks are considered single-class when there is a only one binary decision of whether an item belongs to a single class of interest, and multi-class when there are a larger number of classes that an object may belong to. Multi-label classification is the special case of multi-class classification where categories are not mutually exclusive \svp{, that is, }an item may fall into a combination of categories simultaneously. While the true classification is a binary decision, it is common for models to predict these classifications \svp{using} probability, thus reporting a value between 0 and 1 reflecting how certainly the item can be attributed to a given class.

Evaluating model accuracy on classification requires addressing both the labels that are assigned and the labels that are not. \svp{Ideally, an image is labeled perfectly, and a true positive occurs when the model assigns a label which matches that of the image. A false positive in this scenario occurs when the model assigns a label which does not match that of the image. A true negative occurs if the model does not assign a label which does not occur in the image, and a false negative occurs when the model does not assign a label which does occur in the image.} \svp{XXX This is probably better done with a table.} \svp{Images which are not perfectly labeled (either labeled incorrectly or incompletely) further complicate the assignment of true and false positives and negatives. } \svp{XXX Add either words or a couple of good tables here describing incomplete/incorrect labeling.} The good scenarios: a true positive is assigning a correct label to an item, and a true negative is when a label is not assigned to an item that should not have that label. Errors can be made in failing to assign a correct label (false positive) or assigning an incorrect label (false negative).

Recall is the proportion of true labels that are assigned by the model. The converse of recall is precision, which measures the proportion of false positives to true positives\svp{XXX see note below paragraph}. An ideal model has high recall and high precision, which means it accurately identifies labels that should be present but does not assign too many false positives in the process. \svp{Transition directly to ROC curves- plot of True positive rate vs. false positive rate, etc. }

\svp{XXX This isn't quite true - they're inversely related, but logically one isn't the converse of the other (https://www.creighton.edu/fileadmin/user/HSL/docs/ref/Searching\_-\_Recall\_Precision.pdf). This is a tricky area and requires us to be *very* careful when picking words. The Wiki ROC curve page is very helpful at figuring out what's what.}

Receiver Operating Characteristic (ROC) curves show the relationship between the true positive rate and the false positive rate. Ideally, we want a high true positive rate and a low false positive rate, so a perfect ROC curve would hug the top-left corner of the graph and a straight line between corners would indicate that the prediction is no better than random chance. The Area Under the Curve (AUC) quantifies the shape of the ROC curve, with an AUC of 1 corresponding to perfect prediction and 0.5 indicating random chance.

\mt{possible graphic: https://towardsdatascience.com/precision-vs-recall-386cf9f89488}

A confusion matrix is a cross-tabulation between the observed and the predicted classes of the data. A confusion matrix can help identify which classes are being correctly predicted and which classes are commonly mixed-up with others. \svp{Relate to previously discussed quantities, as in the table at the bottom of this page: https://en.wikipedia.org/wiki/Confusion\_matrix}


