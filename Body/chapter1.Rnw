% Chapter 1 of the Thesis Template File

\Sexpr{knitr::set_parent('../thesis.Rnw')}
\graphicspath{{Images/chapter1/}{figure/chapter1/}}

<<ch1-setup, fig.keep='all', cache=FALSE, echo=FALSE, eval=TRUE, include=FALSE>>=
options(replace.assign=TRUE,width=70,scipen=3)
require(knitr)

wd <- getwd()
# Set paths for everything for compiling subdocument
if(!"Body" %in% list.files()){
  opts_chunk$set(fig.path='figure/chapter1/fig-', cache.path='cache/', fig.align='center', fig.width=5, fig.height=5, fig.show='hold', par=TRUE, cache=TRUE, concordance=TRUE, autodep=TRUE, root.dir="../", message=F, warning=F, error=F)
  datadir <- "../data/chapter1/"
  imgdir <- "../figure/chapter1/"
  codedir <- "../code/"
} else {
  opts_chunk$set(fig.path='figure/chapter1/fig-', cache.path='cache/', fig.align='center', fig.width=5, fig.height=5, fig.show='hold', par=TRUE, cache=TRUE, concordance=TRUE, autodep=TRUE, message=F, warning=F, error=F)
  datadir <- "data/chapter1/"
  imgdir <- "figure/chapter1/"
  codedir <- "code/"
}
@

\chapter{INTRODUCTION}

\section{Motivation}

In forensic science, shoe prints and outsole characteristics fall into the category of pattern evidence. When a shoe print or impression is found at a crime scene, the investigator may ask a series of questions. Initially, it may be important to determine the make and model of the shoe, which may help detectives locate comparison shoes from suspects. Later in the investigation, the forensic examiner may consider individualizing characteristics found in the print; that is, small defects that make it possible to tie a specific shoe to the print left at the scene. In cases where such individualizing characteristics are not considered (estimated at 95\% of cases in the United States according to some experts\footnote{Leslie Hammer, presentation to CSAFE on March 5, 2018}), it is important to be able to assess the probability that the specific model of shoe which made the print would be found in the suspect's posessions. This question is much more difficult than identifying the make and model of the shoe, because it requires that the forensic examiner have access to a database containing information about the frequency of shoes in the local population, where the local population itself may be difficult to define. Any tractable solution to the problem of assessing the random match probability of a shoeprint based only on class characteristics \citep{bodziak_footwear_2000} (make, model, and other characteristics determined during the manufacturing process) requires a way to assemble this database: an automated solution to efficiently classify many types of shoes within a common system. This project is designed to address the computational and statistical process of assembling statistical features which can be used to assess similarity beteween two or more images.

% \section{Background}

\section{Outsole Class Characteristics}\label{sec:class-chars-desc}

According to \citet{grossVariabilitySignificanceClass2013}, the four generally accepted conclusions that can be made from a footwear examination are identification, elimination, class association, and inconclusive. Typically, the ultimate goal of an examination is identification, which is matching a shoe print to an individual shoe, perhaps owned by a suspect of an the investigation. This is difficult because identification to a specific individual's shoe requires the matching of randomly acquired characterisics, such as damage, and that information is \svp{frequently un}available due to \svp{the} quality of the print or impression \svp{recovered from the crime scene}. Most of the work being done with shoeprints involves class association, which means identifying \svp{one or more shoe models and sizes which are consistent with the recovered} print.

Class characteristics are defined as the set of features which allow an object to be placed into a group with other physically similar objects. In the context of footwear, the term refers to the design and physical dimension of the shoe, particularly with regard to the shoe's outsole. While class characteristics are not sufficient for identification, they in many cases enable the exclusion of footwear \citep{bodziak_footwear_2000}.

When categorizing a shoe, it is common to use features like brand, size, and general type (e.g., boot, tennis shoe, dress shoe). When defining useful class characteristics for identification or exclusion, however, these features prove quite difficult to use. Size, for example, is far from straightforward\svp{, as size} standards vary significantly across different manufacturers and scales in different countries, and different shoe styles with same size inside may \svp{have different size outsoles}, making direct measurement or estimation of foot size difficult \citep{bodziak_footwear_2000}. Model of shoe is also not easily characterized due to the large number of look-alikes and the constantly changing market. Thus, when defining features to describe any possible shoe outsole that may be found at a crime scene, it is important \svp{that any set of descriptors} be general enough to describe a large variety of shoes and specific enough to differentiate between shoes that may have similar qualities. \svp{One such set of descriptors are geometric shapes, which} have been found to be able to differentiate between different shoes \citep{grossVariabilitySignificanceClass2013}. \svp{Matching tread patterns by comparing the spatial distribution of geometric shapes} "is of considerable evidential value" \citep{hancockInterpretationShoeprintComparison2012} \svp{for class characteristic comparisons}.

\section{\svp{Computational} Image Analysis and Convolutional Neural Networks}

Shoeprint evidence from crime scenes is most commonly collected in the form of a photograph, so any useful method to automatically identify outsole characteristics must take the form of an image analysis task. There are a number of methods that may be employed to identify shapes and features in an image. The Hough transform is a feature extraction technique carried out in parameter space that was classically used to identify straight lines but has been extended to identifying circles and ellipses, as well as other shapes \citep{ballardGeneralizingHoughTransform1981}. In addition, there are a number of low-level feature extraction methods aimed at detecting specific shapes, such as edges, corners, blobs, or ridges\citep[Ch 15]{machineVision}. While these methods are useful in identifying these specific features at a low level, they are very computationally intensive and features they produce/identify exist on a very small scale, often only a few pixels wide, and they are thus not able to identify large geometric shapes like those that may be found in an outsole image. Furthermore, these methods are extremely sensitive to lighting or color changes, and thus require additional modeling (like random forests) to aggregate low-level features into the geometric shapes found on outsoles.

For novel image classification tasks, Convolutional Neural Networks (CNNs) have become \svp{standard}
\footnote{The Google Trends interest graph shows a massive increase in convolutional neural networks between mid-2014 and 2018 \url{https://trends.google.com/trends/explore?date=2010-01-01\%202019-02-18&q=convolutional\%20neural\%20network,computer\%20vision}}
\svp{\citep{guRecentAdvancesConvolutional2018}}. CNNs have deep architectures that can be trained to identify complex patterns, but they are structurally similar to the human visual architecture and output binary or probabilistic predictions for given labels that are readily interpretable by humans. As CNNs make use of labeled training data, the predictions generated are for features which are similar to those identified by humans, providing greater face validity to the model. Once a CNN is trained, it is relatively fast and easy to apply the model to new images and obtain classifications.

% \subsection{Convolutional Neural Networks}

\subsection{General Approach}
Visual classification (i.e., assigning a label to an object based on visual input) is a complex task that humans do very well \citep{mallot2000computational}. Sight is our dominant sense and a significant part of our brain is dedicated to vision, which means that the structures used to impart meaning on a visual scene have been optimized through millions of years of evolution. As Convolutional Neural Networks are organized to mimic the process of object recognition in the human visual cortex, it will be useful to briefly describe that process.

\paragraph{Human Vision}
\svp{The visual perception process begins with the transfer of information from the visual world to the brain via rods and cones in the retina. Chemical signals travel along the optic nerve from the retina into the brain, where the signals are processed by a series of biological modules which aggregate information across multiple cells and provide meaning and order on otherwise chaotic chemical and electrical signaling.}
\svp{As information is aggregated, spatial relationships between objects in the physical world are maintained in the brain. Specialized feature detector cells respond preferentially to specific stimuli (e.g. cells which respond to lines oriented horizontally, vertically, or at specific angles), and these feature detectors are aggregated to identify more complex stimuli\citep[Ch. 4]{goldsteinSensationPerception2016}. In addition to the successive compilation of increasingly complex features, there are also specific modules for particularly important tasks, such as facial recognition; information from these regions is also integrated into the overall hierarchy of recognized objects from the visual input.}

\svp{The problem of general object recognition is quite difficult - a three-dimensional object has infinitely many projections into two-dimensional space, and in real scenes objects are often at least partially obscured. In addition, a two-dimensional image can map back to many different three-dimensional objects, because of the ambiguity introduced by the projection onto a flat sufrace. Several psychological theories exist as to how object recognition occurs within the brain (gestalt heuristics, recognition-by-components, and inferential contexts all have experimental support), but in general the process seems to require both spatial integration and learned associations\citep[Ch. 5]{goldsteinSensationPerception2016}.}

Differentiating between two objects is quite easy when features are distinct; however, there are many cases when differentiating features are rather subtle. For example, \svp{as shown in \autoref{fig:caterpillar-carrots},} an orange caterpillar and a baby carrot may be of similar color, shape, and size, but one is distinctly more fuzzy than the other\svp{; the distinction between the two categories is very strong even with all of the features that are shared}. Thus, our brains have learned that when faced with a small, cylindrical orange object, texture \svp{is a critically} important feature when assigning a label to that object (which keeps us from accidentally ingesting caterpillars).

\begin{figure}[htbp]\centering
\includegraphics[height=.25\linewidth]{caterpillar.png}
\hskip 1cm
\includegraphics[height=.25\linewidth]{carrots.png}
\caption[\svp{Feature Similarity.}]{\svp{A fuzzy caterpillar and a bunch of carrots have many similar visual features, but our brains easily distinguish between them.}}\label{fig:caterpillar-carrots}
\end{figure}

\paragraph{Computer Vision Using Neural Networks}
While our brains are adept at parsing images and classifying the objects within them, the task has proved much more difficult for computers, as evidenced by \autoref{fig:xkcd-img-recognition}. \svp{Human visual processing is so complex in part because of the successive aggregation of increasingly complex features; only within the last 10 years have we been able to adequately mimic this process with computer modeling. }

% \mt{I will read Goldstein this week and fill in this paragraph.}
% (Describe the human visual/classification process: feature detection, routing to the brain, and label application) \svp{Use Sensation \& Perception (Goldstein) heavily here. You might also use some of the computer vision books to compare/contrast.} \svp{Skip the retina, talk briefly about "feature detectors" e.g. Hubel and Wiesel (flag 1). Describe how those feature detectors are assembled into progressively higher-order features (flag 2), and then talk about special processing regions (flag 3). Describe why object recognition is hard(things look different from different angles, general class vs. specifics, etc.) and talk about recognition-by-components (flag 4). The assembly of these components into an intelligent framework is what makes human object recognition so much more advanced (flag 5), and only recently have we been able to mimic that level of organizational structure with computer modeling. You can integrate the caterpillar/carrot example here with feature integration and recognition-by-components (we're using a more general definition of components than the geom-based components in the text)}

\begin{figure}[htbp]\centering
\includegraphics[width = .3\linewidth]{xkcd.png}
\caption[\svp{Computer vision is a difficult problem.}]{Computer vision was thought to be easy in 1966 when a researcher at MIT believed that teaching a computer to separate picture regions into objects and background regions could be completed as a summer project \citep{papert_summer_1966}. The task proved much more difficult than expected, and \svp{has only become tractable with convolutional neural network based approaches.}}\label{fig:xkcd-img-recognition}
\end{figure}

Convolutional Neural Networks are a widely implemented method for automated image recognition; their structure typically emulates the successive aggregation of low-level features into higher-level features that is seen in the human visual cortex. CNNs perform comparably to humans on certain image recognition tasks \citep{geirhosComparingDeepNeural2017}. The ImageNet Large Scale Visual Recognition Competition (ILSVRC) is a widely followed contest to produce the best algorithm for image classification; since 2014, it has been dominated by convolutional neural networks \citep{russakovsky_imagenet_2015}. \svp{Various models} are tested on about 1.2 million images spanning 1000 categories. These categories range from natural and man-made objects (e.g., daisy, chainsaw) to living creatures (e.g., ring-tailed lemur, sea lion, and dingo). There are also many categories which require subtle distinctions, such as differentiating between a grass snake and a vine snake.

Convolutional neural networks (CNNs) are a tool for supervised deep-learning that have become standard in recent years for automatic image classification. CNNs are a form of artificial neural network, which were inspired by biological processes in the brain \citep{ANNasModelsofNeuralInfoProcessing}. CNNs primarily use combinations of convolutional and pooling layers to filter raw information into features. These features are then fed into densely connected layers which are trained to associate given sets of features with their desired labels. This translation-invariant automated classification closely mimics the human eye-to-brain classification process.

\subsection{Building Blocks of a Convolutional Neural Network}
\svp{CNNs are made up of several distinct types of layers which transition from input image to output class probabilities; in the remainder of this section, we discuss several important layer types which are used in most CNNs.}

\paragraph{Image Convolution and Convolutional Layers}
\svp{Convolutional Neural Networks make use of convolutional layers, which use image convolution as a primary operation. Defined mathematically, image convolution is an function performed on an image $x$ using a smaller-dimension matrix $\beta$.}

Let \(x\) be an image represented as a numerical matrix, indexed by \(i, j\), and \(\beta\) be a filter of dimension \((2a + 1) \times (2b + 1)\). The convolution of image \(x\) and filter \(\beta\) is $$(\beta \ast x)(i, j) = \sum_{s = -a}^a\sum_{t = -b}^b \beta(s, t) x(i-s, j-t)$$

Convolutional Neural Networks are named to highlight their use of \svp{image convolution operations} to extract information from an image. \svp{As shown in \autoref{fig:image-convolution-setup},} a single convolutional filter is a small array of real valued weights that represents some feature (shown in green). When a filter is applied to a portion of the image (shown in blue), a single value is returned that is associated with the presence of the feature for a given subsection of the input image. When applied over an entire image, the resulting matrix of values maps the strength of the feature across the entire image\svp{, as shown in \autoref{fig:image-convolution-illustration}}. Once the entire image has been convolved with the filter, the feature map is transformed using a nonlinear activation function. A convolutional layer of a CNN takes a large number of these filters and passes them over the image to return one feature map per filter.

\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{filter.png}
\caption{An image (blue) and a convolutional filter (green)}\label{fig:image-convolution-setup}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=.45\textwidth]{filter1.png}\hfill\includegraphics[width=.45\textwidth]{filter2.png}
\caption[Image convolution, illustrated]{The convolution operation consists of the smaller filter (green) applied to each region of the larger image (blue); each application results in a single value which is stored in the feature map. }\label{fig:image-convolution-illustration}
\end{figure}

% \svp{XXX Add in pictures showing convolutional layer, activation map. Be sure to talk about the fact that there is a nonlinear activation function (ReLU) applied - this is what makes the convolution operation part of a neural network.}

\svp{Convolutional layers typically do not hugely decrease the dimensions of the matrix; in many cases, the input image is padded in order to prevent any reduction in dimension. In many neural network structures, dimension reduction is performed by pooling layers, which identify the strongest local features in each filter layer of the convolutional output.}

\paragraph{Max Pooling Layers}
Max-pooling is a technique to reduce the size, and therefore computational load, of feature maps through structured down-sampling. Max-pooling layers apply a maximum function over adjacent regions of a feature map (like using a sliding window) to encode the important information of how strongly a feature was activated in a given region of the image while simultaneously reducing redundant or unneccessary information about smaller activations. For example, taking 2x2 pieces of a feature map and keeping only the largest of the four values reduces the size of the feature map by a factor of 4\svp{, as shown in \autoref{fig:max-pooling}}. Max-pooling is also beneficial in that it allows CNN "vision" to be relatively translation invariant, because it emphasizes the relative position of a feature rather than its absolute position.

\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{maxpooling}
\caption{Max pooling layer illustration}\label{fig:max-pooling}
\end{figure}

\svp{Once the dimension of the image has been reduced and features have been identified using a combination of convolutional and pooling layers, local features must be integrated into a more unified whole. This integration is performed using densely connected layers.}

\paragraph{Activation Functions}
\svp{The convolution and max pooling operations discussed above rely on activation functions, which operate on each feature map value. Different activation functions are used to produce different network effects - in the convolutional layers, the desire is often to minimize the computational complexity, so very simple activation functions are used, such as the Rectified Linear Unit (ReLU), Exponential Linear Unit (ELU), or a differentiable analog, SoftPlus. The output layer must map features onto binary predictions or probabilities, so the sigmoid activation function is commonly used for this purpose. \autoref{fig:activation-functions} shows several common nonlinear activation functions.}

<<activation-functions, echo = F, fig.scap = "Common nonlinear activation functions used in neural networks.", fig.cap = "Activation functions commonly used in neural networks.", out.width = "\\textwidth", fig.width = 6, fig.height = 2>>=
library(tidyverse)
sigmoid <- function(x) 1/(1 + exp(-x))
relu <- function(x) pmax(x, 0)
elu <- function(x, a) a*(exp(x) - 1) * (x < 0) + x * (x > 0)
softplus <- function(x) log(1 + exp(x))

x <- seq(-3, 3, .005)
tibble(x = x, Linear = x, Sigmoid = sigmoid(x), ReLU = relu(x), ELU = elu(x, 1), SoftPlus = softplus(x)) %>%
  gather(key = key, value = value, -x) %>%
  mutate(key = factor(key, levels = c("Linear", "ReLU", "ELU", "SoftPlus", "Sigmoid"))) %>%
  ggplot(aes(x = x, y = value)) + geom_line() + facet_grid(.~key) +
  ggtitle("Common Activation Functions") +
  theme(axis.title = element_blank()) +
  coord_fixed()
@

\paragraph{Densely Connected Layers}
Densely connected layers are typically the final layers in a CNN, \svp{making up what is known as the model head}. These layers form the meaningful connection between the features of an image (detected by convolutional and max-pooling layers) and the corresponding labels associated with the image. These layers act like the human brain: just as we learned which combinations of features should be associated with a given label, densely connected layers use real-valued weights to represent these associations. For example, an item \svp{which} is orange, small, and fuzzy \svp{is commonly associated with the word ``caterpillar"}. Fuzzy is not a feature \svp{typically associated with the word} carrot, so there is little connection between the feature ``fuzzy" and the label ``carrot". Similarly, in densely connected or fully connected layers, each final feature is connected to each label through \svp{weights} (hence the name ``densely connected") \svp{learned during the training process}. \svp{Weights are optimized in order to minimize errors (measured by a loss function)} and thus improve classification accuracy.

<<dense-layer, echo = F, fig.width = 5, fig.height = 3, out.width = ".5\\textwidth", fig.cap = "A densely-connected layer with 12 input nodes and 4 output nodes.">>=
source(file.path(codedir, "Generate_Model_Images.R"))
plot_deepviz2(c(12, 4), r = .005)
@

\svp{In some cases, fully connected layers may utilize a pruning mechanism known as a dropout rate, which forces a set percentage of the weights to be zero. This results in the maintenance of only the strongest connections between two layers of the model.}
<<dense-layer2, echo = F, fig.width = 5, fig.height = 3, out.width = ".5\\textwidth", fig.cap = "A densely-connected layer with 12 input nodes, 4 output nodes, and a 50\\% dropout rate.">>=
source(file.path(codedir, "Generate_Model_Images.R"))
plot_deepviz_sample(c(12, 4), r = .005, dropout_rate = 0.5) +
  theme(legend.position = c(1, 0), legend.justification = c(1, 0))
@

\paragraph{Output Classification Layer}
\mt{Include a description of } \svp{sigmoid} \mt{ and softmax and when each is used.}
\svp{In order to transform the neural network node values into output class probabilities, the final layer of the model head uses an activation function selected to conform to the problem specifications. For instance, if the goal is to perform binary classification, the sigmoid activation function shown in \autoref{fig:activation-functions} will map any real valued number to a value between 0 and 1 (that is, to a class probability). }
\svp{In a multinomial classification problem, an extension of the sigmoid activation function, the \emph{softmax} activation function, is used: For inputs $y_i$, $S(y_i) = \frac{e^{y_i}}{\sum_j e^{y_j}}$. The softmax activation function produces class probabilities which sum to 1. }

\svp{In classification problems where there are $n$ classes, but each object can have between 0 and $n$ assigned labels, the sigmoid activation function can be used for each class label separately, producing a $n$-dimensional vector of probabilities between 0 and 1.}

\subsection{Forward and Backward Propagation}
\svp{All the equations go in here. }

\subsection{Transfer Learning}
Convolutional layers and max-pooling layers in a CNN are analogous to the human visual perception process, and densely connected layers behave like the human brain. In short, the approach to classifying an image is to detect the features in the image (like our eyes) and then assign labels to combinations of those features (brain). This analogy is also appropriate because it reflects the difficulty of the task: it takes many years and a significant amount of effort for humans to learn how to distinguish a large variety of features and also to connect those features to labels that are often complex, hierarchical, and subtle. Similarly, training a CNN is no small task. \svp{Even relatively simple convnets can have many millions of trainable parameters in the model base (the convolutional and pooling layers). Optimizing all of these weights requires an incredible amount of computational power. In addition, the features learned by a neural network trained on one dataset often generalize to different data sets: particularly in the initial layers, the feature maps of a trained neural network typically activate based on color, low-level textures, and other features which are broadly generalizable\citep{yosinskiHowTransferableAre2014}.} \svp{\emph{Transfer learning} is the process of using layers from a CNN (or other classification model) trained on a general image recognition task when fitting a model intended for a more specific purpose. The layers which are deemed to identify broadly generalizable patterns are used with their pre-trained weights; new layers are added to customize the model to the specific task at hand, and typically, only these new weights are updated when the model is fit. Transfer learning allows CNNs to be applied to smaller datasets of several thousand images, with additional gains in the amount of time required to fit the model.}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{vgg16-base-head}
\caption[Transfer learning using pre-trained neural networks]{This diagram is for a pre-trained convolutional neural network. The model base consists of 5 convolutional blocks; the model head consists of several fully connected layers capped with an activation layer which transforms the aggregate visual input to output class probabilities. During transfer learning, the model base weights are fixed to the values derived from the initial input material used to train the original model; only the weights in the model head are retrained to accommodate the input training data.} \label{fig:transfer-learning-diagram}
\end{figure}

\svp{Transfer learning leverages the modularity of neural networks - the pretrained base of the model can be separated from the full model and a new model head can be trained to connect that base to the output classes, as shown in \autoref{fig:transfer-learning-diagram}. One of the simpler pre-trained convolutional networks available, VGG16, has a structure which is ideal for our purposes - it consists of several blocks of convolutional layers, with a fully connected head, as shown in \autoref{fig:vgg16-structure}. The simplicity of this structure provides the ability to peer into the inner workings of the network for diagnostic purposes, providing a distinct advantage over more complicated network structures with slightly higher accuracy ratings.}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{vgg16-layer-size}
\caption[VGG16 model structure]{The VGG16 model structure, for input images of 256 x 256 pixels. Each convolutional block operates on a different image matrix size; max pooling layers are used to transition between convolutional blocks. There are 5 convolutional blocks in the VGG16 model, which are then connected to output classes using several fully connected layers. } \label{fig:vgg16-structure}
\end{figure}

\svp{VGG16 and a similar network, VGG19, won the 2014 ILSVR challenge; they have since been outcompeted by networks with more complicated structures, such as GoogLeNet/Inception\citep{szegedyGoingDeeperConvolutions2015} and ResNet \citep{heDeepResidualLearning2015}. VGG-style networks are still commonly used as building blocks for other types of convolutional networks; their streamlined structure allows for easy extension to other tasks, such as texture detection and style transfer \citep{gatysImageStyleTransfer2016}.}

\section{Machine Learning Model Evaluation}

Classification tasks are considered single-class when there is a only one binary decision of whether an item belongs to a single class of interest, and multi-class when there are a larger number of classes that an object may belong to. Multi-label classification is the special case of multi-class classification where categories are not mutually exclusive \svp{, that is, }an item may fall into a combination of categories simultaneously. While the true classification is a binary decision, it is common for models to predict these classifications \svp{using} probability, thus reporting a value between 0 and 1 reflecting how certainly the item can be attributed to a given class.

Evaluating model accuracy on classification requires addressing both the labels that are assigned and the labels that are not. Ideally, an image is labeled perfectly, and a true positive occurs when the model assigns a label which matches that of the image. A false positive in this scenario occurs when the model assigns a label which does not match that of the image. A true negative occurs if the model does not assign a label which does not occur in the image, and a false negative occurs when the model does not assign a label which does occur in the image. \svp{\autoref{tab:error-binary-problem} illustrates these terms for a simple binary classification problem. }

\begin{table}
\centering
\begin{tabular}{cccc}
 & & \multicolumn{2}{c}{Model - Assigned Label} \\\cline{3-4}
 & & A & Not A \\\hline
 \multirow{2}{*}{True Label} & A & True Positive & False Negative\\\cline{2-4}
 & Not A & False Positive & True Negative \\\hline
\end{tabular}
\caption{Model errors for a two-class binary decision problem. Correct decisions are shown along the diagonal; incorrect decisions are in the off-diagonal cells.}\label{tab:error-binary-problem}
\end{table}

\svp{Recall, or sensitivity, which is the true positive rate, is the sum of all true positives divided by the number of positive cases in the data. Specificity, which is the true negative rate, is the sum of all true negatives divided by the number of negative cases in the data. Precision is the sum of all true positives divided by the number of positive predictions made by the model. }

\svp{ROC curves plot the false positive rate (1 - specificity) against the true positive rate. } Ideally, we want a high true positive rate and a low false positive rate, so a perfect ROC curve would hug the top-left corner of the graph and a straight line between corners would indicate that the prediction is no better than random chance. The Area Under the Curve (AUC) quantifies the shape of the ROC curve, with an AUC of 1 corresponding to perfect prediction and 0.5 indicating random chance.
\svp{XXX - Add a sample ROC curve from ggplot2 to show as an example visualization}

\svp{A confusion matrix is a cross-tabulation between the observed and the predicted classes of the data. A confusion matrix can help identify which classes are being correctly predicted and which classes are commonly mixed-up with others. Confusion matrices are typically used with binary classification problems; an aggregate version of \autoref{tab:error-binary-problem} would be a confusion matrix, where totals for each decision would be shown in the matrix cells. Confusion matrices do not extend easily to multi-class problems because any single miscategorization produces both a false negative decision and a false positive decision; it is not clear which should be shown in each cell, as shown in \autoref{tab:error-3class-problem}.}\svp{XXX - this table and last sentance may not be essential and could probably be cut, but serve as a nice tease for the original work done in this CC.}
\svp{XXX - Add a confusion matrix from ggplot2 to show as an example visualization}

\begin{table}
\centering
\begin{tabular}{ccccc}
 & & \multicolumn{3}{c}{Model - Assigned Label} \\\cline{3-5}
 & & A & B & C \\\hline
 \multirow{6}{*}{True Label} & \multirow{2}{*}{A} & \multirow{2}{*}{True Positive (A)} & False Positive (B) & False Positive (C) \\
 & & & False Negative (A) & False Negative (A)\\\cline{2-5}
 & \multirow{2}{*}{B} & False Positive (A) & \multirow{2}{*}{True Positive (B)} & False Positive (C) \\
 & & False Negative (B) & & False Negative (B) \\\cline{2-5}
 & \multirow{2}{*}{C} & False Positive (A) & False Positive (B) & \multirow{2}{*}{True Positive (C)} \\
 & & False Negative (C) & False Negative (C) & \\\hline
\end{tabular}
\caption{Model errors for a three-class binary decision problem. Correct decisions are shown along the diagonal; incorrect decisions are in the off-diagonal cells. Note that for each misclassification, two incorrect decisions are made - the ommission of the correct label and the addition of an incorrect label.}\label{tab:error-3class-problem}
\end{table}

\svp{However, in some situations, it is possible to reduce the classification problem to a series of binary classifications; in these situations, which will be described in more detail in \autoref{ch3:model-accuracy}, a modification of the confusion matrix is possible to better visualize model performance.}
