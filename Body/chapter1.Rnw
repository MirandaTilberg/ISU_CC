% Chapter 1 of the Thesis Template File

\Sexpr{knitr::set_parent('../thesis.Rnw')}
\graphicspath{{Images/chapter1/}{figure/chapter1/}}

<<ch1-setup, fig.keep='all', cache=FALSE, echo=FALSE, eval=TRUE, include=FALSE>>=
options(replace.assign=TRUE,width=70,scipen=3)
require(knitr)

wd <- getwd()
# Set paths for everything for compiling subdocument
if(!"Body" %in% list.files()){
  opts_chunk$set(fig.path='figure/chapter1/fig-', cache.path='cache/', fig.align='center', fig.width=5, fig.height=5, fig.show='hold', par=TRUE, cache=TRUE, concordance=TRUE, autodep=TRUE, root.dir="../", message=F, warning=F, error=F)
  datadir <- "../data/chapter1/"
  imgdir <- "../figure/chapter1/"
} else {
  opts_chunk$set(fig.path='figure/chapter1/fig-', cache.path='cache/', fig.align='center', fig.width=5, fig.height=5, fig.show='hold', par=TRUE, cache=TRUE, concordance=TRUE, autodep=TRUE, message=F, warning=F, error=F)
  datadir <- "data/chapter1/"
  imgdir <- "figure/chapter1/"
}
@

\chapter{INTRODUCTION}

\section{Motivation}

In forensic science, shoe prints and outsole characteristics fall into the category of pattern evidence. When a shoe print or impression is found at a crime scene, the investigator may ask a series of questions. Initially, it may be important to determine the make and model of the shoe, which may help detectives locate comparison shoes from suspects. Later in the investigation, the forensic examiner may consider individualizing characteristics found in the print; that is, small defects that make it possible to tie a specific shoe to the print left at the scene. In cases where such individualizing characteristics are not considered (estimated at 95\% of cases in the United States according to some experts\footnote{Leslie Hammer, presentation to CSAFE on March 5, 2018}), it is important to be able to assess the probability that the specific model of shoe which made the print would be found in the suspect's posessions. This question is much more difficult than identifying the make and model of the shoe, because it requires that the forensic examiner have access to a database containing information about the frequency of shoes in the local population, where the local population itself may be difficult to define. Any tractable solution to the problem of assessing the random match probability of a shoeprint based only on class characteristics \citep{bodziak_footwear_2000} (make, model, and other characteristics determined during the manufacturing process) requires a way to assemble this database: an automated solution to efficiently classify many types of shoes within a common system. This project is designed to address the computational and statistical process of assembling statistical features which can be used to assess similarity beteween two or more images.

\section{Background}

\subsection{Outsole Class Characteristics}\label{sec:class-chars-desc}

\mt{This paragraph is going to be a project for me on its own. I'll overload quickly if I try to go through all the sources at once, so for now I'm going to collect important bits from the textbook and continue from there another day.}

Class characteristics are defined as the set of features which allow an object to be placed into a group with other physically-similar objects. In the context of footwear, the term refers to the design and physical dimension of the shoe, particularly with regard to the shoe's outsole. While class characteristics are not sufficient for identification, or matching an imprint from a crime scene to a specific shoe, they in many cases enable the exclusion of footwear \citep{bodziak_footwear_2000}. \mt{I forget APA guidelines for multiple ideas coming from the same source. If multiple sentences come from the same source, is it cite at the end of them?} \svp{I don't know about APA guidelines (it's been a while), but more generally, just cite them all at the end of the thought unless they're easily separated, then cite at the end of each clause.}

\textbf{\citet{bodziak_footwear_2000}}
\begin{itemize}
\item Size: MANY metrics for size, not consistent across manufacturers/countries, many conversion charts differ widely, so not an easy or consistent metric. Also different shoe styles with same size inside may be differently sized outside
\item Size difficult to determine from crime scene if poor images/impression records
\item Model: If a shoe is popular, can have multiple molds in same size that may all have slight differences
\item Make/model: (Ch 12) Counterfeit designs and look-alikes, shoe models constantly changing, old ones coming back -> make and model are difficult
\end{itemize}

%\svp{This section needs to describe different class characteristics - make, model, size, etc. and the discriminative power of each (if possible). Use Bodziak's book to get some of this information; intro sections of footwear papers may also be useful, and Gross (2013) is probably going to be useful too. This is a research paper section - cite bits from different papers to make the point. If you start with Bodziak and other papers, and end with Gross (2013), you can probably make the point fairly conclusively that make and model are difficult to work with, but that geometric shapes provide a useful feature set for discriminating shoes.}

%Footwear class characteristics include the size and shape of geometric design elements. Size, orientation, and position of geometric elements are capable of distinguishing most shoes collected in samples from the general population \cite{hancockInterpretationShoeprintComparison2012}.


\subsection{Image Analysis}

\mt{Question: I looked into these, but I'm still not quite sure how feature detection isn't just convolutional filters.}
\svp{Low-level feature detection uses convolution in some cases, but generally there are other steps in the process that also matter. The filter thing is definitely true for edge/corner detection. Ridge detection uses watershed segmentation, which is not convolutional in nature as I understand it, though it does use a gaussian kernel as part of the process. Blob detection also uses watershed segmentation, at least with some methods.}

\svp{Hough transforms and other pattern matching algorithms are very different, though they do also use convolution (almost everything image related does at one point or another). \url{https://www.youtube.com/watch?v=4zHbI-fFIlI} (about halfway through) to see this in action.}

\svp{The more important point is that these methods exist and are more primitive than CNNs, but are also more fragile.  You don't have to explain every method - find a paper describing the method, cite it, move to the next method. Or just cite a textbook with all of the methods. You want to show we didn't just leap to the shiny new toy - we actually need all 9 million parameters to get what we want.}

\svp{Methods that are worth briefly describing: (links to wiki, but you can use Computer Vision textbook to get a citeable reference)}
\begin{itemize}
\item\svp{\href{https://en.wikipedia.org/wiki/Feature_extraction\#Low-level}{Low level feature detectors - edge, corner, ridge, blob}}
\item\svp{\href{https://en.wikipedia.org/wiki/Hough_transform}{Hough transforms}}
\end{itemize}
\svp{Gist: these methods 1) work at a very low level, 2) produce features that aren't ``global" - corners are 3x3 pixel region corners, not quadrilateral corners, 3) are computationally intensive, and 4) are very fragile - the parameters used break with lighting or color changes. Even if we did use them, we'd need many different random forest models to handle aggregating low-level features into things like quadrilaterals, lines, etc., and those models would be as fragile as the methods producing the input data.}

\svp{Additional paragraph: Why CNNs are a good option - they've taken over image recognition, they're fast and work at scale on new images, they produce results that are interpretable, and they match human visual structure architecture, so the features they pick out should match human-labeled features (unlike low-level CV methods).}

\subsection{CNN Theory}

\textbf{Classification}

\begin{itemize}
\item Visual classification (i.e., assinging a label to an object based on features that can be seen) is a complex task that humans do very well.
\item Sight is our dominant sense and a significant part of our brain is dedicated to vision --> our visual process is refined and skilled (cite somehow?). \svp{You are 100\% encouraged to use the xkcd comic here to talk about how hard it is to emulate this with a computer}
\item (Describe the human visual/classification process: feature detection, routing to the brain, and label application) \svp{Use Sensation \& Perception (Goldstein) heavily here. You might also use some of the computer vision books to compare/contrast.}
\item Want to make a point about how subtle some differences can be. Can use the caterpillar/carrot example with "fuzzy texture" difference, or use corgi/fox and say that the differences are in the presence of the tongue, curve of the body, size of the mouth.
\end{itemize}


% Differentiating between two objects is quite easy when features are distinct; however, there are many cases when differentiating features are rather subtle. For example, an orange caterpillar and a baby carrot may be of similar color, shape, and size, but one is distinctly more fuzzy than the other. Thus, our brains have learned that when faced with a small, cylindrical orange object, texture becomes an important feature when assigning a label to that object (which keeps us from accidentally ingesting caterpillars).

% \vskip .2cm
%
% \begin{figure}[htbp]\centering
% \includegraphics[height=.25\linewidth]{caterpillar.png}
% \hskip 1cm
% \includegraphics[height=.25\linewidth]{carrots.png}
% \end{figure}

\textbf{Convolutional Neural Networks (CNNs)}

While our brains are adept at parsing images and classifying the objects within them, the task has proved much more difficult for computers. Computer vision was thought to be easy in 1966 when a researcher at MIT believed that teaching a computer to separate picture regions into objects and background regions could be completed as a summer project \citep{papert_summer_1966}. The task proved much more difficult than expected, and remained difficult for decades. Now, CNNs are a widely implemented method for automated image recognition and perform comparably to humans on certain tasks. The ImageNet Large Scale Visual Recognition Competition (ILSVRC)is a widely followed contest to produce the best algorithm for image classification; since 2014, it has been dominated by convolutional neural networks \citep{russakovsky_imagenet_2015}.


Convolutional neural networks (CNNs) are a tool for supervised deep-learning that have become standard in recent years for automatic image classification. CNNs are a form of artificial neural network, which were inspired by biological processes in the brain \citep{ANNasModelsofNeuralInfoProcessing}. CNNs primarily use combinations of convolutional and pooling layers to filter raw information into features. These features are then fed into densely connected layers which are trained to associate given sets of features with their desired labels. This translation-invariant automated classification closely mimics the human eye-to-brain classification process.


\textbf{Filters and Convolution}
Convolutional Neural Networks are named to highlight their use of convolution to extract information from an image. To a computer, an image is stored as a 3-dimensional array with a length and width corresponding to its number of pixels and a depth of 3 to represent the typical RBG color channels.

\mt{Let \(x\) be an image represented as a numerical matrix, indexed by \(i, j\), and \(\beta\) be a filter of dimension \((2a + 1) \times (2b + 1)\)

The convolution of image \(x\) and filter \(\beta\) is $$(\beta \ast x)(i, j) = \sum_{s = -a}^a\sum_{t = -b}^b \beta(s, t) x(i-s, j-t)$$}

\mt{What's the balance between describing with math and using plain English? I like the plainspeak, but maybe it's not so necessary?} \svp{Use math, describe in plain English, then add pictures to be sure. The goal is to communicate.} A single convolutional filter is a small array (say 5x5x3) of real valued weights that represents some feature of the image. When a filter is applied to a portion of the imagea single value is returned that is associated with the presence of the feature for a given subsection of the input image. When applied over an entire image, the resulting matrix of values maps the strength of the feature across the entire image. A convolutional layer of a CNN takes a large number of these filters and passes them over the image to return one feature map per filter.



<<convolution, fig.height = 3, fig.width = 5.4, echo = F, include = F>>=
library(ggplot2)
library(magrittr)
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(gridExtra))

input_data <- expand.grid(x = 1:5, y = 1:5)
set.seed(1)
input_data$labels <- rbinom(p = .5, size = 1, n = 25)

input <- ggplot(data = input_data, aes(x = x, y = y)) +
  geom_tile(fill = "cornflowerblue", color = "gray20") +
  geom_text(aes(label = labels), size = 10) +
  theme_void()

filter_data <- expand.grid(x = 1:3, y = 1:3)
filter_data$labels <-c(rep(1:0, 4), 1)

filter <- ggplot(data = filter_data, aes(x = x, y = y)) +
  geom_tile(fill = "lightgreen", color = "gray20") +
  geom_text(aes(label = labels), size = 10) +
  theme_void()

rt_layout <- matrix(NA, ncol = 4, nrow = 5)
rt_layout[2:4, 2:4] <- 2

layout <- cbind(matrix(1, ncol = 5, nrow = 5),
                rt_layout)

grid.arrange(grobs = list(input, filter),
             layout_matrix = layout)
@



%In the case of VGG16, early convolutional layers contain 64 features that primarily detect colors and edge patterns. Later convolutional layers of VGG16, in contrast, contain 512 filters that represent much more complex features, like animal fur patterns or distinct bird heads.

\textbf{Max-Pooling}
Max-pooling is a technique to reduce the size, and therefore computational load, of feature maps through structured down-sampling. Max-pooling layers apply a maximum function over adjacent regions of a feature map (like using a sliding window) to encode the important information of how strongly a feature was activated in a given region of the image while simultaneously reducing redundant or unneccessary information about smaller activations. For example, taking 2x2 pieces of a feature map and keeping only the largest of the four values reduces the size of the feature map by a factor of 4! Max-pooling is also beneficial in that it allows CNN "vision" to be translation invariant, because it emphasizes the relative position of a feature rather than its absolute position. VGG16 follows groups of 2 or 3 convolutional layers with a max-pooling layer, which ultimately takes in initial feature maps of size 224x224 and ends with maps of size 7x7.

\textbf{Densely Connected Layers}
Densely connected layers are typically the final layers in a CNN. These layers form the meaningful connection between the features of an image (detected by convolutional and max-pooling layers) and the corresponding labels associated with the image. These layers act like the human brain: just as we learned which combinations of features should be associated with a given label, densely connected layers use real-valued weights to represent these associations. For example, if we see an item that is orange, small, and fuzzy, we are taught to call it "caterpillar". Fuzzzy is not a feature we meaningfully associate with a baby carrot, so there is little connection between the feature "fuzzy" and the label "carrot". Similarly, in CNNs, each final feature is connected to each label through a weight (hence the name "densely connected"), and those weights are learned through the training process (using an algorithm called back-propogation) to minimize loss and thus improve classification accuracy. \svp{Go ahead and add the pictures in here - you have them, may as well use them...}

%\includegraphics


\textbf{Using a Pre-Trained CNN for a New Task}
\svp{"Transfer learning" is the technical term.} \svp{You may also want to talk about "modularity" in that the Convolutional part is one module, the head is another, and they're separately trainable and useable. }

*I went off on a tangent and am purposely not restructuring this paragraph yet. Sorry.*
As we have just seen, convolutional layers and max-pooling layers in a CNN are analogous to the human visual perception process, and densely connected layers behave like the human brain. In short, the approach to classifying an image is to detect the features in the image (like our eyes) and then assign labels to combinations of those features (brain). This analogy is also appropriate because it reflects the difficulty of the task: it takes many years and a significant amount of effort for humans to learn how to distinguish a large variety of features and also to connect those features to labels that are often complex, hierarchical, and subtle. Similarly, training a CNN is no small task. VGG16, in particular, has over 14.7 million trainable parameters in its "eyes" alone. Luckily, CNNs offer one benefit that humans do not: you can utilize the eyes and replace the brain for new tasks. In terms of CNNs, it is possible to build a CNN that uses the weights already trained on over 1.2 million images in the convolutional layers, and then only retrain a new classifier for any new classification task. This reduced task brings the number of required training images down from millions to only thousands. Furthermore, this kind of approach is quite reasonable when considering what the CNN was originally trained to classsify. Since the 1,000 categories from the ILSVRC span a huge variety of natural and unnatural objects, we can likely trust that the features detected by the pre-trained CNN to be diverse enough to be applied to a new task.


\textbf{Pre-trained CNNs}

%The structure of convolutional neural networks can vary depending on the type of classification the model is designed for, as well as the desired speed and accuracy. While networks can be built and trained from scratch, to do so typically requires a large amount of computing power and millions of images for any practical classification task. Another common way to use CNNS for a novel task is to use a pre-trained network

*I don't love this paragraph. Not sure how much to include, awkward flow.*
Pre-trained CNNs are CNNs that have been trained on a standard data set. The standard data set comes from ImageNet, a database containing over 14 million images in about 22,000 categories (called "synsets", short for "synonym sets"). The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) was established in 2010 as a contest for CNN accuracy on a specific subset of ImageNet. Various CNN structures are tested on about 1.2 million images spanning 1,00 categories. These categories range from natural and man-made objects (e.g., daisy, chainsaw) to living creatures (e.g., ring-tailed lemur, sea lion, and dingo). There are also many categories which require subtle distinctions, such as differentiating between a grass snake and a vine snake. *Something about the best CNNs for this task are the ones that are famous.* Some of the most well-known pre-trained CNNs include AlexNet, GoogLeNet/Inception, VGG, and ResNet. *Can use just structure and train weights yourself, use fully trained model to reproduce ILSVRC results, or just use pre-trained weights for feature detection*

\textbf{VGG16 Architecture}
The main difference between different CNNs is their structure, meaning the number of layers they contain and the pattern those layers are in. In our research, we have tested a few pre-trained CNNs, and we are currently using VGG16. Developed by Oxford's Visual Graphics Group, VGG16 has 16 "functional" (i.e., convolutional and densely connected) layers and 5 max-pooling layers, which function more to alter the structure of the information at each step.

\svp{Show VGG16 architecture image (use gimp + XCF file from my presentation to get the "right" image - let me know if you need help tweaking it...) and talk through the image dimension changes.}

\svp{Contrast VGG16 structure with ResNet - \href{https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624}{one source}, explain VGG16 is simpler, so we can more easily explain it and produce diagnostic images.}

\subsection{Model Evaluation Metrics}

Multi-class classification refers to the number of categories to which an item may be identified. Multi-label classification is the special case of multi-class classification where categories are not mutually exclusive and, thus, an item may fall into a combination of categories simultaneously. Evaluating the accuracy of multi-label predictions requires attention to both false-positive and false-negative predictions. When predictions are probablistic and labels are binary, like in the case of our CNN, \mt{something about cut-offs for 0-1 metrics}

Will include graphic from:
https://towardsdatascience.com/precision-vs-recall-386cf9f89488

Confusion matrix shows

\svp{Accuracy, Precision/Recall, Confusion Matrix, and any other brilliant ways of examining multi-label multi-class model effectiveness.}
TBD
