% Chapter 1: Motivation and Background

\Sexpr{knitr::set_parent('../thesis.Rnw')}
\graphicspath{{Images/chapter1/}{figure/chapter1/}}

<<ch1-setup, fig.keep='all', cache = F, echo = F, eval = T, include = F>>=
options(replace.assign = TRUE, width = 70, scipen = 3)
require(knitr)

wd <- getwd()
# Set paths for everything for compiling subdocument
if (!"Body" %in% list.files()) {
  opts_chunk$set(fig.path = 'figure/chapter1/fig-', cache.path = 'cache/',
                 fig.align = 'center', fig.width = 5, fig.height = 5,
                 fig.show = 'hold', par = TRUE, cache = TRUE,
                 concordance = TRUE, autodep = TRUE, root.dir = "../",
                 message = F, warning = F, error = F)
  datadir <- "../data/chapter1/"
  imgdir <- "../figure/chapter1/"
  codedir <- "../code/"
  modeldir <- "../model/"
} else {
  opts_chunk$set(fig.path = 'figure/chapter1/fig-', cache.path = 'cache/',
                 fig.align = 'center', fig.width = 5, fig.height = 5,
                 fig.show = 'hold', par = TRUE, cache = TRUE,
                 concordance = TRUE, autodep = TRUE,
                 message = F, warning = F, error = F)
  datadir <- "data/chapter1/"
  imgdir <- "figure/chapter1/"
  codedir <- "code/"
  modeldir <- "model/"
}
library(tidyverse)
@

\chapter{INTRODUCTION}

\section{Motivation}

In forensic science, shoe prints and outsole characteristics fall into the category of pattern evidence. When a shoe print or impression is found at a crime scene, the investigator may ask a series of questions. Initially, it may be important to determine the make and model of the shoe, which may help detectives locate comparison shoes from suspects. Later in the investigation, the forensic examiner may consider individualizing characteristics found in the print; that is, small defects that make it possible to tie a specific shoe to the print left at the scene. In cases where such individualizing characteristics are not considered (estimated at 95\% of cases in the United States according to some experts\footnote{Leslie Hammer, presentation to CSAFE on March 5, 2018}), it is important to be able to assess the probability that the specific model of shoe which made the print would be found in the suspect's posessions. This question is much more difficult than identifying the make and model of the shoe, because it requires that the forensic examiner have access to a database containing information about the frequency of shoes in the local population, where the local population itself may be difficult to define. Any tractable solution to the problem of assessing the random match probability of a shoeprint based only on class characteristics \citep{bodziak_footwear_2000} (make, model, and other characteristics determined during the manufacturing process) requires a way to assemble this database: an automated solution to efficiently classify many types of shoes within a common system. This project is designed to address the computational and statistical process of assembling statistical features which can be used to assess similarity beteween two or more images, with the goal of producing a set of software which can be integrated into an automatic data collection system.


\section{Outsole Class Characteristics}\label{sec:class-chars-desc}

According to \citet{grossVariabilitySignificanceClass2013}, the four generally accepted conclusions that can be made from a footwear examination are elimination, inconclusive, class association, and identification. Typically, the ultimate goal of an examination is identification: matching a shoe print to an individual shoe, ideally owned by the suspect. This is difficult because identification to a specific individual's shoe requires the matching of randomly acquired characterisics, which occur due to wear and damage; that information is frequently unavailable due to the quality of the print or impression recovered from the crime scene. Examiners spend most of their time considering class associations; identifying one or more shoe models and sizes which are consistent with the recovered print.

Class characteristics are defined as the set of features which allow an object to be placed into a group with other physically similar objects. In the context of footwear, the term refers to the design and physical dimension of the shoe, particularly with regard to the shoe's outsole. While class characteristics are not sufficient for identification, they in many cases enable the exclusion of footwear \citep{bodziak_footwear_2000}.

When categorizing a shoe, it is common to use features like brand, size, and general type (e.g., boot, tennis shoe, dress shoe). When defining useful class characteristics for identification or exclusion, however, these features prove quite difficult to use. Size, for example, is far from straightforward, as size standards vary significantly across different manufacturers and scales in different countries, and different shoe styles with same size inside may have different size outsoles, making direct measurement or estimation of foot size difficult \citep{bodziak_footwear_2000}. Identifying the model of shoe is also not easily characterized because manufactuers are constantly developing new models, discontinuing existing models, or reviving discontinued models, and there exist look-alikes for many common models that are difficult to distinguish from the models they emulate. Thus, when defining features to describe any possible shoe outsole that may be found at a crime scene, it is important that any set of descriptors be general enough to describe a large variety of shoes and specific enough to differentiate between shoes that may have similar qualities. One such set of descriptors are geometric shapes, which have been found to be able to differentiate between different shoes \citep{grossVariabilitySignificanceClass2013}. Matching tread patterns by comparing the spatial distribution of geometric shapes ``is of considerable evidential value" \citep{hancockInterpretationShoeprintComparison2012} for class characteristic comparisons.

\section{Computational Image Analysis and Convolutional Neural Networks}

Shoeprint evidence from crime scenes is most commonly collected in the form of a photograph, so any useful method to automatically identify outsole characteristics must take the form of an image analysis task. There are a number of methods that may be employed to identify shapes and features in an image. The Hough transform is a feature extraction technique carried out in parameter space that was classically used to identify straight lines but has been extended to identifying circles and ellipses, as well as other shapes \citep{ballardGeneralizingHoughTransform1981}. In addition, there are a number of low-level feature extraction methods aimed at detecting specific shapes, such as edges, corners, blobs, or ridges\citep[Ch 15]{machineVision}. While these methods are useful in identifying these specific features at a low level, they are very computationally intensive and only identify features on a very small scale, often only a few pixels wide, and they are thus not able to identify large geometric shapes like those that may be found in an outsole image. Furthermore, these methods are extremely sensitive to lighting or color changes, and thus require additional modeling (like random forests) to aggregate low-level features into the geometric shapes found on outsoles.

%which were inspired by biological processes in the brain \citep{ANNasModelsofNeuralInfoProcessing},
%## Remove above citation from bib?

For novel image classification tasks, Convolutional Neural Networks (CNNs) and related methods are current industry best-practice. CNNs are a form of artificial neural network which make use of the image convolution operator used by many low-level feature extraction methods \footnote{The Google Trends interest graph shows a massive increase in convolutional neural networks between mid-2014 and 2018 \url{https://trends.google.com/trends/explore?date=2010-01-01\%202019-02-18&q=convolutional\%20neural\%20network,computer\%20vision}}. As CNNs have evolved, their architecture has become more complex, but the fundamental reliance on the image convolution operation sets CNNs apart from other artificial neural networks \citep{guRecentAdvancesConvolutional2018}.
CNNs have deep architectures that can be trained to identify complex patterns, but they are structurally similar to the human visual architecture and output binary or probabilistic predictions for given labels that are readily interpretable by humans. As CNNs make use of labeled training data, the predictions generated are for features which are similar to those identified by humans, resulting in models with greater face validity. Once a CNN is trained, it is relatively fast and easy to apply the model to new images and obtain classifications.

\subsection{General Approach}
Visual classification (i.e., assigning a label to an object based on visual input) is a complex task that humans do very well \citep{mallot2000computational}. Sight is our dominant sense and a significant part of our brain is dedicated to vision, which means that the structures used to impart meaning on a visual scene have been optimized through millions of years of evolution. As Convolutional Neural Networks are organized to mimic the process of object recognition in the human visual cortex, it will be useful to briefly describe that process.

\paragraph{Human Vision}
The visual perception process begins with the transfer of information from the visual world to the brain via rods and cones in the retina. Chemical signals travel along the optic nerve from the retina into the brain, where the signals are processed by a series of biological modules which aggregate information across multiple cells and provide meaning and order on otherwise chaotic chemical and electrical signaling. As information is aggregated, spatial relationships between objects in the physical world are maintained in the brain. Specialized feature detector cells respond preferentially to specific stimuli (e.g. cells which respond to lines oriented horizontally, vertically, or at specific angles), and these feature detectors are aggregated to identify more complex stimuli \citep[Ch. 4]{goldsteinSensationPerception2016}. In addition to the successive compilation of increasingly complex features, there are also specific modules for particularly important tasks, such as facial recognition; information from these regions is also integrated into the overall hierarchy of recognized objects from the visual input.

The problem of general object recognition is quite difficult---a three-dimensional object has infinitely many projections into two-dimensional space, and in real scenes objects are often at least partially obscured. In addition, a two-dimensional image can map back to many different three-dimensional objects, because of the ambiguity introduced by the projection onto a flat sufrace. Several psychological theories exist as to how object recognition occurs within the brain (gestalt heuristics, recognition-by-components, and inferential contexts all have experimental support), but in general the process seems to require both spatial integration and learned associations \citep[Ch. 5]{goldsteinSensationPerception2016}.

Differentiating between two objects is quite easy when features are distinct; however, there are many cases when differentiating features are rather subtle. For example, as shown in \autoref{fig:caterpillar-carrots}, an orange caterpillar and a carrot may be of similar color, shape, and size, but one is more fuzzy than the other; remarkably, the distinction between the two categories is very strong even with all of the features that are shared. Thus, our brains have learned that when faced with a small, cylindrical orange object, texture is a critically important feature when assigning a label to that object (which keeps us from accidentally ingesting caterpillars).

\begin{figure}[htbp]\centering
\includegraphics[height=.25\linewidth]{caterpillar.png}
\hskip 1cm
\includegraphics[height=.25\linewidth]{carrots.png}
\caption[Feature Similarity.]{A fuzzy caterpillar and a bunch of carrots have many similar visual features, but our brains easily distinguish between them.}\label{fig:caterpillar-carrots}
\end{figure}

\paragraph{Computer Vision Using Neural Networks}
While our brains are adept at parsing images and classifying the objects within them, the task has proved much more difficult for computers, as evidenced by \autoref{fig:xkcd-img-recognition}. Human visual processing is so complex in part because of the successive aggregation of increasingly complex features; only within the last 10 years have we been able to adequately mimic this process with computer modeling.

\begin{figure}[htbp]\centering
\includegraphics[width = .3\linewidth]{xkcd.png}
\caption[Computer vision is a difficult problem.]{Computer vision was thought to be easy in 1966 when a researcher at MIT believed that teaching a computer to separate picture regions into objects and background regions could be completed as a summer project \citep{papert_summer_1966}. The task proved much more difficult than expected, and has only become tractable with convolutional neural network based approaches.}\label{fig:xkcd-img-recognition}
\end{figure}

Convolutional Neural Networks are a widely implemented method for automated image recognition; their structure typically emulates the successive aggregation of low-level features into higher-level features that is seen in the human visual cortex. CNNs perform comparably to humans on certain image recognition tasks \citep{geirhosComparingDeepNeural2017}. The ImageNet Large Scale Visual Recognition Competition (ILSVRC) is a widely followed contest to produce the best algorithm for image classification; since 2014, it has been dominated by convolutional neural networks \citep{russakovsky_imagenet_2015}. Various models are tested on about 1.2 million images spanning 1000 categories, which are part of the ImageNet image dataset\citep{deng2009imagenet}. These categories range from natural and man-made objects (e.g., daisy, chainsaw) to living creatures (e.g., ring-tailed lemur, sea lion, and dingo). There are also many categories which require subtle distinctions, such as golden retrievers and labrador retrievers, \svp{as shown in \autoref{fig:retrievers}.}


\begin{figure}[htbp]\centering
\includegraphics[height=.25\linewidth]{labrador-retriever.jpg}
\hskip 1cm
\includegraphics[height=.25\linewidth]{golden-retriever.jpg}
\caption[Feature Similarity.]{The features used to distinguish between two similar categories may be subtle, like the features that would differentiate a golden retriever from a labrador retriever.\citep[Images from ][]{deng2009imagenet}}\label{fig:retrievers}
\end{figure}

\subsection{Building Blocks of a Convolutional Neural Network}
CNNs are made up of several distinct types of layers which transition from input image to output class probabilities; in the remainder of this section, we discuss several important layer types which are used in most CNNs.

\paragraph{Image Convolution and Convolutional Layers}
Convolutional Neural Networks make use of convolutional layers, which use image convolution as a primary operation. Defined mathematically, image convolution is an function performed on an image $x$ using a smaller-dimension matrix $\beta$.

Let \(x\) be an image represented as a numerical matrix, indexed by \(i, j\), and \(\beta\) be a filter of dimension \((2a + 1) \times (2b + 1)\). The convolution of image \(x\) and filter \(\beta\) is \begin{equation}\label{eqn:convolution}(\beta \ast x)(i, j) = \sum_{s = -a}^a\sum_{t = -b}^b \beta(s, t) x(i-s, j-t)\end{equation}

Convolutional Neural Networks are named to highlight their use of image convolution operations to extract information from an image. As shown in \autoref{fig:image-convolution-setup}, a single convolutional filter is a small array of real valued weights that represents some feature (shown in green). When a filter is applied to a portion of the image (shown in blue), a single value is returned that is associated with the presence of the feature for a given subsection of the input image. When applied over an entire image, the resulting matrix of values maps the strength of the feature across the entire image, as shown in \autoref{fig:image-convolution-illustration}. Once the entire image has been convolved with the filter, the feature map is transformed using a nonlinear activation function \mt{(see \autoref{fig:activation-functions})}. A convolutional layer of a CNN takes a large number of these filters and passes them over the image to return one feature map per filter.

\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{filter.png}
\caption[Images and convolutional filters]{An image (blue) and a convolutional filter (green). Image from \protect\citet{prabhuUnderstandingConvolutionalNeural2018}.}\label{fig:image-convolution-setup}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=.45\textwidth]{filter1.png}\hfill\includegraphics[width=.45\textwidth]{filter2.png}
\caption[Image convolution, illustrated]{The convolution operation consists of the smaller filter (green) applied to each region of the larger image (blue); each application results in a single value which is stored in the feature map. Image from \protect\citet{prabhuUnderstandingConvolutionalNeural2018}.}\label{fig:image-convolution-illustration}
\end{figure}

Convolutional layers typically do not decrease the dimensions of the matrix \svp{by a significant amount, as the filter $\beta$ is typically small: $3\times 3$ or $5\times 5$}. \svp{Many models pad the input image} in order to prevent any reduction in dimension. In \svp{these} neural network\svp{s}, dimension reduction is \svp{generally} performed by pooling layers, which identify the strongest local features in each filter layer of the convolutional output.

\paragraph{Pooling Layers}
\svp{Pooling layers are added to} reduce the size, and therefore computational load, of feature maps through structured down-sampling. \svp{Most commonly, pooling layers apply the maximum function (max pooling)} over adjacent regions of a feature map (using a sliding window). \svp{This encodes the maximum strength of a feature in a region of an image, while reducing } redundant or unneccessary information about \svp{less prominent} activations.

In most cases, the stride, or offset between subsequent pieces, is the same as the window size \svp{(non-overlapping pooling)}; for this \svp{simplified} case, the pooling layer values can be calculated using the following relationship, for pooling function $f$, window size $s$, layer $\ell-1$, and output layer $\ell$:
\begin{align}\label{eqn:element-pooling}
x^\ell_{ij} &= f\left(x^{\ell - 1}_{(i - 1)s + 1 \leq y \leq is, (j - 1)s + 1\leq z\leq js} \right)
\end{align}

\svp{As a general matrix notation for pooling is difficult to specify intuitively, we will define a pooling operator, $p$, which applies \autoref{eqn:element-pooling} element-wise.}
\begin{align}
p\left(x, f, w, s\right) :=&\text{matrix-wise pooling on matrix } x \text{ with function } f, \\& \text{ with window } w, \text{ and stride } s\nonumber
\end{align}

For example, taking 2x2 pieces of a feature map and keeping only the largest of the four values reduces the size of the feature map by a factor of 4, as shown in \autoref{fig:max-pooling}. \svp{\citet{krizhevskyImageNetClassificationDeep2012} suggests pooling layers reduce overfitting on certain datasets and increase translation invariance. Using too large of a pooling window can be destructive and limits the total number of convolutional layers which can be combined in a single network\citep{CS231nConvolutionalNeural}. Pooling layers do disrupt the model optimization process; as an alternative, some fully convolutional networks eliminate pooling layers and use convolutional layers with increased stride and window size to reduce layer dimensions \citep{springenbergStrivingSimplicityAll2014}.}


\begin{figure}
\centering
\includegraphics[width=.5\textwidth]{maxpooling}
\caption[Max  pooling layer]{Max pooling layer. Image from \protect\citet{prabhuUnderstandingConvolutionalNeural2018}.}\label{fig:max-pooling}
\end{figure}

Once the dimension of the image has been reduced and features have been identified using a combination of convolutional and pooling layers, local features must be integrated into a more unified whole. This integration is performed using densely connected layers.

\paragraph{Activation Functions}
\svp{The convolution operation discussed above relies on nonlinear activation functions}, which operate on each feature map value. Different activation functions are used to produce different network effects---in the convolutional layers, the desire is often to minimize the computational complexity, so very simple activation functions are used, such as the Rectified Linear Unit (ReLU), Exponential Linear Unit (ELU), or a differentiable analog, SoftPlus. The output layer must map features onto binary predictions or probabilities, so the sigmoid activation function is commonly used for this purpose. \autoref{fig:activation-functions} shows several common nonlinear activation functions.

<<activation-functions, echo = F, fig.scap = "Common nonlinear activation functions used in neural networks.", fig.cap = "Activation functions commonly used in neural networks.", out.width = "\\textwidth", fig.width = 6, fig.height = 2>>=
sigmoid <- function(x) 1/(1 + exp(-x))
relu <- function(x) pmax(x, 0)
elu <- function(x, a) a*(exp(x) - 1) * (x < 0) + x * (x > 0)
softplus <- function(x) log(1 + exp(x))

x <- seq(-3, 3, .005)
tibble(x = x, Linear = x, Sigmoid = sigmoid(x),
       ReLU = relu(x), ELU = elu(x, 1), SoftPlus = softplus(x)) %>%
  gather(key = key, value = value, -x) %>%
  mutate(key = factor(key, levels = c("Linear", "ReLU", "ELU",
                                      "SoftPlus", "Sigmoid"))) %>%
  ggplot(aes(x = x, y = value)) + geom_line() + facet_grid(.~key) +
  ggtitle("Common Activation Functions") +
  theme(axis.title = element_blank()) +
  coord_fixed()
@

\svp{Activation functions are also used in densely connected layers, which are discussed in the next section. Frequently, the same activation function is used throughout a CNN, though it is (mathematically) possible to use a different activation function for each layer.}

\paragraph{Densely Connected Layers}
Densely connected layers are typically the final layers in a CNN, making up what is known as the model head. These layers form the meaningful connection between the features of an image (detected by convolutional and max-pooling layers) and the corresponding labels associated with the image. Just as humans learn which combinations of features should be associated with a given label, densely connected layers use real-valued weights to represent these associations. For example, an item which is orange, small, and fuzzy is commonly associated with the word ``caterpillar". As seen in \autoref{fig:caterpillar-carrot-nodes}, fuzzy is not a feature typically associated with the word carrot, so there is little connection between the feature ``fuzzy" and the label ``carrot".

<<caterpillar-carrot-nodes, fig.height = 3, fig.width = 3, echo = F, fig.cap = 'A representation of how different features are connected to labels for classification. Note that the feature ``fuzzy" is connected to the notion of caterpillar but not to carrot, and ``pointy" is only related to carrots.' , fig.scap = "A representation of the connections between features and classification labels">>=
source(file.path(codedir, "Generate_Model_Images.R"))
col_vec <- rep("grey50", 32)
col_vec[seq(1, 29, by = 4)] <- rep(c("blue", "orange"), 4)

lty <- rep("solid", 32)
lty[c(21,25)] <- "dashed"

plot <- plot_deepviz2(c(4,2), edge_col = col_vec, line_type = lty)

df <- data.frame(x = plot$data$x,
                 y = plot$data$y,
                 labs = c("Orange", "Long", "Fuzzy", "Pointy",
                          "Caterpillar", "Carrot"))

plot + geom_label(aes(x = x, y = y, label = labs), data = df)
@

Similarly, in densely connected or fully connected layers, each final feature produced by the convolutional and pooling layers is connected to each possible label through weights (hence the name ``densely connected") learned during the training process. Weights are optimized via backpropogation \svp{(discussed in \autoref{sec:prop})} in order to minimize errors (measured by a loss function) and thus improve classification accuracy.

When training fully connected layers there is a danger that co-dependence will develop between nodes, which lessens the power of each individual node and usually leads to over-fitting. To prevent this, fully connected layers are often trained by utilizing a pruning mechanism known as drop-out, where at any given stage each node is kept in the model with probability $p$ or temporarily disabled with probability $1-p$ and training is performed on the reduced model. Densely connected layers with and without dropout nodes are shown in \autoref{fig:dense-layer}.

<<dense-layer, echo = F, fig.width = 5, fig.height = 3, out.width = ".45\\textwidth", fig.cap = "(Left) A densely-connected layer with 12 input nodes and 4 output nodes. (Right) A densely-connected layer with 12 input nodes, 4 output nodes, and a 50\\% dropout rate.", fig.scap = "Densely connected layers with and without dropout.", fig.show =  'hold'>>=
source(file.path(codedir, "Generate_Model_Images.R"))
plot_deepviz2(c(12, 4), r = .005)

plot_deepviz_sample(c(12, 4), r = .005, dropout_rate = 0.5) +
  theme(legend.position = c(1, 0), legend.justification = c(1, 0))
@

\paragraph{Output Classification Layer}
In order to transform the neural network node values into output class probabilities, the final layer of the model uses an activation function selected to conform to the problem specifications. For instance, if the goal is to perform binary classification, the sigmoid activation function shown in \autoref{fig:activation-functions} will map any real valued number to a value between 0 and 1 (that is, to a class probability). In a multinomial classification problem, an extension of the sigmoid activation function, the \emph{softmax} activation function, is used: For inputs $y_i$, $\displaystyle S(y_i) = \frac{e^{y_i}}{\sum_j e^{y_j}}.$ The softmax activation function produces class probabilities which sum to 1. In classification problems where there are $n$ classes, but each object can have between 0 and $n$ assigned labels \svp{(e.g. a multi-class, multi-label problem)}, the sigmoid activation function can be used for each class label separately, producing a $n$-dimensional vector of probabilities between 0 and 1.

\subsection{Forward and Backward Propagation}\label{sec:prop}
In order to evaluate an input image, it is necessary to move from the image representation through each of the layers in the network, with a final result of a set of $n$ output class probabilities.

We first define some notation. Let $x^{(0)}$ be an input image, represented as a numerical matrix with two dimensions of length and height, and additionally a third dimension representing the color channels, if the input image is in color. Let $x^\ell$ be the layers in the network, that is, $x^1$ is the first (convolutional) layer, $x^2$ is the second, and so on. Convolutional layers are assembled from a set of filters, $\beta^\ell_k$, where there are a set of $p^\ell$ $m\times m$ filters convolved with $x^{\ell-1}$ to create layer $x^\ell$. Each convolutional layer also has a bias matrix $\gamma^\ell$ which is used in the calculation of all filters in the $\ell$-th layer. We additionally define $\sigma^\ell(\cdot)$ as the nonlinear activation function used in layer $\ell$ (some common nonlinear activation functions are shown in \autoref{fig:activation-functions}). Finally, we define $W^\ell$ to be a weight matrix used in fully connected layer $\ell$; $W$'s dimensions are chosen such that the output dimension is equal to the pre-specified number of output classes.

During forward propagation, the calculation of the $\ell$th layer uses the $\ell-1$th layer in an interative process:
\begin{align}\label{math:forwardprop}
x^{(\ell)}_k &= \sigma^\ell\left({\beta^\ell_k}\ast x^{(\ell-1)} + \gamma^\ell\right) \text{ for convolution}\\
x^{(\ell)}_k &= p\left(x^{(\ell-1)}, \max, s, s\right) \text{ for max pooling layers}\nonumber\\
x^{(\ell)}_k &= \sigma^\ell\left(Wx^{(\ell-1)}\right) \text{ for densely connected layers}\nonumber
\end{align}


Backpropogation, short for ``the backward propagation of errors", is the name of the \svp{optimization algorithm} that is used to train deep neural networks. In the process, the error of each output is calculated and then distributed backwards through the network's layers; \svp{weights are updated in order to reduce the errors for the next iteration of the algorithm}.
%\mt{cite https://deepai.org/machine-learning-glossary-and-terms/backpropagation}
In essence, the goal is to use gradient descent to adjust the parameters of the network to achieve a local minimum of the cost function, where the gradient is computed through repeated application of the chain rule.

\svp{For convolutional layers, backpropagation works using the recurrence relationship in \autoref{eqn:backprop-conv}.}

\begin{align}\label{eqn:backprop-conv}
\left(\frac{\partial L}{\partial \beta^\ell_k}\right) &= \underbrace{\frac{\partial L}{\partial \left(\beta^\ell_k \ast x^{\ell - 1}\right)}}_\text{gradient} x^{\ell-1}\\
\frac{\partial L}{\partial \left(\beta^\ell_k \ast x^{\ell - 1}\right)} &= \frac{\partial L}{\partial x^\ell} \left[\sigma'\left(\beta^\ell_k \ast x^{\ell - 1}\right)\right]\nonumber
\end{align}

\svp{There is no backpropagation through pooling layers because there are no weights to optimize, so these layers are just pass-through layers. Backpropagation through fully connected layers takes place similar to \autoref{eqn:backprop-conv}, shown in \autoref{eqn:backprop-fc}.}
\begin{align}\label{eqn:backprop-fc}
\left(\frac{\partial L}{\partial W^\ell}\right) &= \underbrace{\frac{\partial L}{\partial \left(W^\ell x^{\ell - 1}\right)}}_\text{gradient} x^{\ell-1}\\
\frac{\partial L}{\partial \left(W^\ell x^{\ell - 1}\right)} &= \frac{\partial L}{\partial x^\ell} \left[\sigma'\left(W^\ell x^{\ell - 1}\right)\right]\nonumber
\end{align}


\subsection{Transfer Learning}
Convolutional layers and max pooling layers in a CNN are analogous to the human visual perception process, and densely connected layers behave like the human brain. In short, the approach to classifying an image is to detect the features in the image (like our eyes) and then assign labels to combinations of those features (brain). This analogy is also appropriate because it reflects the difficulty of the task: it takes many years and a significant amount of effort for humans to learn how to distinguish a large variety of features and also to connect those features to labels that are often complex, hierarchical, and subtle. Similarly, training a CNN is no small task. Even relatively simple CNNs can have many millions of trainable parameters in the model base (the convolutional and pooling layers). Optimizing all of these weights requires an incredible amount of computational power. In addition, the features learned by a neural network trained on one dataset often generalize to different data sets: particularly in the initial layers, the feature maps of a trained neural network typically activate based on color, low-level textures, and other features which are broadly generalizable \citep{yosinskiHowTransferableAre2014}. \fix{Add reference to VGG16 layers, specifically discuss how different block2 filters are from block5 in complexity.} \emph{Transfer learning} is the process of using layers from a CNN (or other classification model) trained on a general image recognition task when fitting a model intended for a more specific purpose. The layers which are deemed to identify broadly generalizable patterns are used with their pre-trained weights; new layers are added to customize the model to the specific task at hand, and typically, only these new weights are updated when the model is fit. Transfer learning allows CNNs to be applied to smaller datasets of several thousand images, with additional gains in the amount of time required to fit the model.

\begin{figure}
\centering
\includegraphics[width=.95\textwidth]{vgg16-base-head}
\caption[Transfer learning using pre-trained neural networks]{This diagram is for a pre-trained convolutional neural network. The model base consists of 5 convolutional blocks; the model head consists of several fully connected layers capped with an activation layer which transforms the aggregate visual input to output class probabilities. During transfer learning, the model base weights are fixed to the values derived from the initial input material used to train the original model; only the weights in the model head are retrained to accommodate the input training data.} \label{fig:transfer-learning-diagram}
\end{figure}

Transfer learning leverages the modularity of neural networks---the pretrained base of the model can be separated from the full model and a new model head can be trained to connect that base to the output classes, as shown in \autoref{fig:transfer-learning-diagram}. One of the simpler pre-trained convolutional networks available, VGG16, has a structure which is ideal for our purposes---it consists of several blocks of convolutional layers, with a fully connected head, as shown in \autoref{fig:vgg16-structure}. The simplicity of this structure provides the ability to peer into the inner workings of the network for diagnostic purposes, providing a distinct advantage over more complicated network structures with slightly higher accuracy ratings.

\begin{figure}
\centering
\includegraphics[width=.95\textwidth]{vgg16-layer-size}
\caption[VGG16 model structure]{The VGG16 model structure, for input images of 256 x 256 pixels. Each convolutional block operates on a different image matrix size; max pooling layers are used to transition between convolutional blocks. There are 5 convolutional blocks in the VGG16 model, which are then connected to output classes using several fully connected layers. } \label{fig:vgg16-structure}
\end{figure}

VGG16 and a similar network, VGG19, won the 2014 ILSVR challenge; they have since been outcompeted by networks with more complicated structures, such as GoogLeNet/Inception \citep{szegedyGoingDeeperConvolutions2015} and ResNet \citep{heDeepResidualLearning2015}. VGG-style networks are still commonly used as building blocks for other types of convolutional networks; their streamlined structure allows for easy extension to other tasks, such as texture detection and style transfer \citep{gatysImageStyleTransfer2016}.

\section{Machine Learning Model Evaluation}\label{sec:model-eval}

Classification tasks are considered single-class when there is a only one binary decision of whether an item belongs to a single class of interest, and multi-class when there is a larger number of classes that an object may belong to. Multi-label classification is the special case of multi-class classification where categories are not mutually exclusive, that is, an item may fall into a combination of categories simultaneously. While the true classification is a binary decision, it is common for models to predict these classifications using probability, thus reporting a value between 0 and 1 reflecting how certainly the item can be attributed to a given class.

Evaluating model accuracy on classification requires addressing both the labels that are assigned and the labels that are not. Ideally, an image is labeled perfectly, and a true positive occurs when the model assigns a label which matches that of the image. A false positive in this scenario occurs when the model assigns a label which does not match that of the image. A true negative occurs if the model does not assign a label which does not occur in the image, and a false negative occurs when the model does not assign a label which does occur in the image. \autoref{tab:error-binary-problem} illustrates these terms for a simple binary classification problem.

\begin{table}
\centering
\begin{tabular}{cccc}
 & & \multicolumn{2}{c}{Model---Assigned Label} \\\cline{3-4}
 & & A & Not A \\\hline
 \multirow{2}{*}{True Label} & A & True Positive & False Negative\\\cline{2-4}
 & Not A & False Positive & True Negative \\\hline
\end{tabular}
\caption[Model errors for a two-class binary decision problem.]{Model errors for a two-class binary decision problem. Correct decisions are shown along the diagonal; incorrect decisions are in the off-diagonal cells.}\label{tab:error-binary-problem}
\end{table}

Recall, or sensitivity, which is the true positive rate, is the number of all true positives divided by the number of positive cases in the data. Specificity, which is the true negative rate, is the number of all true negatives divided by the number of negative cases in the data. Precision is the sum of all true positives divided by the number of positive predictions made by the model.

ROC curves plot the false positive rate (1 - specificity) against the true positive rate. Ideally, we want a high true positive rate and a low false positive rate, so a perfect ROC curve would hug the top-left corner of the graph and a straight line between corners would indicate that the prediction is no better than random chance. The Area Under the Curve (AUC) quantifies the shape of the ROC curve, with an AUC of 1 corresponding to perfect prediction and 0.5 indicating random chance. Examples of ROC curves are shown in \autoref{fig:example-roc} for a random forest model to predict species for Fisher's Iris Data from sepal length; note that ROC curves are only applicable for single-label classification, so a separate curve is required for each species.  \svp{ROC curves are used for binary classification problems and have been extended to multi-class problems, but in most multi-class extension methods\citep{handSimpleGeneralisationArea2001}, it is not easy to see how the model performs for each class; this issue is exacerbated when classes are unbalanced.}

<<example-roc, echo = F, fig.scap = "ROC curve plots, showing model performance.", fig.cap = "ROC curves, showing the performance of a random forest to predict iris species from sepal length from Fisher's Iris Data. Note that prediction is very accurate for species Setosa, but is not much better than random chance for species Versicolor.", fig.width = 9, fig.height = 3>>=

library(pROC)
mod <- randomForest::randomForest(y = iris$Species,
                                  x = iris[,"Sepal.Length", drop = F])
spec <- unique(iris$Species)

p <- list()
for (i in 1:3){
  roc_list <- roc(iris$Species == spec[i], mod$votes[,i])
  roc_df <- tibble(tpr = roc_list$sensitivities,
                   fpr = 1 - roc_list$specificities,
                   thresholds = roc_list$thresholds,
                   auc = roc_list$auc[1]) %>%
    nest(tpr, fpr, thresholds, .key =  "roc_plot") %>%
    mutate(eer = purrr::map(roc_plot, eer))

  p[[i]] <- ggplot() +
    geom_line(aes(x = fpr, y = tpr), data = unnest(roc_df, roc_plot) %>% 
                arrange(fpr, tpr), size = 1.25) +
    geom_label(aes(x = 1, y = .07, label = sprintf("AUC: %0.2f", auc)), 
               hjust = 1, vjust = -0.2, data = roc_df) +
    geom_point(aes(x = fpr, y = tpr, color = "Equal Error Rate"), 
               data = unnest(roc_df, eer), size = 4) +
    scale_color_manual("", values = "black") +
    scale_x_continuous("False Positive Rate", 
                       breaks = c(0, .25, .5, .75, 1), 
                       labels = c("0.0", "", "0.5", "", "1.0")) +
    scale_y_continuous("True Positive Rate", 
                       breaks = c(0, .25, .5, .75, 1), 
                       labels = c("0.0", "", "0.5", "", "1.0")) +
    ggtitle(as.character(spec[i]) %>% 
              stringr::str_to_title()) +
    coord_fixed() + 
    theme_bw() +
    theme(legend.position = c(1, 0), 
          legend.justification = c(1.01, -0.01), 
          legend.title = element_blank(), 
          legend.background = element_rect(fill = "white"))
}

gridExtra::grid.arrange(grobs = p, nrow = 1)
@

A confusion matrix is a cross-tabulation between the observed and the predicted classes of the data. A confusion matrix can help identify which classes are being correctly predicted and which classes are commonly mixed-up with others. Confusion matrices are typically used with binary classification problems; an aggregate version of \autoref{tab:error-binary-problem} would be a confusion matrix, where totals for each decision would be shown in the matrix cells. Confusion matrices do not extend easily to multi-class problems because any single miscategorization produces both a false negative decision and a false positive decision; it is not clear which should be shown in each cell, as shown in \autoref{tab:error-3class-problem}.
\svp{XXX - Add a confusion matrix from ggplot2 to show as an example visualization}

<<example-conf-mat, eval = F>>=
mod$confusion[,1:3] %>%
  as.data.frame() %>%
  sweep(., MARGIN = 1, STATS = colSums(.), FUN = "/") %>%
  set_names(str_to_title(spec)) %>%
  ggcorrplot(., hc.order = F, outline.col = "white", lab = T) +
  scale_fill_gradient("Classification\nRate", low = "white",
                      high = "cornflowerblue", limits = c(0, 1)) +
  scale_x_discrete("Image Label") + scale_y_discrete("Prediction") +
  theme(axis.title.x = element_text(size = 14),
        axis.title.y = element_text(size = 14, angle = 90, vjust = 1)) +
  ggtitle("Prediction Accuracy of Species by Sepal Length") +
  theme_bw() +
  theme(panel.grid.major = element_line(color = "grey50"),
        panel.grid.minor = element_line(color = "grey60")) +
  theme(plot.margin = grid::unit(c(0,0,0,0), "mm"),
        plot.background = element_rect(fill = "transparent", color = NA),
        plot.subtitle = element_blank(), plot.caption = element_blank(),
        panel.spacing = unit(c(0, 0, 0, 0), "mm"))
@

\begin{table}
\centering
\begin{tabular}{ccccc}
 & & \multicolumn{3}{c}{Model---Assigned Label} \\\cline{3-5}
 & & A & B & C \\\hline
 \multirow{6}{*}{True Label} & \multirow{2}{*}{A} & \multirow{2}{*}{True Positive (A)} & False Positive (B) & False Positive (C) \\
 & & & False Negative (A) & False Negative (A)\\\cline{2-5}
 & \multirow{2}{*}{B} & False Positive (A) & \multirow{2}{*}{True Positive (B)} & False Positive (C) \\
 & & False Negative (B) & & False Negative (B) \\\cline{2-5}
 & \multirow{2}{*}{C} & False Positive (A) & False Positive (B) & \multirow{2}{*}{True Positive (C)} \\
 & & False Negative (C) & False Negative (C) & \\\hline
\end{tabular}
\caption[Model errors for a three-class binary decision problem.]{Model errors for a three-class binary decision problem. Correct decisions are shown along the diagonal; incorrect decisions are in the off-diagonal cells. Note that for each misclassification, two incorrect decisions are made---the ommission of the correct label and the addition of an incorrect label.}\label{tab:error-3class-problem}
\end{table}

However, in some situations, it is possible to reduce the classification problem to a series of binary classifications; in these situations, which will be described in more detail in \autoref{ch3:model-accuracy}, a modification of the confusion matrix is possible to better visualize model performance.
